<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NGINX: From Zero to Hero - Big Data & IA</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            background: #0f172a;
            color: #e2e8f0;
            padding: 40px 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        
        h1 {
            font-size: 2.8em;
            margin-bottom: 20px;
            background: linear-gradient(135deg, #60a5fa 0%, #3b82f6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .subtitle {
            font-size: 1.2em;
            color: #94a3b8;
            margin-bottom: 50px;
            border-left: 4px solid #3b82f6;
            padding-left: 20px;
        }
        
        .block {
            background: #1e293b;
            border-radius: 12px;
            padding: 40px;
            margin-bottom: 40px;
            border: 1px solid #334155;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
        }
        
        .block h2 {
            font-size: 2em;
            color: #60a5fa;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 2px solid #334155;
        }
        
        .block h3 {
            font-size: 1.5em;
            color: #93c5fd;
            margin: 30px 0 20px 0;
        }
        
        .question {
            background: #0f172a;
            padding: 25px;
            border-radius: 8px;
            margin: 25px 0;
            border-left: 4px solid #f59e0b;
            font-size: 1.15em;
            font-weight: 600;
        }
        
        .alert-critical {
            background: #7f1d1d;
            color: #fca5a5;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 5px solid #dc2626;
            font-weight: bold;
            font-size: 1.2em;
        }
        
        .alert-warning {
            background: #78350f;
            color: #fcd34d;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 5px solid #f59e0b;
            font-weight: bold;
        }
        
        .alert-info {
            background: #1e3a5f;
            color: #93c5fd;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 5px solid #3b82f6;
        }
        
        .comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }
        
        .comparison-item {
            background: #0f172a;
            padding: 20px;
            border-radius: 8px;
            border: 2px solid #334155;
        }
        
        .comparison-item.bad {
            border-color: #dc2626;
        }
        
        .comparison-item.good {
            border-color: #10b981;
        }
        
        .comparison-item h4 {
            margin-bottom: 15px;
            font-size: 1.2em;
        }
        
        .bad h4 {
            color: #ef4444;
        }
        
        .good h4 {
            color: #10b981;
        }
        
        code {
            background: #0f172a;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: #fbbf24;
            font-size: 0.95em;
        }
        
        pre {
            background: #0f172a;
            padding: 25px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid #334155;
        }
        
        pre code {
            background: none;
            padding: 0;
            color: #e2e8f0;
        }
        
        ul, ol {
            margin: 20px 0 20px 30px;
        }
        
        li {
            margin: 10px 0;
        }
        
        strong {
            color: #fbbf24;
        }
        
        .key-concept {
            background: #1e293b;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #8b5cf6;
        }
        
        .exercise {
            background: #14532d;
            padding: 25px;
            border-radius: 8px;
            margin: 30px 0;
            border-left: 5px solid #10b981;
        }
        
        .exercise h4 {
            color: #34d399;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
        }
        
        th, td {
            padding: 15px;
            text-align: left;
            border: 1px solid #334155;
        }
        
        th {
            background: #0f172a;
            color: #60a5fa;
            font-weight: bold;
        }
        
        tr:nth-child(even) {
            background: #1e293b;
        }
        
        .arrow {
            color: #60a5fa;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>NGINX: From Zero to Hero</h1>
        <div class="subtitle">
            Curso especializado para Big Data e IA - De servidor web a orquestador de microservicios
        </div>

        <!-- BLOQUE 1 -->
        <div class="block">
            <h2>Bloque 1: Fundamentos y Arquitectura de Red</h2>
            
            <div class="question">
                Tienes una API con FastAPI y Uvicorn. ¿La expones directamente al puerto 80/443?
            </div>
            
            <div class="alert-critical">
                Error Crítico.
            </div>
            
            <p>Uvicorn es un servidor de aplicaciones, no de infraestructura:</p>
            
            <ul>
                <li>No maneja SSL/TLS nativamente en producción</li>
                <li>No tiene rate limiting</li>
                <li>No balancea carga entre múltiples workers</li>
                <li>Expone directamente tu aplicación Python</li>
            </ul>

            <h3>¿Por qué NGINX domina en Big Data e IA?</h3>
            
            <div class="comparison">
                <div class="comparison-item bad">
                    <h4>❌ Apache/Tradicional</h4>
                    <p>Un proceso/hilo por cada conexión.</p>
                    <p><strong>Resultado:</strong> Consumo masivo de RAM con miles de usuarios.</p>
                </div>
                <div class="comparison-item good">
                    <h4>✓ NGINX</h4>
                    <p>Usa un <strong>Event Loop</strong> no bloqueante.</p>
                    <p><strong>Resultado:</strong> Gestiona 10,000+ conexiones concurrentes con mínimos recursos.</p>
                </div>
            </div>

            <div class="alert-info">
                <strong>Perfecto match:</strong> Ideal para la naturaleza asíncrona de <code>FastAPI</code>.
            </div>

            <h3>Proxy vs Reverse Proxy</h3>
            
            <table>
                <tr>
                    <th>Tipo</th>
                    <th>¿A quién protege?</th>
                    <th>Ejemplo</th>
                </tr>
                <tr>
                    <td><strong>Forward Proxy</strong></td>
                    <td>Protege al cliente. Oculta quién eres tú ante el servidor.</td>
                    <td>Una VPN</td>
                </tr>
                <tr>
                    <td><strong>Reverse Proxy</strong></td>
                    <td>Protege al servidor. Oculta la infraestructura interna ante el cliente.</td>
                    <td>NGINX</td>
                </tr>
            </table>

            <h3>Anatomía de la Configuración</h3>
            
            <p>La configuración es jerárquica (Bloques):</p>

<pre><code>http {
    # Configuración global de tráfico web
    
    server {
        listen 80;
        server_name api.mi-ia.com;
        
        location / {
            # Aquí es donde ocurre la magia del Proxy
            proxy_pass http://localhost:8000;
        }
    }
}
</code></pre>

            <div class="key-concept">
                <strong>Jerarquía de configuración:</strong>
                <code>http ⮕ server ⮕ location</code>
                <br><br>
                NGINX busca coincidencias en <code>location</code> de la más específica a la más general.
            </div>

            <h3>Conceptos Clave del Config</h3>
            
            <ul>
                <li><code>worker_processes:</code> Define cuántos núcleos de CPU usará NGINX (usualmente 'auto').</li>
                <li><code>events:</code> Donde se configura la capacidad de conexiones por worker.</li>
                <li><code>upstream:</code> Define un grupo de servidores para balanceo de carga.</li>
                <li><code>server:</code> Un host virtual (equivalente a un dominio/subdominio).</li>
                <li><code>location:</code> Reglas de enrutamiento dentro de un server.</li>
            </ul>
        </div>

        <!-- BLOQUE 2 -->
        <div class="block">
            <h2>Bloque 2: Configuración Práctica con FastAPI</h2>
            
            <div class="question">
                ¿Cómo paso del <code>uvicorn main:app --host 0.0.0.0 --port 8000</code> a producción real?
            </div>

            <h3>Paso 1: Setup básico con Docker Compose</h3>

<pre><code># docker-compose.yml
version: '3.8'

services:
  fastapi:
    build: ./app
    container_name: fastapi_app
    expose:
      - "8000"
    networks:
      - app_network
  
  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      - fastapi
    networks:
      - app_network

networks:
  app_network:
    driver: bridge
</code></pre>

            <div class="alert-warning">
                <strong>Nota:</strong> Usamos <code>expose</code> en FastAPI, no <code>ports</code>. FastAPI solo debe ser accesible por NGINX, no por internet.
            </div>

            <h3>Paso 2: Configuración mínima de NGINX</h3>

<pre><code># nginx/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    log_format main '$remote_addr - $remote_user [$time_local] '
                    '"$request" $status $body_bytes_sent '
                    '"$http_referer" "$http_user_agent"';
    
    access_log /var/log/nginx/access.log main;
    
    sendfile on;
    keepalive_timeout 65;
    
    include /etc/nginx/conf.d/*.conf;
}
</code></pre>

<pre><code># nginx/conf.d/fastapi.conf
upstream fastapi_backend {
    server fastapi:8000;
}

server {
    listen 80;
    server_name localhost;
    
    client_max_body_size 100M;
    
    location / {
        proxy_pass http://fastapi_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</code></pre>

            <h3>Headers Críticos del Proxy</h3>
            
            <table>
                <tr>
                    <th>Header</th>
                    <th>¿Por qué es crítico?</th>
                </tr>
                <tr>
                    <td><code>X-Real-IP</code></td>
                    <td>Sin esto, FastAPI ve la IP de NGINX (172.x.x.x), no la del usuario real.</td>
                </tr>
                <tr>
                    <td><code>X-Forwarded-For</code></td>
                    <td>Mantiene la cadena completa de proxies. Esencial para logs y seguridad.</td>
                </tr>
                <tr>
                    <td><code>X-Forwarded-Proto</code></td>
                    <td>Indica si la petición original era HTTP o HTTPS. Crucial para redirects.</td>
                </tr>
                <tr>
                    <td><code>Host</code></td>
                    <td>Preserva el dominio original. FastAPI lo necesita para generar URLs correctas.</td>
                </tr>
            </table>

            <div class="exercise">
                <h4>Ejercicio 1: Primer Reverse Proxy</h4>
                <ol>
                    <li>Crea un FastAPI simple que retorne <code>{"client_ip": request.client.host}</code></li>
                    <li>Despliega con docker-compose (FastAPI + NGINX)</li>
                    <li>Verifica que ves tu IP real, no la del contenedor</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 3 -->
        <div class="block">
            <h2>Bloque 3: Load Balancing - Escalado Horizontal</h2>
            
            <div class="question">
                Tu modelo de IA tarda 5 segundos en procesar cada request. ¿Cómo escalar?
            </div>

            <h3>Upstream: Múltiples Backends</h3>

<pre><code># docker-compose.yml
services:
  fastapi_1:
    build: ./app
    expose:
      - "8000"
    networks:
      - app_network
  
  fastapi_2:
    build: ./app
    expose:
      - "8000"
    networks:
      - app_network
  
  fastapi_3:
    build: ./app
    expose:
      - "8000"
    networks:
      - app_network
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      - fastapi_1
      - fastapi_2
      - fastapi_3
    networks:
      - app_network
</code></pre>

<pre><code># nginx/conf.d/load_balancer.conf
upstream fastapi_cluster {
    # Estrategia por defecto: Round Robin
    server fastapi_1:8000;
    server fastapi_2:8000;
    server fastapi_3:8000;
}

server {
    listen 80;
    
    location / {
        proxy_pass http://fastapi_cluster;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
</code></pre>

            <h3>Estrategias de Balanceo</h3>

            <table>
                <tr>
                    <th>Estrategia</th>
                    <th>Configuración</th>
                    <th>Caso de Uso</th>
                </tr>
                <tr>
                    <td><strong>Round Robin</strong></td>
                    <td>(por defecto)</td>
                    <td>Todos los backends tienen igual capacidad</td>
                </tr>
                <tr>
                    <td><strong>Least Connections</strong></td>
                    <td><code>least_conn;</code></td>
                    <td>Requests con tiempos de procesamiento variables</td>
                </tr>
                <tr>
                    <td><strong>IP Hash</strong></td>
                    <td><code>ip_hash;</code></td>
                    <td>Necesitas sesiones sticky (mismo usuario → mismo servidor)</td>
                </tr>
                <tr>
                    <td><strong>Weighted</strong></td>
                    <td><code>server backend1:8000 weight=3;</code></td>
                    <td>Backends con diferente potencia (CPU/GPU)</td>
                </tr>
            </table>

            <h3>Ejemplo: Weighted para GPUs heterogéneas</h3>

<pre><code>upstream ml_inference {
    server gpu_v100:8000 weight=3;   # Tesla V100 - 3x más requests
    server gpu_t4:8000 weight=2;     # Tesla T4 - 2x más requests
    server cpu_fallback:8000 weight=1;  # CPU backup
}
</code></pre>

            <h3>Health Checks y Failover</h3>

<pre><code>upstream fastapi_cluster {
    server fastapi_1:8000 max_fails=3 fail_timeout=30s;
    server fastapi_2:8000 max_fails=3 fail_timeout=30s;
    server fastapi_3:8000 backup;  # Solo se usa si los demás fallan
}
</code></pre>

            <div class="key-concept">
                <strong>max_fails:</strong> Número de intentos fallidos antes de marcar como down.
                <br>
                <strong>fail_timeout:</strong> Tiempo que espera antes de reintentar un servidor caído.
                <br>
                <strong>backup:</strong> Solo recibe tráfico cuando los demás están down.
            </div>

            <div class="exercise">
                <h4>Ejercicio 2: Load Balancer con Logs</h4>
                <ol>
                    <li>Despliega 3 réplicas de tu FastAPI</li>
                    <li>Cada una debe loggear su <code>hostname</code> al procesar requests</li>
                    <li>Lanza 100 requests con <code>curl</code> o <code>requests</code></li>
                    <li>Verifica la distribución. ¿Es uniforme?</li>
                    <li>Mata uno de los backends y vuelve a probar</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 4 -->
        <div class="block">
            <h2>Bloque 4: HTTPS y Seguridad con Let's Encrypt</h2>
            
            <div class="question">
                ¿Por qué HTTP ya no es aceptable, ni siquiera en desarrollo?
            </div>

            <div class="alert-critical">
                Google Chrome marca como "No Seguro" cualquier site HTTP que acepte datos.
            </div>

            <ul>
                <li>Los navegadores bloquean APIs geográficas, cámara, micrófono en HTTP</li>
                <li>HTTP/2 y HTTP/3 <strong>requieren</strong> HTTPS</li>
                <li>Service Workers (PWA) solo funcionan con HTTPS</li>
            </ul>

            <h3>Certificados SSL/TLS: Conceptos</h3>

            <div class="comparison">
                <div class="comparison-item bad">
                    <h4>Certificado Auto-Firmado</h4>
                    <p><strong>Pros:</strong> Gratis, inmediato.</p>
                    <p><strong>Contras:</strong> El navegador muestra error de seguridad. Solo para desarrollo local.</p>
                </div>
                <div class="comparison-item good">
                    <h4>Let's Encrypt</h4>
                    <p><strong>Pros:</strong> Gratis, trusted por navegadores, auto-renovable.</p>
                    <p><strong>Contras:</strong> Requiere dominio público y puerto 80 accesible.</p>
                </div>
            </div>

            <h3>Setup con Certbot (Docker)</h3>

<pre><code># docker-compose.yml
services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./certbot/conf:/etc/letsencrypt:ro
      - ./certbot/www:/var/www/certbot:ro
    networks:
      - app_network
  
  certbot:
    image: certbot/certbot
    volumes:
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
</code></pre>

<pre><code># nginx/conf.d/ssl.conf
server {
    listen 80;
    server_name tu-dominio.com;
    
    # ACME challenge para Let's Encrypt
    location /.well-known/acme-challenge/ {
        root /var/www/certbot;
    }
    
    # Redirect todo lo demás a HTTPS
    location / {
        return 301 https://$host$request_uri;
    }
}

server {
    listen 443 ssl http2;
    server_name tu-dominio.com;
    
    ssl_certificate /etc/letsencrypt/live/tu-dominio.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/tu-dominio.com/privkey.pem;
    
    # Configuración SSL moderna
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256';
    ssl_prefer_server_ciphers off;
    
    location / {
        proxy_pass http://fastapi_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
</code></pre>

            <h3>Obtener el certificado inicial</h3>

<pre><code># En el servidor, primera vez
docker-compose up -d nginx
docker-compose run --rm certbot certonly \
    --webroot \
    --webroot-path /var/www/certbot/ \
    -d tu-dominio.com \
    --email tu@email.com \
    --agree-tos \
    --no-eff-email

docker-compose restart nginx
</code></pre>

            <div class="alert-info">
                <strong>Renovación automática:</strong> El contenedor de certbot revisa cada 12h y renueva si faltan menos de 30 días.
            </div>

            <div class="exercise">
                <h4>Ejercicio 3: HTTPS en Local (auto-firmado)</h4>
                <ol>
                    <li>Genera certificado auto-firmado: <code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout nginx-selfsigned.key -out nginx-selfsigned.crt</code></li>
                    <li>Configura NGINX para usar el certificado</li>
                    <li>Accede por HTTPS (acepta el warning del navegador)</li>
                    <li>Inspecciona el certificado desde DevTools</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 5 -->
        <div class="block">
            <h2>Bloque 5: Caching - Performance para Modelos de IA</h2>
            
            <div class="question">
                Tu endpoint <code>/predict</code> ejecuta un modelo que siempre devuelve lo mismo para los mismos inputs. ¿Por qué recalcular?
            </div>

            <h3>Tipos de Cache en NGINX</h3>

            <table>
                <tr>
                    <th>Tipo</th>
                    <th>Cuándo usar</th>
                    <th>Ejemplo</th>
                </tr>
                <tr>
                    <td><strong>Proxy Cache</strong></td>
                    <td>Responses HTTP de tu backend</td>
                    <td>Predicciones de modelos deterministas</td>
                </tr>
                <tr>
                    <td><strong>Static Files</strong></td>
                    <td>Assets que no cambian (JS, CSS, imágenes)</td>
                    <td>Frontend de dashboards</td>
                </tr>
                <tr>
                    <td><strong>FastCGI Cache</strong></td>
                    <td>Aplicaciones PHP (no relevante para Python)</td>
                    <td>-</td>
                </tr>
            </table>

            <h3>Configuración de Proxy Cache</h3>

<pre><code># nginx.conf (nivel http)
http {
    proxy_cache_path /var/cache/nginx/predictions 
                     levels=1:2 
                     keys_zone=predictions_cache:10m 
                     max_size=1g 
                     inactive=60m 
                     use_temp_path=off;
    
    include /etc/nginx/conf.d/*.conf;
}
</code></pre>

<pre><code># nginx/conf.d/api.conf
upstream fastapi_backend {
    server fastapi:8000;
}

server {
    listen 80;
    
    # Endpoint con predicciones cacheadas
    location /api/v1/predict {
        proxy_cache predictions_cache;
        proxy_cache_key "$request_method$request_uri$request_body";
        proxy_cache_valid 200 10m;
        proxy_cache_valid 404 1m;
        proxy_cache_methods GET POST;
        
        # Headers de debug
        add_header X-Cache-Status $upstream_cache_status;
        
        proxy_pass http://fastapi_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    # Endpoint sin cache (datos en tiempo real)
    location /api/v1/stream {
        proxy_cache off;
        proxy_pass http://fastapi_backend;
        proxy_buffering off;
    }
}
</code></pre>

            <h3>Desglose de la Config de Cache</h3>

            <ul>
                <li><code>keys_zone=predictions_cache:10m</code> - 10MB de RAM para metadata (aprox. 80,000 keys)</li>
                <li><code>max_size=1g</code> - Máximo 1GB de respuestas cacheadas en disco</li>
                <li><code>inactive=60m</code> - Elimina entries no accedidas en 60 min, aunque no hayan expirado</li>
                <li><code>proxy_cache_key</code> - Define qué hace única una request (incluye el body para POSTs)</li>
            </ul>

            <div class="alert-warning">
                <strong>Cuidado con cachear POSTs:</strong> Solo hazlo si el endpoint es idempotente y determinista.
            </div>

            <h3>Cache Busting y Purging</h3>

<pre><code># Endpoint para limpiar cache (protegido por IP)
location ~ /purge(/.*) {
    allow 127.0.0.1;
    deny all;
    
    proxy_cache_purge predictions_cache "$request_method$1";
}
</code></pre>

            <h3>Headers de Control desde FastAPI</h3>

            <p>Tu backend puede controlar el cache vía headers:</p>

<pre><code># En tu FastAPI
from fastapi import Response

@app.get("/api/v1/predict")
def predict(response: Response):
    result = run_model()
    
    # Forzar cache por 5 minutos
    response.headers["Cache-Control"] = "public, max-age=300"
    
    return result

@app.get("/api/v1/user_data")
def user_data(response: Response):
    # Prohibir cualquier cache
    response.headers["Cache-Control"] = "no-store, no-cache, must-revalidate"
    
    return get_sensitive_data()
</code></pre>

            <div class="exercise">
                <h4>Ejercicio 4: Cache de Predicciones</h4>
                <ol>
                    <li>Crea un endpoint FastAPI <code>/heavy_model</code> que <code>time.sleep(3)</code></li>
                    <li>Configura NGINX para cachear las responses por 2 minutos</li>
                    <li>Lanza 10 requests idénticos y mide el tiempo</li>
                    <li>Inspecciona el header <code>X-Cache-Status</code> (HIT/MISS/BYPASS)</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 6 -->
        <div class="block">
            <h2>Bloque 6: Rate Limiting y Seguridad Avanzada</h2>
            
            <div class="question">
                Tu API de IA consume 2 segundos de GPU por request. ¿Qué pasa si un usuario lanza 1000 requests/segundo?
            </div>

            <div class="alert-critical">
                Sin rate limiting, un solo usuario puede tumbar tu infraestructura o disparar tu factura cloud.
            </div>

            <h3>Límites de Requests</h3>

<pre><code># nginx.conf (nivel http)
http {
    # Define zonas de rate limiting
    limit_req_zone $binary_remote_addr zone=general:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=api:10m rate=2r/s;
    limit_req_zone $binary_remote_addr zone=heavy:10m rate=1r/m;
    
    include /etc/nginx/conf.d/*.conf;
}
</code></pre>

<pre><code># nginx/conf.d/api.conf
server {
    listen 80;
    
    # Rutas públicas (frontend)
    location / {
        limit_req zone=general burst=20 nodelay;
        root /usr/share/nginx/html;
    }
    
    # API general
    location /api/ {
        limit_req zone=api burst=5 nodelay;
        proxy_pass http://fastapi_backend;
    }
    
    # Endpoints pesados (IA/ML)
    location /api/v1/train {
        limit_req zone=heavy burst=2;
        proxy_pass http://fastapi_backend;
        proxy_read_timeout 300s;
    }
}
</code></pre>

            <h3>Anatomía del Rate Limit</h3>

            <ul>
                <li><code>rate=10r/s</code> - 10 requests por segundo (promedio sostenido)</li>
                <li><code>burst=20</code> - Permite picos de hasta 20 requests, pero luego aplica el rate</li>
                <li><code>nodelay</code> - Procesa el burst inmediatamente (sin forzar delay artificial)</li>
                <li><code>$binary_remote_addr</code> - Limita por IP (usa menos RAM que <code>$remote_addr</code>)</li>
            </ul>

            <div class="comparison">
                <div class="comparison-item">
                    <h4>Sin nodelay</h4>
                    <p>NGINX encola requests del burst y las procesa al rate configurado.</p>
                    <p><strong>Efecto:</strong> El usuario ve delays artificiales.</p>
                </div>
                <div class="comparison-item">
                    <h4>Con nodelay</h4>
                    <p>Procesa el burst inmediatamente, pero rechaza si excede burst.</p>
                    <p><strong>Efecto:</strong> Respuestas rápidas o error 429.</p>
                </div>
            </div>

            <h3>Limitar por API Key (avanzado)</h3>

<pre><code># Extrae API key del header
map $http_x_api_key $api_limit {
    default                    slow;
    "premium_key_abc123"       fast;
    "enterprise_key_xyz789"    unlimited;
}

# Define zonas por tier
limit_req_zone $binary_remote_addr zone=slow:10m rate=1r/s;
limit_req_zone $binary_remote_addr zone=fast:10m rate=10r/s;

server {
    location /api/ {
        # Aplica límite según el tier
        limit_req zone=$api_limit burst=5;
        
        proxy_pass http://fastapi_backend;
    }
}
</code></pre>

            <h3>Protección contra DDoS Básico</h3>

<pre><code>http {
    # Limita conexiones concurrentes por IP
    limit_conn_zone $binary_remote_addr zone=addr:10m;
    
    # Limita solicitudes por segundo
    limit_req_zone $binary_remote_addr zone=req_limit:10m rate=5r/s;
}

server {
    # Máximo 10 conexiones concurrentes por IP
    limit_conn addr 10;
    
    # Timeout agresivos
    client_body_timeout 10s;
    client_header_timeout 10s;
    send_timeout 10s;
    
    # Limita tamaño de uploads
    client_max_body_size 10M;
    
    location / {
        limit_req zone=req_limit burst=10 nodelay;
        proxy_pass http://fastapi_backend;
    }
}
</code></pre>

            <h3>Custom Error Page para 429</h3>

<pre><code>server {
    error_page 429 /429.json;
    
    location = /429.json {
        internal;
        default_type application/json;
        return 429 '{"error": "Rate limit exceeded", "retry_after": 60}';
    }
}
</code></pre>

            <div class="exercise">
                <h4>Ejercicio 5: Romper el Rate Limit</h4>
                <ol>
                    <li>Configura rate limit de 2 req/s con burst=5</li>
                    <li>Usa este script Python:</li>
                </ol>
<pre><code>import requests
import time

for i in range(20):
    r = requests.get("http://localhost/api/predict")
    print(f"{i}: {r.status_code}")
    time.sleep(0.1)  # 10 req/s
</code></pre>
                <ol start="3">
                    <li>Observa cuándo empieza a devolver 429</li>
                    <li>Ajusta el <code>burst</code> y prueba de nuevo</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 7 -->
        <div class="block">
            <h2>Bloque 7: Logging y Monitoring Avanzado</h2>
            
            <div class="question">
                ¿Cómo debuggeas un endpoint que falla solo el 0.1% de las veces?
            </div>

            <h3>Custom Log Formats</h3>

<pre><code># nginx.conf
http {
    # Log format para análisis de ML
    log_format ml_api '$remote_addr - $remote_user [$time_local] '
                      '"$request" $status $body_bytes_sent '
                      '"$http_referer" "$http_user_agent" '
                      'rt=$request_time uct="$upstream_connect_time" '
                      'uht="$upstream_header_time" urt="$upstream_response_time" '
                      'cache=$upstream_cache_status '
                      'api_key=$http_x_api_key '
                      'model_version=$http_x_model_version';
    
    access_log /var/log/nginx/access.log ml_api;
}
</code></pre>

            <h3>Variables Críticas de Performance</h3>

            <table>
                <tr>
                    <th>Variable</th>
                    <th>Qué mide</th>
                    <th>Valor normal</th>
                </tr>
                <tr>
                    <td><code>$request_time</code></td>
                    <td>Tiempo total (desde que llega la request hasta la última byte de respuesta)</td>
                    <td>< 1s</td>
                </tr>
                <tr>
                    <td><code>$upstream_connect_time</code></td>
                    <td>Tiempo en conectar con el backend</td>
                    <td>< 10ms</td>
                </tr>
                <tr>
                    <td><code>$upstream_header_time</code></td>
                    <td>Tiempo hasta recibir el header de respuesta del backend</td>
                    <td>< 500ms</td>
                </tr>
                <tr>
                    <td><code>$upstream_response_time</code></td>
                    <td>Tiempo total de procesamiento en el backend</td>
                    <td>Depende del modelo</td>
                </tr>
            </table>

            <h3>Conditional Logging (solo errores lentos)</h3>

<pre><code>map $status $loggable {
    ~^[23]  0;  # No loggear 2xx y 3xx
    default 1;  # Loggear 4xx y 5xx
}

map $request_time $slow_request {
    ~^[0-4]\. 0;  # Menos de 5 segundos
    default   1;  # 5+ segundos
}

server {
    # Solo loggea errores o requests lentas
    access_log /var/log/nginx/errors.log combined if=$loggable;
    access_log /var/log/nginx/slow.log ml_api if=$slow_request;
}
</code></pre>

            <h3>Integración con Prometheus (métricas)</h3>

<pre><code># Usar nginx-prometheus-exporter
# docker-compose.yml
services:
  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    ports:
      - "80:80"
      - "8080:8080"  # Stub status
  
  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:latest
    command:
      - '-nginx.scrape-uri=http://nginx:8080/stub_status'
    ports:
      - "9113:9113"
    depends_on:
      - nginx
</code></pre>

<pre><code># nginx.conf
server {
    listen 8080;
    location /stub_status {
        stub_status on;
        access_log off;
        allow 127.0.0.1;
        allow 172.16.0.0/12;  # Docker networks
        deny all;
    }
}
</code></pre>

            <h3>Análisis de Logs con Python</h3>

<pre><code>import re
import pandas as pd
from datetime import datetime

# Parsear logs de NGINX
log_pattern = r'(?P<ip>[\d\.]+) .* \[(?P<datetime>.*?)\] "(?P<method>\w+) (?P<path>.*?) .*?" (?P<status>\d+) .* rt=(?P<request_time>[\d\.]+) urt="(?P<upstream_time>[\d\.]+)"'

def parse_nginx_logs(log_file):
    logs = []
    with open(log_file) as f:
        for line in f:
            match = re.search(log_pattern, line)
            if match:
                logs.append(match.groupdict())
    
    df = pd.DataFrame(logs)
    df['request_time'] = df['request_time'].astype(float)
    df['upstream_time'] = df['upstream_time'].astype(float)
    
    return df

# Análisis
df = parse_nginx_logs('/var/log/nginx/access.log')

print("P95 de request_time:", df['request_time'].quantile(0.95))
print("P99 de request_time:", df['request_time'].quantile(0.99))
print("\nEndpoints más lentos:")
print(df.groupby('path')['request_time'].mean().sort_values(ascending=False).head())
</code></pre>

            <div class="exercise">
                <h4>Ejercicio 6: Performance Profiling</h4>
                <ol>
                    <li>Configura custom log format con timings</li>
                    <li>Crea un endpoint que haga <code>await asyncio.sleep(random.uniform(0.1, 2.0))</code></li>
                    <li>Genera 1000 requests</li>
                    <li>Analiza los logs: calcula P50, P95, P99</li>
                    <li>Identifica outliers (requests > 1.5s)</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 8 -->
        <div class="block">
            <h2>Bloque 8: Microservicios - Routing Avanzado</h2>
            
            <div class="question">
                Tienes 5 servicios diferentes (FastAPI, Streamlit, MLflow, Jupyter, Grafana). ¿5 dominios o 5 puertos?
            </div>

            <div class="alert-critical">
                Ni uno ni otro. Usa path-based routing en un solo dominio.
            </div>

            <h3>Arquitectura de Microservicios</h3>

<pre><code># docker-compose.yml
services:
  api:
    build: ./api
    expose: ["8000"]
  
  dashboard:
    image: streamlit/streamlit
    command: streamlit run app.py
    expose: ["8501"]
  
  mlflow:
    image: ghcr.io/mlflow/mlflow
    command: mlflow server --host 0.0.0.0
    expose: ["5000"]
  
  jupyter:
    image: jupyter/scipy-notebook
    expose: ["8888"]
  
  nginx:
    image: nginx:alpine
    ports: ["80:80"]
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
    depends_on:
      - api
      - dashboard
      - mlflow
      - jupyter
</code></pre>

            <h3>Routing por Path</h3>

<pre><code># nginx/conf.d/microservices.conf
upstream api_backend {
    server api:8000;
}

upstream dashboard_backend {
    server dashboard:8501;
}

upstream mlflow_backend {
    server mlflow:5000;
}

upstream jupyter_backend {
    server jupyter:8888;
}

server {
    listen 80;
    server_name ml-platform.com;
    
    # API de producción
    location /api/ {
        proxy_pass http://api_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    # Dashboard de visualización
    location /dashboard/ {
        proxy_pass http://dashboard_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # WebSocket support para Streamlit
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
    
    # MLflow tracking server
    location /mlflow/ {
        proxy_pass http://mlflow_backend/;
        proxy_set_header Host $host;
    }
    
    # Jupyter (protegido, solo red interna)
    location /jupyter/ {
        allow 192.168.1.0/24;
        deny all;
        
        proxy_pass http://jupyter_backend/;
        proxy_set_header Host $host;
        
        # WebSocket support
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
    
    # Frontend estático
    location / {
        root /usr/share/nginx/html;
        index index.html;
        try_files $uri $uri/ /index.html;
    }
}
</code></pre>

            <h3>Reescritura de URLs</h3>

            <div class="alert-warning">
                <strong>Problema:</strong> Si Streamlit genera URLs como <code>/static/main.js</code>, NGINX buscará en <code>/static</code>, no en <code>/dashboard/static</code>.
            </div>

<pre><code># Solución 1: Rewrite transparente
location /dashboard/ {
    rewrite ^/dashboard/(.*) /$1 break;
    proxy_pass http://dashboard_backend;
    proxy_set_header X-Script-Name /dashboard;  # Streamlit detecta el prefijo
}

# Solución 2: Configurar el servicio con base path
# En streamlit:
# streamlit run app.py --server.baseUrlPath=/dashboard
</code></pre>

            <h3>Routing por Subdominio</h3>

<pre><code># Alternativa: usar subdominios
server {
    listen 80;
    server_name api.ml-platform.com;
    
    location / {
        proxy_pass http://api_backend;
    }
}

server {
    listen 80;
    server_name dashboard.ml-platform.com;
    
    location / {
        proxy_pass http://dashboard_backend;
    }
}

server {
    listen 80;
    server_name mlflow.ml-platform.com;
    
    location / {
        proxy_pass http://mlflow_backend;
    }
}
</code></pre>

            <div class="comparison">
                <div class="comparison-item">
                    <h4>Path-based</h4>
                    <p><strong>Pros:</strong> Un solo certificado SSL, setup simple.</p>
                    <p><strong>Contras:</strong> Puede requerir ajustes en cada servicio.</p>
                </div>
                <div class="comparison-item">
                    <h4>Subdomain-based</h4>
                    <p><strong>Pros:</strong> Aislamiento limpio, no hay conflictos de paths.</p>
                    <p><strong>Contras:</strong> Certificado wildcard o múltiples certs, más DNS records.</p>
                </div>
            </div>

            <h3>API Gateway Pattern</h3>

<pre><code># Routing basado en versiones
location ~ ^/api/(v1|v2|v3)/(.*)$ {
    set $api_version $1;
    set $api_path $2;
    
    # Diferentes backends por versión
    proxy_pass http://api_$api_version:8000/$api_path$is_args$args;
}

# Routing basado en modelo
location ~ ^/predict/(model_a|model_b|model_c)$ {
    set $model_name $1;
    proxy_pass http://$model_name:8000/predict;
}
</code></pre>

            <div class="exercise">
                <h4>Ejercicio 7: Plataforma Multi-Servicio</h4>
                <ol>
                    <li>Despliega 3 servicios: FastAPI (puerto 8000), Streamlit (8501), y un servidor simple Python HTTP (8080)</li>
                    <li>Configura NGINX para:
                        <ul>
                            <li><code>/api/*</code> → FastAPI</li>
                            <li><code>/dashboard/*</code> → Streamlit</li>
                            <li><code>/*</code> → Python HTTP</li>
                        </ul>
                    </li>
                    <li>Verifica que cada ruta funciona correctamente</li>
                    <li>Bonus: Agrega autenticación básica solo para <code>/dashboard/</code></li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 9 -->
        <div class="block">
            <h2>Bloque 9: WebSockets y Streaming</h2>
            
            <div class="question">
                Tu modelo genera respuestas token-by-token (como GPT streaming). ¿Cómo lo sirves con NGINX?
            </div>

            <h3>El Problema del Buffering</h3>

            <div class="alert-warning">
                Por defecto, NGINX <strong>bufferiza</strong> la respuesta completa del backend antes de enviarla al cliente.
                <br><br>
                <strong>Problema:</strong> El streaming no funciona. El usuario ve toda la respuesta de golpe al final.
            </div>

            <h3>Configuración para SSE (Server-Sent Events)</h3>

<pre><code># FastAPI con streaming
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio

app = FastAPI()

async def generate_tokens():
    tokens = ["Hola", " mundo", " desde", " streaming", " de", " IA"]
    for token in tokens:
        yield f"data: {token}\n\n"
        await asyncio.sleep(0.5)

@app.get("/stream")
async def stream():
    return StreamingResponse(
        generate_tokens(),
        media_type="text/event-stream"
    )
</code></pre>

<pre><code># nginx/conf.d/streaming.conf
location /stream {
    proxy_pass http://fastapi_backend;
    
    # CRÍTICO: Desactiva buffering
    proxy_buffering off;
    proxy_cache off;
    
    # Headers para SSE
    proxy_set_header Connection '';
    proxy_http_version 1.1;
    chunked_transfer_encoding on;
    
    # Timeouts largos
    proxy_read_timeout 3600s;
    proxy_send_timeout 3600s;
}
</code></pre>

            <h3>WebSockets (bidireccional)</h3>

<pre><code># FastAPI con WebSocket
from fastapi import FastAPI, WebSocket

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    while True:
        data = await websocket.receive_text()
        response = process_with_model(data)
        await websocket.send_text(response)
</code></pre>

<pre><code># nginx/conf.d/websocket.conf
map $http_upgrade $connection_upgrade {
    default upgrade;
    ''      close;
}

server {
    listen 80;
    
    location /ws {
        proxy_pass http://fastapi_backend;
        
        # Headers obligatorios para WebSocket
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection $connection_upgrade;
        proxy_set_header Host $host;
        
        # Timeouts
        proxy_read_timeout 86400s;
        proxy_send_timeout 86400s;
    }
}
</code></pre>

            <h3>Caso Especial: gRPC Streaming</h3>

<pre><code>upstream grpc_backend {
    server grpc_server:50051;
}

server {
    listen 80 http2;
    
    location /grpc/ {
        grpc_pass grpc://grpc_backend;
        
        # Headers
        grpc_set_header X-Real-IP $remote_addr;
        
        # Timeouts para streams largos
        grpc_read_timeout 1h;
        grpc_send_timeout 1h;
    }
}
</code></pre>

            <h3>Streaming de Archivos Grandes</h3>

<pre><code># Servir datasets grandes sin consumir RAM
location /datasets/ {
    alias /data/datasets/;
    
    # Streaming eficiente con sendfile
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    
    # Limita velocidad de descarga
    limit_rate 5m;  # 5 MB/s por conexión
}
</code></pre>

            <div class="exercise">
                <h4>Ejercicio 8: Streaming en Vivo</h4>
                <ol>
                    <li>Crea un endpoint FastAPI que haga streaming de números del 1 al 100 con delay de 100ms</li>
                    <li>Configura NGINX con <code>proxy_buffering off</code></li>
                    <li>Desde el navegador, usa <code>fetch()</code> con <code>ReadableStream</code> para consumirlo</li>
                    <li>Verifica que ves los números aparecer uno a uno, no todos al final</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE 10 -->
        <div class="block">
            <h2>Bloque 10: Optimización y Tuning de Producción</h2>
            
            <div class="question">
                Tu NGINX está en producción sirviendo 10,000 req/s. ¿Qué ajustes críticos te faltan?
            </div>

            <h3>Worker Processes y Connections</h3>

<pre><code># nginx.conf
user nginx;

# Un worker por núcleo de CPU
worker_processes auto;

# Permite más de 1024 file descriptors (default de Linux)
worker_rlimit_nofile 65535;

events {
    # Máximo de conexiones concurrentes por worker
    # Total = worker_processes * worker_connections
    worker_connections 4096;
    
    # Método más eficiente (Linux)
    use epoll;
    
    # Acepta múltiples conexiones al mismo tiempo
    multi_accept on;
}
</code></pre>

            <div class="key-concept">
                <strong>Cálculo de capacidad:</strong>
                <br>
                Workers: 8 (CPU cores) × Connections: 4096 = <strong>32,768 conexiones simultáneas</strong>
                <br><br>
                Cada conexión consume ~2-3KB de RAM → ~100MB total
            </div>

            <h3>Optimizaciones de TCP</h3>

<pre><code>http {
    # Mantiene conexiones TCP abiertas para reutilizarlas
    keepalive_timeout 65;
    keepalive_requests 100;
    
    # Optimiza envío de archivos
    sendfile on;
    tcp_nopush on;    # Agrupa packets (menos overhead)
    tcp_nodelay on;   # Envía inmediatamente (baja latencia)
    
    # Compresión gzip
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml;
    gzip_comp_level 6;
    gzip_proxied any;
    
    # Timeouts
    client_body_timeout 12s;
    client_header_timeout 12s;
    send_timeout 10s;
    
    # Buffers
    client_body_buffer_size 16k;
    client_header_buffer_size 1k;
    client_max_body_size 100m;
    large_client_header_buffers 4 8k;
}
</code></pre>

            <h3>Open File Cache</h3>

<pre><code>http {
    # Cachea file descriptors de archivos estáticos
    open_file_cache max=10000 inactive=30s;
    open_file_cache_valid 60s;
    open_file_cache_min_uses 2;
    open_file_cache_errors on;
}
</code></pre>

            <div class="alert-info">
                <strong>Impacto:</strong> Reduce llamadas a <code>stat()</code> y <code>open()</code>. Mejora latencia en 20-30% para assets estáticos.
            </div>

            <h3>Kernel Tuning (Linux)</h3>

<pre><code># /etc/sysctl.conf
# Aumenta límite de conexiones
net.core.somaxconn = 65535
net.ipv4.tcp_max_syn_backlog = 8192

# Reutiliza sockets en TIME_WAIT
net.ipv4.tcp_tw_reuse = 1

# Optimiza buffers TCP
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216

# Aplica cambios
# sysctl -p
</code></pre>

            <h3>Monitoring de Recursos</h3>

<pre><code># Ver workers activos y sus estadísticas
ps aux | grep nginx

# Conexiones por estado
ss -s

# Monitorear en tiempo real
watch -n1 'echo "Active connections:"; ss -tan | grep ESTAB | wc -l'

# Logs de NGINX en tiempo real con filtros
tail -f /var/log/nginx/access.log | grep "HTTP/1.1\" 5"
</code></pre>

            <h3>Profiling con strace</h3>

<pre><code># Encuentra qué está haciendo un worker lento
ps aux | grep "nginx: worker"
strace -p <PID> -c  # Resumen de syscalls

# Output ejemplo:
# % time     seconds  usecs/call     calls    errors syscall
# ------ ----------- ----------- --------- --------- ----------------
#  45.23    0.012345          12      1024           epoll_wait
#  23.11    0.006321           6      1024           writev
#  ...
</code></pre>

            <h3>Checklist de Producción</h3>

            <table>
                <tr>
                    <th>Configuración</th>
                    <th>Desarrollo</th>
                    <th>Producción</th>
                </tr>
                <tr>
                    <td><code>worker_processes</code></td>
                    <td>1</td>
                    <td>auto (= CPU cores)</td>
                </tr>
                <tr>
                    <td><code>worker_connections</code></td>
                    <td>1024</td>
                    <td>4096+</td>
                </tr>
                <tr>
                    <td><code>gzip</code></td>
                    <td>off</td>
                    <td>on</td>
                </tr>
                <tr>
                    <td><code>access_log</code></td>
                    <td>on</td>
                    <td>buffer o async</td>
                </tr>
                <tr>
                    <td><code>open_file_cache</code></td>
                    <td>off</td>
                    <td>on</td>
                </tr>
                <tr>
                    <td><code>keepalive_timeout</code></td>
                    <td>65s</td>
                    <td>30-65s</td>
                </tr>
            </table>

            <div class="exercise">
                <h4>Ejercicio 9: Benchmark y Optimización</h4>
                <ol>
                    <li>Setup base: worker_processes=1, worker_connections=1024</li>
                    <li>Genera carga con: <code>ab -n 10000 -c 100 http://localhost/api/</code></li>
                    <li>Anota: requests/second, tiempo promedio, P95</li>
                    <li>Ajusta: worker_processes=auto, worker_connections=4096, gzip on</li>
                    <li>Re-ejecuta benchmark. ¿Cuánto mejoró?</li>
                </ol>
            </div>
        </div>

        <!-- BLOQUE FINAL -->
        <div class="block">
            <h2>Proyecto Final: Arquitectura Completa de Producción</h2>
            
            <div class="question">
                Integra todo lo aprendido en un despliegue real de microservicios de IA.
            </div>

            <h3>Arquitectura Objetivo</h3>

            <ul>
                <li><strong>Frontend:</strong> React/Vue servido como archivos estáticos</li>
                <li><strong>API Gateway (NGINX):</strong> Routing, SSL, rate limiting, caching</li>
                <li><strong>Backend:</strong> 3 réplicas de FastAPI para inferencia</li>
                <li><strong>Heavy Processing:</strong> Servicio separado para entrenamiento (rate limit bajo)</li>
                <li><strong>Monitoring:</strong> Grafana dashboard con métricas de NGINX</li>
            </ul>

            <h3>Requisitos del Proyecto</h3>

            <ol>
                <li><strong>SSL/TLS:</strong> HTTPS con Let's Encrypt o certificado auto-firmado</li>
                <li><strong>Load Balancing:</strong> Mínimo 3 réplicas de FastAPI con health checks</li>
                <li><strong>Rate Limiting:</strong> 
                    <ul>
                        <li><code>/api/predict</code> → 10 req/s</li>
                        <li><code>/api/train</code> → 1 req/min</li>
                    </ul>
                </li>
                <li><strong>Caching:</strong> Predicciones cacheadas por 5 minutos</li>
                <li><strong>Logging:</strong> Custom format con timings, exportado a Prometheus</li>
                <li><strong>Security:</strong> CORS configurado, headers de seguridad, no exponer errores internos</li>
            </ol>

            <h3>Estructura de Carpetas</h3>

<pre><code>proyecto-final/
├── docker-compose.yml
├── nginx/
│   ├── nginx.conf
│   └── conf.d/
│       ├── ssl.conf
│       ├── api.conf
│       └── monitoring.conf
├── api/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── main.py
├── frontend/
│   └── dist/  # Build del frontend
└── certbot/
    ├── conf/
    └── www/
</code></pre>

            <h3>Métricas de Éxito</h3>

            <ul>
                <li>Soporta 1000+ req/s sin errores</li>
                <li>P95 de latencia < 100ms (endpoints sin IA)</li>
                <li>Failover automático si cae un backend</li>
                <li>Cache hit rate > 40%</li>
                <li>Rate limiting funcionando (devuelve 429)</li>
            </ul>

            <div class="alert-info">
                <strong>Entregables:</strong>
                <ul>
                    <li>Repositorio con toda la configuración</li>
                    <li>README con instrucciones de despliegue</li>
                    <li>Screenshots de Grafana mostrando métricas</li>
                    <li>Reporte de benchmarks (ab o locust)</li>
                </ul>
            </div>
        </div>

        <div style="margin-top: 60px; padding: 30px; background: #1e293b; border-radius: 12px; text-align: center; border: 2px solid #3b82f6;">
            <h2 style="color: #60a5fa; margin-bottom: 20px;">🎯 From Zero to Hero Completado</h2>
            <p style="font-size: 1.2em; color: #94a3b8;">
                Has dominado NGINX desde conceptos básicos hasta arquitecturas de producción para IA y Big Data.
                <br><br>
                <strong style="color: #fbbf24;">Próximo paso:</strong> Aplica estos conocimientos en tu pipeline de NoSQL, PySpark y Golang.
            </p>
        </div>
    </div>
</body>
</html>
