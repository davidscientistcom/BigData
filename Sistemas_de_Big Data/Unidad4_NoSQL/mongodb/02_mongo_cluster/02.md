## **2. Fundamentos de Arquitectura Distribuida**

### **2.1. Replica Sets: Alta disponibilidad y redundancia**

Un Replica Set es el mecanismo fundamental que MongoDB utiliza para proporcionar alta disponibilidad y redundancia de datos. Este concepto es esencial para cualquier sistema en producción, ya que garantiza que nuestra base de datos pueda seguir funcionando incluso cuando uno o más servidores fallen. [mongodb](https://www.mongodb.com/docs/v2.6/administration/)

Un Replica Set está compuesto por un mínimo de tres nodos, aunque en producción es común encontrar configuraciones de 5, 7 o más nodos según los requisitos de disponibilidad y distribución geográfica. De estos nodos, uno actúa como **primario (PRIMARY)** y los demás como **secundarios (SECONDARY)**. El nodo primario es el único que acepta operaciones de escritura, mientras que los secundarios replican continuamente los datos del primario para mantener copias exactas.

**El mecanismo de replicación** funciona mediante el **oplog** (operations log), una colección especial llamada `local.oplog.rs` que registra todas las operaciones de escritura en orden cronológico. Los nodos secundarios leen constantemente el oplog del primario y aplican las mismas operaciones en sus propios datos. Este proceso es asíncrono, lo que significa que puede existir un pequeño retraso (lag) entre el primario y los secundarios, aunque en condiciones normales de red este retraso es de milisegundos.

**Ejemplo de configuración básica de un Replica Set:**

Imaginemos que tenemos tres servidores: `mongo1.ejemplo.com:27017`, `mongo2.ejemplo.com:27017` y `mongo3.ejemplo.com:27017`. Para configurar un Replica Set, primero iniciamos cada instancia mongod con el parámetro `--replSet`:

```bash
# En mongo1
mongod --replSet "rsProduccion" --dbpath /data/db --bind_ip localhost,mongo1.ejemplo.com

# En mongo2
mongod --replSet "rsProduccion" --dbpath /data/db --bind_ip localhost,mongo2.ejemplo.com

# En mongo3
mongod --replSet "rsProduccion" --dbpath /data/db --bind_ip localhost,mongo3.ejemplo.com
```

Una vez iniciadas las instancias, nos conectamos a una de ellas (por ejemplo, mongo1) y ejecutamos el comando de inicialización:

```javascript
rs.initiate({
  _id: "rsProduccion",
  members: [
    { _id: 0, host: "mongo1.ejemplo.com:27017", priority: 2 },
    { _id: 1, host: "mongo2.ejemplo.com:27017", priority: 1 },
    { _id: 2, host: "mongo3.ejemplo.com:27017", priority: 1 }
  ]
})
```

En este ejemplo, hemos configurado `mongo1` con una prioridad mayor (2), lo que significa que en caso de elección siempre será el candidato preferido para convertirse en primario. Las prioridades van de 0 a 1000, donde 0 indica que el nodo nunca puede ser primario (útil para nodos de análisis o backup).

**El proceso de elección (election)** es uno de los aspectos más importantes de los Replica Sets. Cuando el nodo primario falla o se vuelve inaccesible, los nodos secundarios detectan la pérdida de conexión mediante heartbeats (latidos) que se envían cada 2 segundos. Si un secundario no recibe respuesta del primario durante 10 segundos, inicia un proceso de elección. Para que un nodo sea elegido como nuevo primario, debe obtener la mayoría de votos de los miembros del Replica Set.

**Tipos de nodos en un Replica Set:**

- **Nodos con datos (data-bearing)**: Almacenan una copia completa de los datos y pueden convertirse en primarios.
- **Árbitros (arbiters)**: Nodos ligeros que no almacenan datos pero participan en las elecciones. Se usan para desempatar cuando hay un número par de nodos. Sin embargo, no se recomiendan en producción porque no añaden redundancia de datos.
- **Nodos de solo lectura (priority 0)**: Nodos que nunca pueden ser primarios pero sí contienen datos y pueden servir lecturas.
- **Nodos ocultos (hidden)**: Invisibles para las aplicaciones, se usan típicamente para análisis o backups sin afectar el tráfico de producción.
- **Nodos con retraso (delayed)**: Mantienen una versión de los datos retrasada en el tiempo, útiles para recuperación ante errores humanos (por ejemplo, un borrado accidental).

**Ejemplo de configuración de un nodo retrasado:**

```javascript
cfg = rs.conf()
cfg.members [geeksforgeeks](https://www.geeksforgeeks.org/mongodb/convert-a-replica-set-to-a-sharded-cluster-in-mongodb/).priority = 0
cfg.members [geeksforgeeks](https://www.geeksforgeeks.org/mongodb/convert-a-replica-set-to-a-sharded-cluster-in-mongodb/).hidden = true
cfg.members [geeksforgeeks](https://www.geeksforgeeks.org/mongodb/convert-a-replica-set-to-a-sharded-cluster-in-mongodb/).slaveDelay = 3600  // 1 hora de retraso
rs.reconfig(cfg)
```

Este nodo mantendrá los datos con una hora de retraso, permitiéndonos recuperar información si alguien borra datos accidentalmente y no nos damos cuenta inmediatamente.

### **2.2. Sharding: Escalado horizontal**

El sharding es la técnica que MongoDB utiliza para distribuir datos horizontalmente entre múltiples máquinas. Mientras que un Replica Set proporciona redundancia y disponibilidad, el sharding proporciona capacidad de procesamiento y almacenamiento prácticamente ilimitada al dividir los datos en fragmentos (shards) que se distribuyen entre diferentes servidores. [geeksforgeeks](https://www.geeksforgeeks.org/mongodb/convert-a-replica-set-to-a-sharded-cluster-in-mongodb/)

El concepto fundamental del sharding es la **shard key** (clave de fragmentación), que determina cómo se distribuyen los documentos entre los diferentes shards. MongoDB divide el rango de valores de la shard key en **chunks** (fragmentos lógicos), cada uno con un tamaño máximo de 64MB por defecto. Estos chunks se distribuyen entre los shards disponibles, y el **balanceador** se encarga de mover chunks automáticamente cuando detecta que un shard tiene significativamente más datos que otros.

**¿Cuándo necesitamos sharding?** No todos los despliegues de MongoDB requieren sharding. Esta técnica introduce complejidad operacional y solo debe implementarse cuando:

1. El conjunto de datos supera la capacidad de almacenamiento de un único servidor
2. El volumen de operaciones (throughput) excede la capacidad de I/O de una única máquina
3. Necesitamos aislar geográficamente los datos (por ejemplo, datos europeos en Europa, datos americanos en América)
4. El working set (datos activos en memoria) supera la RAM disponible en un servidor

**Tipos de estrategias de sharding:**

**Range-based sharding (por rangos)**: Los documentos se distribuyen según rangos de valores de la shard key. Por ejemplo, si usamos el campo `fecha` como shard key, MongoDB podría asignar documentos de enero a marzo al shard1, de abril a junio al shard2, etc.

```javascript
// Ejemplo: colección de sensores de temperatura
{
  _id: ObjectId("..."),
  sensor_id: "TEMP001",
  timestamp: ISODate("2026-02-09T16:00:00Z"),
  temperatura: 22.5,
  ubicacion: "Valencia"
}

// Shard key: timestamp
sh.shardCollection("monitorizacion.sensores", { timestamp: 1 })
```

Esta estrategia es eficiente para consultas por rangos, pero puede causar **hotspots** (puntos calientes) si los datos se insertan cronológicamente, ya que todas las escrituras nuevas irían al mismo shard (el que contiene el rango más reciente).

**Hash-based sharding (por hash)**: MongoDB calcula un hash de la shard key y distribuye los documentos basándose en ese hash. Esto garantiza una distribución uniforme pero imposibilita las consultas eficientes por rangos.

```javascript
// Misma colección, pero con hash sharding
sh.shardCollection("monitorizacion.sensores", { sensor_id: "hashed" })
```

Con esta configuración, documentos con `sensor_id` consecutivos se distribuirán aleatoriamente entre los shards, evitando hotspots pero haciendo que consultas como "todos los sensores del TEMP001 al TEMP100" requieran consultar todos los shards.

**Compound shard keys (claves compuestas)**: Combinan múltiples campos para lograr mejor distribución y selectividad en consultas.

```javascript
sh.shardCollection("monitorizacion.sensores", 
  { ubicacion: 1, timestamp: 1 })
```

Esta estrategia distribuye los datos primero por ubicación y luego por fecha dentro de cada ubicación, permitiendo consultas eficientes por ubicación mientras se mantiene cierta distribución temporal.

### **2.3. Diferencias entre escalado vertical y horizontal**

El **escalado vertical (scale-up)** consiste en añadir más recursos (CPU, RAM, almacenamiento más rápido) a un servidor existente. Por ejemplo, pasar de un servidor con 16GB de RAM a uno con 64GB, o añadir más núcleos de CPU. Esta estrategia tiene límites físicos y económicos evidentes: existe un máximo de recursos que se pueden añadir a una única máquina, y el coste crece exponencialmente a medida que nos acercamos al hardware de gama más alta.

El **escalado horizontal (scale-out)** consiste en añadir más servidores al sistema, distribuyendo la carga entre ellos. En lugar de un servidor potente, tenemos muchos servidores de capacidad media trabajando en conjunto. MongoDB está diseñado específicamente para este modelo mediante sharding. [geeksforgeeks](https://www.geeksforgeeks.org/mongodb/convert-a-replica-set-to-a-sharded-cluster-in-mongodb/)

**Comparativa práctica:**

| Aspecto | Escalado Vertical | Escalado Horizontal |
|---------|-------------------|---------------------|
| Límite de capacidad | Limitado por hardware disponible | Prácticamente ilimitado |
| Coste | Crecimiento exponencial | Crecimiento lineal  [geeksforgeeks](https://www.geeksforgeeks.org/mongodb/convert-a-replica-set-to-a-sharded-cluster-in-mongodb/) |
| Disponibilidad | Punto único de fallo | Alta disponibilidad nativa |
| Complejidad | Baja (un solo servidor) | Alta (coordinación distribuida) |
| Latencia de red | No aplicable | Posible overhead de red |
| Mantenimiento | Requiere downtime | Puede ser sin downtime |

**Ejemplo real de decisión de escalado:**

Supongamos que tenemos una aplicación de e-commerce con 100,000 productos y 1 millón de pedidos anuales. Inicialmente, un servidor con 32GB RAM y 8 CPU cores maneja la carga sin problemas. Después de una campaña de marketing exitosa, las ventas se multiplican por 10: ahora tenemos 10 millones de pedidos anuales y los picos de tráfico saturan el servidor.

**Opción vertical**: Actualizar a un servidor con 128GB RAM y 32 cores. Coste: ~€5,000/mes. Límite: cuando volvamos a crecer, no hay más espacio.

**Opción horizontal**: Implementar un sharded cluster con 4 shards (cada uno con 32GB RAM y 8 cores). Coste inicial similar, pero podemos añadir más shards según necesitemos. Si el tráfico se duplica, añadimos 4 shards más. Esta es la estrategia que MongoDB favorece.

### **2.4. Config Servers y mongos (query routers)**

En una arquitectura con sharding, dos componentes adicionales son esenciales: los **Config Servers** y los **mongos** (routers de consultas). [mongodb](https://www.mongodb.com/docs/manual/reference/command/nav-administration/)

**Config Servers** son un Replica Set especial que almacena todos los metadatos del cluster. Estos metadatos incluyen:

- Qué bases de datos y colecciones están fragmentadas (sharded)
- Cuál es la shard key de cada colección fragmentada
- Qué rangos de datos (chunks) están en qué shard
- El historial de migraciones de chunks
- La configuración de cada shard

Los Config Servers deben ser **extremadamente fiables**, ya que sin ellos el cluster no puede funcionar. Por este motivo, MongoDB requiere que los Config Servers sean un Replica Set de al menos 3 nodos (recomendado 3 o 5 en producción). Si todos los Config Servers fallan simultáneamente, el cluster queda inoperativo aunque los shards individuales sigan funcionando.

**Ejemplo de inicio de Config Servers:**

```bash
# Config Server 1
mongod --configsvr --replSet configRS --dbpath /data/configdb1 \
  --bind_ip localhost,config1.ejemplo.com --port 27019

# Config Server 2
mongod --configsvr --replSet configRS --dbpath /data/configdb2 \
  --bind_ip localhost,config2.ejemplo.com --port 27019

# Config Server 3
mongod --configsvr --replSet configRS --dbpath /data/configdb3 \
  --bind_ip localhost,config3.ejemplo.com --port 27019
```

Nota el flag `--configsvr` que indica que esta instancia es un Config Server, y el puerto 27019 (por convención, aunque puede ser cualquiera).

**Mongos** es un proceso ligero que actúa como router de consultas. No almacena datos, sino que recibe las peticiones de las aplicaciones cliente, consulta los Config Servers para determinar qué shards contienen los datos relevantes, y enruta las operaciones apropiadamente. Si una consulta requiere datos de múltiples shards, mongos ejecuta la consulta en paralelo en todos los shards relevantes y combina los resultados antes de devolverlos al cliente.

**Flujo de una operación de escritura a través de mongos:**

```
1. Aplicación → mongos: db.pedidos.insertOne({cliente: "C123", total: 150})
2. Mongos consulta Config Servers: ¿Cuál es la shard key de 'pedidos'?
3. Config Servers responden: shard key es 'cliente'
4. Mongos calcula: hash('C123') → pertenece al rango del shard2
5. Mongos → shard2: ejecuta la inserción
6. Shard2 → mongos: operación completada
7. Mongos → aplicación: documento insertado exitosamente
```

**Flujo de una consulta que requiere múltiples shards:**

```javascript
// Consulta: encontrar todos los pedidos con total > 100
db.pedidos.find({ total: { $gt: 100 } })
```

Si la shard key es `cliente` y estamos buscando por `total`, mongos debe consultar **todos los shards** (broadcast query):

```
1. Aplicación → mongos: find({ total: { $gt: 100 } })
2. Mongos detecta: 'total' no es parte de la shard key
3. Mongos → todos los shards: ejecuta la consulta en paralelo
4. Shard1, shard2, shard3 → mongos: envían resultados parciales
5. Mongos: combina y ordena resultados
6. Mongos → aplicación: resultado completo
```

Este tipo de consultas son menos eficientes porque requieren tocar todos los shards. Por eso, el diseño de la shard key debe considerar los patrones de consulta más frecuentes de la aplicación.

**Despliegue de mongos:**

Los procesos mongos son stateless (sin estado), lo que significa que se pueden ejecutar múltiples instancias sin coordinación entre ellas. De hecho, es una buena práctica ejecutar al menos dos mongos para alta disponibilidad, y es común ejecutar mongos en los mismos servidores donde se ejecutan las aplicaciones.

```bash
# Mongos 1
mongos --configdb configRS/config1.ejemplo.com:27019,config2.ejemplo.com:27019,config3.ejemplo.com:27019 \
  --bind_ip localhost,app1.ejemplo.com --port 27017

# Mongos 2 (en otro servidor)
mongos --configdb configRS/config1.ejemplo.com:27019,config2.ejemplo.com:27019,config3.ejemplo.com:27019 \
  --bind_ip localhost,app2.ejemplo.com --port 27017
```

Las aplicaciones se conectan a mongos exactamente como se conectarían a un mongod standalone o a un Replica Set. La distribución y el sharding son completamente transparentes para el código de la aplicación, que simplemente ejecuta consultas normales sin necesidad de saber en qué shard están los datos.

Esta arquitectura distribuida, aunque compleja, proporciona la base para sistemas MongoDB que pueden escalar a petabytes de datos y millones de operaciones por segundo, manteniendo alta disponibilidad y rendimiento consistente. El administrador debe comprender estos componentes y cómo interactúan para diseñar y mantener clusters robustos.

