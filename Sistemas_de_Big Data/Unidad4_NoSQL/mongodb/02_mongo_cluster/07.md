## **7. Ejemplo Práctico Completo: Despliegue de un Cluster con Escalado Horizontal**

### **7.1. Escenario y requisitos**

Vamos a desplegar un cluster MongoDB con sharding completo desde cero para una aplicación de comercio electrónico que está experimentando crecimiento rápido. Este ejemplo será paso a paso, con todos los comandos necesarios para un entorno de producción. [phoenixnap](https://phoenixnap.com/kb/mongodb-sharding)

**Requisitos del negocio:**

Nuestra plataforma de e-commerce actualmente tiene:
- 800GB de datos de productos, pedidos y usuarios
- 25,000 operaciones por segundo en hora pico
- Proyección de crecimiento: 200GB adicionales cada trimestre
- Necesidad de alta disponibilidad (99.95% uptime)
- Distribución geográfica: dos datacenters

**Arquitectura objetivo:**

Diseñaremos un cluster con los siguientes componentes: [mongodb](https://www.mongodb.com/docs/manual/core/sharded-cluster-components/)

- **3 Config Servers** (Replica Set): almacenan metadatos del cluster
- **3 Shards** (cada uno un Replica Set de 3 nodos): 9 servidores total para datos
- **2 Mongos routers**: puntos de entrada para aplicaciones

**Total: 14 servidores** (3 config + 9 shard nodes + 2 mongos)

**Inventario de servidores:**

```
Config Servers (Replica Set "configRS"):
  - config1.ejemplo.com  (192.168.1.11)
  - config2.ejemplo.com  (192.168.1.12)
  - config3.ejemplo.com  (192.168.1.13)

Shard 1 (Replica Set "shard01RS"):
  - shard01-a.ejemplo.com  (192.168.1.21) - Primary preferido
  - shard01-b.ejemplo.com  (192.168.1.22) - Secondary
  - shard01-c.ejemplo.com  (192.168.1.23) - Secondary

Shard 2 (Replica Set "shard02RS"):
  - shard02-a.ejemplo.com  (192.168.1.31) - Primary preferido
  - shard02-b.ejemplo.com  (192.168.1.32) - Secondary
  - shard02-c.ejemplo.com  (192.168.1.33) - Secondary

Shard 3 (Replica Set "shard03RS"):
  - shard03-a.ejemplo.com  (192.168.1.41) - Primary preferido
  - shard03-b.ejemplo.com  (192.168.1.42) - Secondary
  - shard03-c.ejemplo.com  (192.168.1.43) - Secondary

Mongos Routers:
  - mongos1.ejemplo.com  (192.168.1.51)
  - mongos2.ejemplo.com  (192.168.1.52)
```

**Preparación previa:**

En todos los servidores, asumimos que:
- Sistema operativo: Ubuntu 22.04 LTS
- MongoDB 7.0 instalado
- Firewall configurado para permitir tráfico entre nodos
- DNS o /etc/hosts configurado con todos los nombres de host

### **7.2. Paso 1: Configuración de Config Servers**

Los Config Servers son el cerebro del cluster sharded, almacenando todos los metadatos. Deben configurarse primero y ser extremadamente fiables. [silicloud](https://www.silicloud.com/blog/how-to-deploy-a-sharded-mongodb-cluster/)

**En config1.ejemplo.com, crear `/etc/mongod-config.conf`:**

```yaml
# Configuración para Config Server 1
storage:
  dbPath: /data/configdb
  journal:
    enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 2  # Config servers no necesitan mucha RAM

systemLog:
  destination: file
  path: /var/log/mongodb/mongod-config.log
  logAppend: true
  logRotate: reopen
  timeStampFormat: iso8601-utc

net:
  port: 27019  # Puerto estándar para config servers
  bindIp: 0.0.0.0
  maxIncomingConnections: 1000

security:
  keyFile: /etc/mongodb/keyfile

replication:
  replSetName: configRS

sharding:
  clusterRole: configsvr  # IMPORTANTE: identifica como config server

processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongod-config.pid
```

**Replicar esta configuración en config2 y config3** (ajustando paths si es necesario).

**Generar keyfile para autenticación (solo en config1, luego copiar):**

```bash
# En config1
sudo mkdir -p /etc/mongodb
sudo openssl rand -base64 756 > /etc/mongodb/keyfile
sudo chmod 400 /etc/mongodb/keyfile
sudo chown mongodb:mongodb /etc/mongodb/keyfile

# Copiar a los demás config servers
for host in config2.ejemplo.com config3.ejemplo.com; do
  sudo scp /etc/mongodb/keyfile $host:/etc/mongodb/
  ssh $host "sudo chmod 400 /etc/mongodb/keyfile && sudo chown mongodb:mongodb /etc/mongodb/keyfile"
done
```

**Iniciar mongod en los tres config servers:**

```bash
# En cada config server (config1, config2, config3)
sudo mkdir -p /data/configdb
sudo chown mongodb:mongodb /data/configdb
sudo mongod -f /etc/mongod-config.conf
```

**Verificar que están corriendo:**

```bash
# En cada uno
ps aux | grep mongod
# Debería mostrar el proceso mongod con --configsvr
```

**Inicializar el Replica Set de Config Servers:**

Conectarse a config1 (antes de habilitar autenticación):

```bash
mongosh --host config1.ejemplo.com --port 27019
```

Dentro del shell:

```javascript
// Inicializar el Replica Set
rs.initiate({
  _id: "configRS",
  configsvr: true,  // CRÍTICO: marcar como config server RS
  members: [
    { _id: 0, host: "config1.ejemplo.com:27019" },
    { _id: 1, host: "config2.ejemplo.com:27019" },
    { _id: 2, host: "config3.ejemplo.com:27019" }
  ]
})

// Esperar a que se complete la elección (30 segundos aprox)
// Verificar estado
rs.status()

// Debería mostrar:
// - Un PRIMARY
// - Dos SECONDARY
// - state: 1 (PRIMARY) o 2 (SECONDARY) para todos
```

**Crear usuario administrador (ANTES de que otros componentes se conecten):**

```javascript
// En el PRIMARY del configRS
use admin
db.createUser({
  user: "clusterAdmin",
  pwd: "SuperSecurePassword123!",
  roles: [
    { role: "root", db: "admin" }
  ]
})

// Verificar
db.auth("clusterAdmin", "SuperSecurePassword123!")
```

Ahora los Config Servers están listos. **No reinicies los procesos todavía**, ya que necesitamos mantenerlos corriendo para el siguiente paso.

### **7.3. Paso 2: Configuración de Shards (Replica Sets)**

Cada shard será un Replica Set completo de 3 nodos. Configuraremos los tres shards en paralelo. [mongodb](https://www.mongodb.com/docs/manual/tutorial/add-shards-to-shard-cluster/)

**Configuración para Shard 1:**

**En shard01-a.ejemplo.com, crear `/etc/mongod-shard01.conf`:**

```yaml
storage:
  dbPath: /data/shard01
  journal:
    enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 24  # Ajustar según RAM disponible (50% RAM típicamente)
    collectionConfig:
      blockCompressor: zstd  # Mejor compresión

systemLog:
  destination: file
  path: /var/log/mongodb/mongod-shard01.log
  logAppend: true
  logRotate: reopen
  timeStampFormat: iso8601-utc

net:
  port: 27018  # Puerto estándar para shards
  bindIp: 0.0.0.0
  maxIncomingConnections: 10000

security:
  keyFile: /etc/mongodb/keyfile

replication:
  replSetName: shard01RS
  oplogSizeMB: 10240  # 10GB de oplog

sharding:
  clusterRole: shardsvr  # IMPORTANTE: identifica como shard

processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongod-shard01.pid

operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 100
```

**Replicar configuración en shard01-b y shard01-c** (mismo contenido).

**Distribuir keyfile a todos los nodos de shard:**

```bash
# Desde config1 (donde ya tenemos el keyfile)
for host in shard01-a.ejemplo.com shard01-b.ejemplo.com shard01-c.ejemplo.com \
            shard02-a.ejemplo.com shard02-b.ejemplo.com shard02-c.ejemplo.com \
            shard03-a.ejemplo.com shard03-b.ejemplo.com shard03-c.ejemplo.com; do
  sudo scp /etc/mongodb/keyfile $host:/etc/mongodb/
  ssh $host "sudo chmod 400 /etc/mongodb/keyfile && sudo chown mongodb:mongodb /etc/mongodb/keyfile"
done
```

**Iniciar mongod en todos los nodos de shard01:**

```bash
# En shard01-a, shard01-b, shard01-c
sudo mkdir -p /data/shard01
sudo chown mongodb:mongodb /data/shard01
sudo mongod -f /etc/mongod-shard01.conf
```

**Inicializar Replica Set shard01RS:**

```bash
mongosh --host shard01-a.ejemplo.com --port 27018
```

```javascript
// Inicializar
rs.initiate({
  _id: "shard01RS",
  members: [
    { _id: 0, host: "shard01-a.ejemplo.com:27018", priority: 2 },
    { _id: 1, host: "shard01-b.ejemplo.com:27018", priority: 1 },
    { _id: 2, host: "shard01-c.ejemplo.com:27018", priority: 1 }
  ]
})

// Crear usuario administrador local
use admin
db.createUser({
  user: "shardAdmin",
  pwd: "ShardPassword123!",
  roles: [ { role: "root", db: "admin" } ]
})
```

**Repetir exactamente el mismo proceso para Shard 2 y Shard 3:**

**Shard 2:**

```bash
# En cada nodo: shard02-a, shard02-b, shard02-c
# Usar /etc/mongod-shard02.conf con replSetName: shard02RS
# dbPath: /data/shard02
# Inicializar con rs.initiate usando shard02RS
```

**Shard 3:**

```bash
# En cada nodo: shard03-a, shard03-b, shard03-c
# Usar /etc/mongod-shard03.conf con replSetName: shard03RS
# dbPath: /data/shard03
# Inicializar con rs.initiate usando shard03RS
```

**Script de verificación de todos los shards:**

```bash
#!/bin/bash
# verify-shards.sh

echo "=== Verificando Shard 1 ==="
mongosh --host shard01-a.ejemplo.com --port 27018 \
  --username shardAdmin --password ShardPassword123! \
  --authenticationDatabase admin \
  --eval "rs.status().members.forEach(m => print(m.name + ': ' + m.stateStr))"

echo "=== Verificando Shard 2 ==="
mongosh --host shard02-a.ejemplo.com --port 27018 \
  --username shardAdmin --password ShardPassword123! \
  --authenticationDatabase admin \
  --eval "rs.status().members.forEach(m => print(m.name + ': ' + m.stateStr))"

echo "=== Verificando Shard 3 ==="
mongosh --host shard03-a.ejemplo.com --port 27018 \
  --username shardAdmin --password ShardPassword123! \
  --authenticationDatabase admin \
  --eval "rs.status().members.forEach(m => print(m.name + ': ' + m.stateStr))"
```

**Salida esperada:**
```
=== Verificando Shard 1 ===
shard01-a.ejemplo.com:27018: PRIMARY
shard01-b.ejemplo.com:27018: SECONDARY
shard01-c.ejemplo.com:27018: SECONDARY

=== Verificando Shard 2 ===
shard02-a.ejemplo.com:27018: PRIMARY
shard02-b.ejemplo.com:27018: SECONDARY
shard02-c.ejemplo.com:27018: SECONDARY

=== Verificando Shard 3 ===
shard03-a.ejemplo.com:27018: PRIMARY
shard03-b.ejemplo.com:27018: SECONDARY
shard03-c.ejemplo.com:27018: SECONDARY
```

### **7.4. Paso 3: Configuración de mongos (Query Routers)**

Los procesos mongos son routers ligeros que no almacenan datos. Son stateless, lo que significa que podemos ejecutar varios sin coordinación. [mongodb](https://www.mongodb.com/docs/manual/reference/program/mongos/)

**En mongos1.ejemplo.com, crear `/etc/mongos.conf`:**

```yaml
# Configuración de Mongos Router 1
systemLog:
  destination: file
  path: /var/log/mongodb/mongos.log
  logAppend: true
  logRotate: reopen
  timeStampFormat: iso8601-utc

net:
  port: 27017  # Puerto estándar (aplicaciones se conectan aquí)
  bindIp: 0.0.0.0
  maxIncomingConnections: 20000  # Mongos puede manejar muchas conexiones

security:
  keyFile: /etc/mongodb/keyfile

sharding:
  configDB: configRS/config1.ejemplo.com:27019,config2.ejemplo.com:27019,config3.ejemplo.com:27019

processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongos.pid
```

**Copiar keyfile a mongos1 y mongos2:**

```bash
# Desde config1
for host in mongos1.ejemplo.com mongos2.ejemplo.com; do
  sudo scp /etc/mongodb/keyfile $host:/etc/mongodb/
  ssh $host "sudo chmod 400 /etc/mongodb/keyfile && sudo chown mongodb:mongodb /etc/mongodb/keyfile"
done
```

**Replicar configuración en mongos2.ejemplo.com** (mismo contenido).

**Iniciar mongos en ambos routers:**

```bash
# En mongos1 y mongos2
sudo mongos -f /etc/mongos.conf

# Verificar
ps aux | grep mongos
# Debería mostrar proceso mongos (NO mongod)

# Ver logs para confirmar conexión a config servers
sudo tail -f /var/log/mongodb/mongos.log
# Deberías ver: "Successfully connected to config server"
```

**Conectarse a mongos y crear usuario de cluster:**

```bash
mongosh --host mongos1.ejemplo.com --port 27017
```

```javascript
// Autenticar con el usuario del config server
use admin
db.auth("clusterAdmin", "SuperSecurePassword123!")

// Crear usuario específico para operaciones de sharding
db.createUser({
  user: "shardingAdmin",
  pwd: "ShardingPass123!",
  roles: [
    { role: "clusterAdmin", db: "admin" },
    { role: "userAdminAnyDatabase", db: "admin" },
    { role: "readWriteAnyDatabase", db: "admin" }
  ]
})

// Verificar conexión al cluster
sh.status()
// Por ahora mostrará: "shards: []" (sin shards todavía)
```

### **7.5. Paso 4: Añadir shards al cluster**

Ahora conectamos los shards al cluster a través de mongos. [mongodb](https://www.mongodb.com/docs/manual/tutorial/add-shards-to-shard-cluster/)

```bash
# Conectar a mongos con usuario autorizado
mongosh --host mongos1.ejemplo.com --port 27017 \
  --username shardingAdmin --password ShardingPass123! \
  --authenticationDatabase admin
```

```javascript
// Añadir Shard 1
sh.addShard("shard01RS/shard01-a.ejemplo.com:27018,shard01-b.ejemplo.com:27018,shard01-c.ejemplo.com:27018")

// Salida esperada:
{
  "shardAdded": "shard01RS",
  "ok": 1,
  "$clusterTime": { /* ... */ }
}

// Añadir Shard 2
sh.addShard("shard02RS/shard02-a.ejemplo.com:27018,shard02-b.ejemplo.com:27018,shard02-c.ejemplo.com:27018")

// Añadir Shard 3
sh.addShard("shard03RS/shard03-a.ejemplo.com:27018,shard03-b.ejemplo.com:27018,shard03-c.ejemplo.com:27018")

// Verificar que todos los shards están añadidos
sh.status()
```

**Salida esperada de `sh.status()`:**

```javascript
--- Sharding Status ---
  sharding version: {
    "_id": 1,
    "minCompatibleVersion": 5,
    "currentVersion": 6,
    "clusterId": ObjectId("...")
  }
  shards:
    {  "_id": "shard01RS", "host": "shard01RS/shard01-a.ejemplo.com:27018,...", "state": 1 }
    {  "_id": "shard02RS", "host": "shard02RS/shard02-a.ejemplo.com:27018,...", "state": 1 }
    {  "_id": "shard03RS", "host": "shard03-a.ejemplo.com:27018,...", "state": 1 }
  active mongoses:
    "7.0.5": 2  // Nuestros dos mongos routers
  autosplit:
    Currently enabled: yes
  balancer:
    Currently enabled: yes
    Currently running: no
    Failed balancer rounds in last 5 attempts: 0
    Migration results for the last 24 hours:
      No recent migrations
  databases:
    {  "_id": "config", "primary": "config", "partitioned": true }
```

¡Perfecto! El cluster está ensamblado. Ahora tenemos 3 shards disponibles pero aún no hay datos fragmentados.

### **7.6. Paso 5: Habilitar sharding en bases de datos y colecciones**

Ahora configuraremos nuestra base de datos de e-commerce para usar sharding. [devops.aibit](https://devops.aibit.im/article/guide-deploying-basic-mongodb-sharded-cluster)

```javascript
// Conectados a mongos1 como shardingAdmin

// Habilitar sharding en la base de datos "ecommerce"
sh.enableSharding("ecommerce")

// Verificar
db.adminCommand({ listDatabases: 1 })
// "ecommerce" aparecerá con "partitioned": true
```

**Crear índices y fragmentar colección de pedidos:**

```javascript
use ecommerce

// Crear usuario para la aplicación
db.createUser({
  user: "appEcommerce",
  pwd: "AppPass123!",
  roles: [
    { role: "readWrite", db: "ecommerce" }
  ]
})

// Colección: pedidos
// Patrón de acceso: casi siempre por cliente_id
// Shard key: { cliente_id: 1, fecha: -1 }

// IMPORTANTE: Crear índice ANTES de fragmentar
db.pedidos.createIndex({ cliente_id: 1, fecha: -1 })

// Fragmentar la colección
sh.shardCollection("ecommerce.pedidos", { cliente_id: 1, fecha: -1 })

// Salida esperada:
{
  "collectionsharded": "ecommerce.pedidos",
  "ok": 1,
  "$clusterTime": { /* ... */ }
}
```

**Fragmentar colección de productos con hash sharding:**

```javascript
// Colección: productos
// Patrón de acceso: búsquedas variadas, no hay campo natural dominante
// Shard key: hash del _id para distribución uniforme

db.productos.createIndex({ _id: "hashed" })

sh.shardCollection("ecommerce.productos", { _id: "hashed" })
```

**Fragmentar colección de usuarios:**

```javascript
// Colección: usuarios
// Shard key: email (hashed para distribución uniforme)

db.usuarios.createIndex({ email: "hashed" })

sh.shardCollection("ecommerce.usuarios", { email: "hashed" })
```

**Verificar configuración de sharding:**

```javascript
sh.status()

// Ahora verás en databases:
databases:
  {
    "_id": "ecommerce",
    "primary": "shard01RS",  // Shard primario para colecciones no fragmentadas
    "partitioned": true,
    "version": {
      "uuid": UUID("..."),
      "timestamp": Timestamp(...)
    }
  }

// Y en la sección de ecommerce.pedidos:
ecommerce.pedidos
  shard key: { "cliente_id": 1, "fecha": -1 }
  unique: false
  balancing: true
  chunks:
    shard01RS  1
    shard02RS  1
    shard03RS  1
  { "cliente_id": { "$minKey": 1 }, "fecha": { "$minKey": 1 } } -->> 
  { "cliente_id": "C1667", "fecha": { "$maxKey": 1 } } on : shard01RS
  
  { "cliente_id": "C1667", "fecha": { "$maxKey": 1 } } -->> 
  { "cliente_id": "C5001", "fecha": { "$maxKey": 1 } } on : shard02RS
  
  { "cliente_id": "C5001", "fecha": { "$maxKey": 1 } } -->> 
  { "cliente_id": { "$maxKey": 1 }, "fecha": { "$maxKey": 1 } } on : shard03RS
```

### **7.7. Paso 6: Añadir nuevos nodos a shards existentes**

A medida que crece el tráfico, podemos necesitar añadir más nodos a los Replica Sets de cada shard. [mongodb](https://www.mongodb.com/docs/manual/tutorial/add-shards-to-shard-cluster/)

**Escenario:** Queremos añadir un cuarto nodo al shard01RS para mayor redundancia.

**Preparar nuevo servidor: shard01-d.ejemplo.com**

```bash
# En shard01-d
sudo mkdir -p /data/shard01
sudo chown mongodb:mongodb /data/shard01

# Copiar keyfile
scp config1.ejemplo.com:/etc/mongodb/keyfile /etc/mongodb/
sudo chmod 400 /etc/mongodb/keyfile
sudo chown mongodb:mongodb /etc/mongodb/keyfile

# Copiar configuración de shard01
# (mismo mongod-shard01.conf que los otros nodos)

# Iniciar mongod
sudo mongod -f /etc/mongod-shard01.conf
```

**Añadir el nuevo nodo al Replica Set:**

```bash
# Conectar al PRIMARY de shard01RS
mongosh --host shard01-a.ejemplo.com --port 27018 \
  --username shardAdmin --password ShardPassword123! \
  --authenticationDatabase admin
```

```javascript
// Añadir el nuevo miembro
rs.add({
  host: "shard01-d.ejemplo.com:27018",
  priority: 0,  // Prioridad 0 inicialmente (no será primario hasta que esté sincronizado)
  votes: 1
})

// Monitorizar sincronización inicial
rs.status().members.find(m => m.name === "shard01-d.ejemplo.com:27018")

// Estado durante sincronización:
{
  "_id": 3,
  "name": "shard01-d.ejemplo.com:27018",
  "health": 1,
  "state": 5,  // STARTUP2 (sincronización inicial)
  "stateStr": "STARTUP2",
  "uptime": 45,
  "syncSourceHost": "shard01-a.ejemplo.com:27018"
}

// Después de completar (puede tardar horas):
{
  "_id": 3,
  "name": "shard01-d.ejemplo.com:27018",
  "health": 1,
  "state": 2,  // SECONDARY
  "stateStr": "SECONDARY",
  "uptime": 7234
}
```

**Una vez sincronizado, ajustar prioridad si se desea:**

```javascript
cfg = rs.conf()
cfg.members [mongodb](https://www.mongodb.com/docs/manual/core/sharded-cluster-components/).priority = 1  // Ahora puede ser primario
rs.reconfig(cfg)
```

### **7.8. Paso 7: Verificación y pruebas de escalado**

Ahora probamos que el cluster funciona correctamente y escala horizontalmente.

**Prueba 1: Insertar datos de prueba**

```javascript
// Conectar a mongos como usuario de aplicación
mongosh "mongodb://appEcommerce:AppPass123!@mongos1.ejemplo.com:27017/ecommerce"

// Insertar 1 millón de pedidos de prueba
for (let i = 0; i < 1000000; i++) {
  db.pedidos.insertOne({
    cliente_id: "C" + Math.floor(Math.random() * 10000).toString().padStart(5, '0'),
    fecha: new Date(2026, 0, Math.floor(Math.random() * 60)),
    total: Math.random() * 1000,
    productos: [
      { sku: "PROD" + Math.floor(Math.random() * 1000), cantidad: Math.floor(Math.random() * 5) + 1 }
    ],
    estado: ["pendiente", "procesando", "enviado", "entregado"][Math.floor(Math.random() * 4)]
  });
  
  if (i % 10000 === 0) {
    print("Insertados: " + i);
  }
}
```

**Prueba 2: Verificar distribución de datos**

```javascript
// Ver distribución de chunks
sh.status()

// Contar documentos por shard
use ecommerce

db.pedidos.aggregate([
  { $group: { _id: "$cliente_id", count: { $sum: 1 } } },
  { $count: "total_clientes" }
])

// Estadísticas detalladas por shard
db.pedidos.getShardDistribution()

// Salida esperada:
Shard shard01RS at shard01RS/shard01-a.ejemplo.com:27018,...
  data: 85.5MB docs: 333245 chunks: 15
  estimated data per chunk: 5.7MB
  estimated docs per chunk: 22216

Shard shard02RS at shard02RS/shard02-a.ejemplo.com:27018,...
  data: 86.1MB docs: 334123 chunks: 15
  estimated data per chunk: 5.74MB
  estimated docs per chunk: 22274

Shard shard03RS at shard03RS/shard03-a.ejemplo.com:27018,...
  data: 85.8MB docs: 332632 chunks: 15
  estimated data per chunk: 5.72MB
  estimated docs per chunk: 22175

Totals
  data: 257.4MB docs: 1000000 chunks: 45
  Shard shard01RS contains 33.32% data, 33.32% docs, 33.33% chunks
  Shard shard02RS contains 33.44% data, 33.41% docs, 33.33% chunks
  Shard shard03RS contains 33.24% data, 33.27% docs, 33.33% chunks
```

**¡Excelente!** Los datos están distribuidos uniformemente (~33% en cada shard).

**Prueba 3: Consultas targeted vs broadcast**

```javascript
// Consulta TARGETED (incluye shard key)
db.pedidos.find({ 
  cliente_id: "C00123",
  fecha: { $gte: ISODate("2026-01-01") }
}).explain("executionStats")

// En el output buscar:
"winningPlan": {
  "stage": "SINGLE_SHARD",  // ← Solo consultó 1 shard
  "shards": [ {
    "shardName": "shard02RS",
    "connectionString": "..."
  } ]
}

// Consulta BROADCAST (sin shard key)
db.pedidos.find({ 
  total: { $gt: 500 }
}).explain("executionStats")

// En el output buscar:
"winningPlan": {
  "stage": "SHARD_MERGE",  // ← Consultó todos los shards
  "shards": [
    { "shardName": "shard01RS", /* ... */ },
    { "shardName": "shard02RS", /* ... */ },
    { "shardName": "shard03RS", /* ... */ }
  ]
}
```

**Prueba 4: Failover automático**

```bash
# Simular fallo del primario de shard01
ssh shard01-a.ejemplo.com "sudo systemctl stop mongod"

# Esperar 10-15 segundos

# Verificar elección automática
mongosh --host shard01-b.ejemplo.com --port 27018 \
  --username shardAdmin --password ShardPassword123! \
  --authenticationDatabase admin \
  --eval "rs.status()"

# Uno de los secundarios (probablemente shard01-b) ahora es PRIMARY

# Verificar que el cluster sigue funcionando
mongosh "mongodb://appEcommerce:AppPass123!@mongos1.ejemplo.com:27017/ecommerce" \
  --eval "db.pedidos.countDocuments()"

# Debería devolver ~1000000 sin errores

# Restaurar shard01-a
ssh shard01-a.ejemplo.com "sudo systemctl start mongod"
# Se reincorporará automáticamente como SECONDARY
```

### **7.9. Scripts y automatización**

**Script de monitorización del cluster:**

```bash
#!/bin/bash
# /usr/local/bin/monitor-cluster.sh

MONGOS_HOST="mongos1.ejemplo.com"
MONGOS_USER="shardingAdmin"
MONGOS_PASS="ShardingPass123!"

echo "=== ESTADO DEL CLUSTER $(date) ==="

# Estado general del sharding
mongosh "mongodb://${MONGOS_USER}:${MONGOS_PASS}@${MONGOS_HOST}:27017/admin?authSource=admin" \
  --quiet --eval "
  print('=== SHARDS ===');
  sh.status().shards.forEach(s => print(s._id + ': ' + s.host));
  
  print('\n=== BALANCER ===');
  let balancer = sh.getBalancerState();
  print('Activo: ' + balancer);
  
  print('\n=== MIGRACIÓN EN PROGRESO ===');
  let lockType = sh.isBalancerRunning();
  print('Migrando: ' + lockType);
  
  print('\n=== DISTRIBUCIÓN DE DATOS ===');
  db.getSiblingDB('config').shards.find().forEach(shard => {
    let stats = db.getSiblingDB('admin').runCommand({ listDatabases: 1 });
    print(shard._id + ' - verificando...');
  });
"

# Verificar salud de cada shard
for SHARD_RS in "shard01RS" "shard02RS" "shard03RS"; do
  echo "=== ${SHARD_RS} ==="
  SHARD_HOST=$(echo $SHARD_RS | sed 's/RS/-a.ejemplo.com/')
  mongosh --host ${SHARD_HOST} --port 27018 \
    --username shardAdmin --password ShardPassword123! \
    --authenticationDatabase admin \
    --quiet --eval "
    rs.status().members.forEach(m => {
      let lag = m.optimeDate ? (new Date() - m.optimeDate) / 1000 : 'N/A';
      print(m.name + ': ' + m.stateStr + ' (lag: ' + lag + 's)');
    });
  "
done
```

**Script de backup automatizado del cluster:**

```bash
#!/bin/bash
# /usr/local/bin/backup-sharded-cluster.sh

BACKUP_DIR="/backup/mongodb"
DATE=$(date +%Y%m%d_%H%M%S)
MONGOS_URI="mongodb://shardingAdmin:ShardingPass123!@mongos1.ejemplo.com:27017/?authSource=admin"

mkdir -p ${BACKUP_DIR}/${DATE}

# Parar el balancer durante backup
mongosh "${MONGOS_URI}" --eval "sh.stopBalancer()"
echo "Balancer detenido"

# Backup de config servers (CRÍTICO)
mongodump --uri="mongodb://clusterAdmin:SuperSecurePassword123!@config1.ejemplo.com:27019,config2.ejemplo.com:27019,config3.ejemplo.com:27019/?replicaSet=configRS&authSource=admin" \
  --out=${BACKUP_DIR}/${DATE}/config \
  --oplog

# Backup de cada shard (en paralelo)
for SHARD in "01" "02" "03"; do
  (
    mongodump --uri="mongodb://shardAdmin:ShardPassword123!@shard${SHARD}-a.ejemplo.com:27018,shard${SHARD}-b.ejemplo.com:27018/?replicaSet=shard${SHARD}RS&authSource=admin" \
      --out=${BACKUP_DIR}/${DATE}/shard${SHARD} \
      --oplog
  ) &
done

wait  # Esperar a que terminen todos los backups paralelos

# Reiniciar balancer
mongosh "${MONGOS_URI}" --eval "sh.startBalancer()"
echo "Balancer reiniciado"

# Comprimir backup
tar czf ${BACKUP_DIR}/cluster-backup-${DATE}.tar.gz ${BACKUP_DIR}/${DATE}
rm -rf ${BACKUP_DIR}/${DATE}

# Sincronizar a almacenamiento remoto
aws s3 cp ${BACKUP_DIR}/cluster-backup-${DATE}.tar.gz s3://mi-bucket/mongodb-backups/

# Retención: eliminar backups de más de 7 días
find ${BACKUP_DIR} -name "cluster-backup-*.tar.gz" -mtime +7 -delete

echo "Backup completado: ${DATE}"
```

**Configurar en crontab:**

```bash
# Backup diario a las 3 AM
0 3 * * * /usr/local/bin/backup-sharded-cluster.sh >> /var/log/mongodb-backup.log 2>&1

# Monitorización cada 5 minutos
*/5 * * * * /usr/local/bin/monitor-cluster.sh >> /var/log/mongodb-monitor.log 2>&1
```

Con esto, tenemos un cluster MongoDB completamente funcional con sharding, alta disponibilidad, y capacidad de escalar horizontalmente añadiendo más shards o más nodos a los shards existentes. El cluster puede crecer de 3 shards a decenas de shards según las necesidades, distribuyendo automáticamente los datos y la carga.

***

¿Continuamos con el Capítulo 8 (Buenas Prácticas)?