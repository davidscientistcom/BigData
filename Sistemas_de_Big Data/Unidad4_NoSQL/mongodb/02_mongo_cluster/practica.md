# PR√ÅCTICA: ARQUITECTURAS MONGODB CON DOCKER-COMPOSE

## **OBJETIVO DE LA PR√ÅCTICA**

En esta pr√°ctica aprender√°s a desplegar y gestionar arquitecturas distribuidas de MongoDB utilizando Docker Compose. Implementar√°s un **Replica Set** (conjunto de r√©plicas) para entender c√≥mo MongoDB proporciona alta disponibilidad, redundancia de datos y tolerancia a fallos. Al finalizar, comprender√°s c√≥mo funciona la replicaci√≥n autom√°tica, el proceso de elecci√≥n de nodo primario y c√≥mo recuperarse ante fallos. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

**Conceptos que dominar√°s:**
- Replica Sets: nodos primarios y secundarios
- Replicaci√≥n autom√°tica mediante oplog
- Proceso de elecci√≥n (election) cuando falla el primario
- Operaciones de lectura/escritura en arquitecturas distribuidas
- Monitorizaci√≥n del estado del cluster

**Requisitos previos:**
- Docker y Docker Compose instalados
- Conocimientos b√°sicos de MongoDB
- Terminal de comandos (bash/zsh)
- 4GB RAM disponible



##  **PARTE 1: ENTENDIENDO LA ARQUITECTURA**

### **¬øQu√© es un Replica Set?**

Un Replica Set es un grupo de instancias MongoDB que mantienen los mismos datos. Proporciona redundancia y alta disponibilidad, que son fundamentales en producci√≥n. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

**Componentes:**
- **PRIMARY (Primario)**: √önico nodo que acepta escrituras
- **SECONDARY (Secundarios)**: Replican datos del primario autom√°ticamente
- **ARBITER (Opcional)**: Participa en elecciones pero no almacena datos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         REPLICA SET: rs0                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  mongo1  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  mongo2  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ PRIMARY  ‚îÇ      ‚îÇSECONDARY ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ       ‚ñ≤                  ‚ñ≤             ‚îÇ
‚îÇ       ‚îÇ                  ‚îÇ             ‚îÇ
‚îÇ       ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ             ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  mongo3  ‚îÇ‚óÑ‚îÄ‚îò             ‚îÇ
‚îÇ            ‚îÇSECONDARY ‚îÇ                ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Replicaci√≥n autom√°tica via OPLOG      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**¬øC√≥mo funciona la replicaci√≥n?**

1. Una aplicaci√≥n escribe en el PRIMARY
2. El PRIMARY registra la operaci√≥n en el **oplog** (operations log)
3. Los SECONDARY leen el oplog y aplican las mismas operaciones
4. Si el PRIMARY cae, los SECONDARY eligen un nuevo PRIMARY autom√°ticamente [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)



##  **PARTE 2: PREPARACI√ìN DEL ENTORNO**

### **Paso 1: Crear estructura de directorios**

Crea una carpeta para la pr√°ctica y organiza los archivos:

```bash
# Crear directorio del proyecto
mkdir mongodb-replicaset-lab
cd mongodb-replicaset-lab

# Crear subdirectorios para datos persistentes
mkdir -p data/mongo1 data/mongo2 data/mongo3

# Crear directorio para scripts
mkdir scripts
```

**¬øQu√© estamos haciendo?**
- `data/mongo1`, `data/mongo2`, `data/mongo3`: Almacenar√°n los datos de cada nodo MongoDB. As√≠ los datos persisten aunque eliminemos los contenedores.
- `scripts`: Contendr√° scripts de inicializaci√≥n y configuraci√≥n.

### **Paso 2: Crear el archivo docker-compose.yml**

Crea el archivo `docker-compose.yml` con el siguiente contenido:

```yaml
version: '3.8'

services:
  # NODO 1: Ser√° el PRIMARY inicialmente
  mongo1:
    image: mongo:7.0
    container_name: mongo1
    hostname: mongo1
    ports:
      - "27017:27017"
    command: mongod --replSet rs0 --bind_ip_all
    volumes:
      - ./data/mongo1:/data/db
    networks:
      - mongo-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 40s

  # NODO 2: SECONDARY
  mongo2:
    image: mongo:7.0
    container_name: mongo2
    hostname: mongo2
    ports:
      - "27018:27017"
    command: mongod --replSet rs0 --bind_ip_all
    volumes:
      - ./data/mongo2:/data/db
    networks:
      - mongo-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 40s

  # NODO 3: SECONDARY
  mongo3:
    image: mongo:7.0
    container_name: mongo3
    hostname: mongo3
    ports:
      - "27019:27017"
    command: mongod --replSet rs0 --bind_ip_all
    volumes:
      - ./data/mongo3:/data/db
    networks:
      - mongo-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 40s

networks:
  mongo-network:
    driver: bridge
```

** Explicaci√≥n l√≠nea por l√≠nea:**

- `image: mongo:7.0`: Usamos MongoDB versi√≥n 7.0 (√∫ltima estable)
- `container_name` y `hostname`: Nombres √∫nicos para cada nodo
- `ports`: Exponemos puertos diferentes en el host (27017, 27018, 27019) pero todos usan 27017 internamente
- `command: mongod --replSet rs0 --bind_ip_all`:
  - `--replSet rs0`: Indica que este nodo pertenece al Replica Set llamado "rs0"
  - `--bind_ip_all`: Permite conexiones desde cualquier IP (necesario en Docker)
- `volumes`: Persiste datos en carpetas locales
- `networks`: Todos los nodos en la misma red para comunicarse
- `healthcheck`: Docker verificar√° cada 10s que MongoDB responde correctamente

### **Paso 3: Iniciar los contenedores**

```bash
# Iniciar todos los contenedores en segundo plano
docker-compose up -d

# Verificar que est√©n corriendo
docker-compose ps
```

**Salida esperada:**
```
NAME      IMAGE       STATUS                    PORTS
mongo1    mongo:7.0   Up 30 seconds (healthy)   0.0.0.0:27017->27017/tcp
mongo2    mongo:7.0   Up 30 seconds (healthy)   0.0.0.0:27018->27017/tcp
mongo3    mongo:7.0   Up 30 seconds (healthy)   0.0.0.0:27019->27017/tcp
```

**¬øQu√© ha pasado?**
Docker ha descargado la imagen de MongoDB 7.0 (si no la ten√≠as) y ha iniciado 3 contenedores independientes. Cada uno ejecuta una instancia de MongoDB, pero **a√∫n no est√°n configurados como Replica Set**. En este momento son 3 instancias standalone separadas.



## **PARTE 3: CONFIGURACI√ìN DEL REPLICA SET**

### **Paso 4: Inicializar el Replica Set**

Ahora conectaremos los 3 nodos para formar un Replica Set: [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

```bash
# Conectar al primer nodo (mongo1)
docker exec -it mongo1 mongosh
```

**Dentro de la shell de MongoDB**, ejecuta:

```javascript
// Configuraci√≥n del Replica Set
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo1:27017", priority: 2 },
    { _id: 1, host: "mongo2:27017", priority: 1 },
    { _id: 2, host: "mongo3:27017", priority: 1 }
  ]
})
```

** Explicaci√≥n del comando:**

- `rs.initiate()`: Inicializa el Replica Set
- `_id: "rs0"`: Nombre del Replica Set (debe coincidir con --replSet del docker-compose)
- `members`: Array con los 3 nodos
  - `_id: 0, 1, 2`: Identificadores √∫nicos de cada miembro
  - `host`: Nombre del host y puerto (usamos nombres de contenedor)
  - `priority: 2` para mongo1: Tiene prioridad m√°s alta, preferido como PRIMARY [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)
  - `priority: 1` para mongo2 y mongo3: Pueden ser PRIMARY si mongo1 falla

**Salida esperada:**
```javascript
{ ok: 1 }
```

**Espera 10-15 segundos** para que se complete la elecci√≥n del primario. Ver√°s que el prompt cambia de `test>` a `rs0 [direct: primary]>` o similar.

### **Paso 5: Verificar el estado del Replica Set**

```javascript
// Ver estado completo del cluster
rs.status()
```

**Busca en la salida:**
```javascript
{
  set: 'rs0',
  members: [
    {
      _id: 0,
      name: 'mongo1:27017',
      stateStr: 'PRIMARY',
      health: 1,
      ...
    },
    {
      _id: 1,
      name: 'mongo2:27017',
      stateStr: 'SECONDARY',
      health: 1,
      ...
    },
    {
      _id: 2,
      name: 'mongo3:27017',
      stateStr: 'SECONDARY',
      health: 1,
      ...
    }
  ],
  ok: 1
}
```

**¬øQu√© significa cada campo?**
- `stateStr: 'PRIMARY'`: Este nodo acepta escrituras
- `stateStr: 'SECONDARY'`: Este nodo replica datos del primario
- `health: 1`: El nodo est√° operativo (0 = ca√≠do)

Tambi√©n puedes usar comandos m√°s simples:

```javascript
// Resumen r√°pido
rs.isMaster()

// Ver configuraci√≥n
rs.conf()
```



##  **PARTE 4: PROBANDO LA REPLICACI√ìN**

### **Paso 6: Insertar datos en el PRIMARY**

Desde la misma shell de mongosh conectada a mongo1 (PRIMARY):

```javascript
// Cambiar a una base de datos de prueba
use tiendaOnline

// Insertar documentos
db.productos.insertMany([
  { nombre: "Laptop Dell", precio: 899, stock: 15, categoria: "Inform√°tica" },
  { nombre: "Mouse Logitech", precio: 25, stock: 100, categoria: "Perif√©ricos" },
  { nombre: "Teclado Mec√°nico", precio: 120, stock: 30, categoria: "Perif√©ricos" },
  { nombre: "Monitor 27''", precio: 350, stock: 20, categoria: "Inform√°tica" },
  { nombre: "Webcam HD", precio: 65, stock: 45, categoria: "Perif√©ricos" }
])

// Verificar inserci√≥n
db.productos.find().pretty()
```

**Salida esperada:**
```javascript
{
  acknowledged: true,
  insertedIds: {
    '0': ObjectId("..."),
    '1': ObjectId("..."),
    ...
  }
}
```

### **Paso 7: Verificar que los datos se replicaron**

Abre **otra terminal** (sin cerrar la anterior) y con√©ctate a mongo2 (SECONDARY):

```bash
# Terminal 2
docker exec -it mongo2 mongosh
```

Dentro de mongosh de mongo2:

```javascript
// Habilitar lecturas en SECONDARY (por defecto est√°n deshabilitadas)
rs.secondaryOk()
// O en versiones m√°s nuevas:
// db.getMongo().setReadPref('secondary')

// Cambiar a la misma base de datos
use tiendaOnline

// ¬°Leer los datos replicados!
db.productos.find().pretty()
```

** ¬°Deber√≠as ver los mismos 5 productos!**

**¬øQu√© ha pasado?**
1. Insertaste datos en el PRIMARY (mongo1)
2. MongoDB registr√≥ las operaciones en el oplog del PRIMARY
3. Los SECONDARY (mongo2 y mongo3) leyeron el oplog autom√°ticamente
4. Aplicaron las mismas operaciones de inserci√≥n en sus propias copias de datos
5. **Replicaci√≥n completada en segundos** (normalmente milisegundos) [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

### **Paso 8: Intentar escribir en un SECONDARY (debe fallar)**

Desde la shell de mongo2 (SECONDARY):

```javascript
// Intentar insertar en un SECONDARY
db.productos.insertOne({ nombre: "Prueba", precio: 10 })
```

**Error esperado:**
```javascript
MongoServerError: not primary
```

**¬øPor qu√©?**
Los SECONDARY **SOLO pueden leer**, no escribir. Esto garantiza consistencia de datos. Todas las escrituras deben ir al PRIMARY, que luego las replica a los SECONDARY. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)



## **PARTE 5: SIMULANDO FALLO DEL PRIMARY (FAILOVER)**

Esta es la parte m√°s importante: ver√°s c√≥mo MongoDB maneja autom√°ticamente la ca√≠da del nodo primario.

### **Paso 9: Detener el nodo PRIMARY**

```bash
# En una nueva terminal
docker stop mongo1
```

**¬øQu√© va a pasar?**
1. Los SECONDARY detectar√°n que mongo1 no responde (despu√©s de 10 segundos de heartbeats fallidos) [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)
2. Iniciar√°n un proceso de elecci√≥n (election)
3. El SECONDARY con mayor prioridad (ambos tienen priority=1) ser√° elegido como nuevo PRIMARY
4. El proceso es **completamente autom√°tico**, sin intervenci√≥n humana

### **Paso 10: Observar la elecci√≥n autom√°tica**

Vuelve a la terminal donde tienes mongo2 abierto y ejecuta:

```javascript
// Ver el nuevo estado del cluster
rs.status()
```

Espera unos 10-15 segundos y vuelve a ejecutar `rs.status()` hasta que veas algo como:

```javascript
{
  members: [
    {
      _id: 0,
      name: 'mongo1:27017',
      stateStr: '(not reachable/healthy)',
      health: 0,  // ¬°Ca√≠do!
      ...
    },
    {
      _id: 1,
      name: 'mongo2:27017',
      stateStr: 'PRIMARY',  // ¬°Ahora es PRIMARY!
      health: 1,
      ...
    },
    {
      _id: 2,
      name: 'mongo3:27017',
      stateStr: 'SECONDARY',
      health: 1,
      ...
    }
  ]
}
```

** ¬°Failover completado! mongo2 es ahora el PRIMARY.**

### **Paso 11: Escribir en el nuevo PRIMARY**

Desde mongo2 (que ahora es PRIMARY):

```javascript
// Insertar nuevo producto
db.productos.insertOne({ 
  nombre: "SSD 1TB", 
  precio: 95, 
  stock: 50, 
  categoria: "Almacenamiento",
  agregadoDespuesFailover: true 
})

// Verificar
db.productos.find({ agregadoDespuesFailover: true })
```

**Ahora mongo2 acepta escrituras** porque es el nuevo PRIMARY.

### **Paso 12: Verificar replicaci√≥n en mongo3**

Abre otra terminal y con√©ctate a mongo3:

```bash
docker exec -it mongo3 mongosh
```

```javascript
rs.secondaryOk()
use tiendaOnline

// ¬°El nuevo producto deber√≠a estar aqu√≠ tambi√©n!
db.productos.find({ agregadoDespuesFailover: true })
```

**¬øQu√© demuestra esto?**
El cluster sigue funcionando con 2 de 3 nodos. La replicaci√≥n contin√∫a normalmente entre el nuevo PRIMARY (mongo2) y el SECONDARY restante (mongo3).



##  **PARTE 6: RECUPERACI√ìN DEL NODO CA√çDO**

### **Paso 13: Reiniciar mongo1**

```bash
# Reiniciar el contenedor
docker start mongo1
```

Espera 20-30 segundos y observa qu√© pasa:

```javascript
// Desde mongo2 (actual PRIMARY), verifica el estado
rs.status()
```

Ver√°s que mongo1 ha vuelto como **SECONDARY**:

```javascript
{
  _id: 0,
  name: 'mongo1:27017',
  stateStr: 'SECONDARY',  // ¬°Ha vuelto pero como SECONDARY!
  health: 1,
  ...
}
```

**¬øPor qu√© no vuelve como PRIMARY autom√°ticamente?**

Aunque mongo1 tiene `priority: 2`, no reemplaza al PRIMARY actual inmediatamente. MongoDB es conservador para evitar elecciones innecesarias que interrumpir√≠an brevemente las operaciones.

**¬ømongo1 tiene los datos actualizados?**

¬°S√≠! Al reconectarse, mongo1 lee el oplog y se sincroniza autom√°ticamente con todos los cambios que ocurrieron mientras estaba ca√≠do.

### **Paso 14: Forzar que mongo1 vuelva a ser PRIMARY (opcional)**

Si quieres que mongo1 recupere su rol de PRIMARY:

```javascript
// Desde cualquier nodo conectado al cluster
rs.stepDown(60)
```

Este comando hace que el PRIMARY actual (mongo2) renuncie voluntariamente durante 60 segundos, forzando una nueva elecci√≥n. mongo1, con mayor prioridad, ser√° elegido.



##  **PARTE 7: MONITORIZACI√ìN Y COMANDOS √öTILES**

### **Comandos de monitorizaci√≥n**

```javascript
// Ver estad√≠sticas de replicaci√≥n
rs.printReplicationInfo()
// Muestra: tama√±o del oplog, cu√°nto tiempo de operaciones contiene

// Ver retraso (lag) de los SECONDARY
rs.printSecondaryReplicationInfo()
// Muestra: cu√°nto van retrasados los SECONDARY respecto al PRIMARY

// Ver qu√© nodos votan en elecciones
rs.conf().members.forEach(m => {
  print(`${m.host}: votes=${m.votes}, priority=${m.priority}`)
})

// Monitorear el oplog (operations log)
use local
db.oplog.rs.find().limit(5).sort({$natural: -1})
// Muestra las √∫ltimas 5 operaciones replicadas
```

### **Verificar sincronizaci√≥n**

```javascript
// Desde el PRIMARY
db.serverStatus().repl
// Muestra informaci√≥n detallada de replicaci√≥n

// Comparar tama√±os de colecciones (deben ser iguales)
db.productos.countDocuments()
```



## üß™ **PARTE 8: EXPERIMENTOS ADICIONALES**

### **Experimento 1: Leer desde SECONDARY**

MongoDB permite configurar preferencias de lectura:

```javascript
// Conectar a mongo2 (SECONDARY)
db.getMongo().setReadPref('secondary')

// Ahora puedes leer sin rs.secondaryOk()
db.productos.find()

// Leer desde el nodo m√°s cercano (√∫til en clusters geogr√°ficamente distribuidos)
db.getMongo().setReadPref('nearest')
```

**Casos de uso:**
- Leer desde SECONDARY: Descargar trabajo de lectura del PRIMARY
- Aplicaciones de an√°lisis/reportes que no requieren datos en tiempo real exacto

### **Experimento 2: A√±adir un nodo ARBITER**

Un ARBITER participa en elecciones pero no almacena datos. √ötil para tener n√∫mero impar de votantes sin el coste de almacenamiento. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

A√±ade esto al `docker-compose.yml`:

```yaml
  mongo-arbiter:
    image: mongo:7.0
    container_name: mongo-arbiter
    command: mongod --replSet rs0 --bind_ip_all
    networks:
      - mongo-network
```

Reinicia y a√±√°delo al Replica Set:

```javascript
rs.addArb("mongo-arbiter:27017")
rs.status()  // Ver√°s stateStr: 'ARBITER'
```

### **Experimento 3: Simular ca√≠da de mayor√≠a**

```bash
# Detener 2 de 3 nodos
docker stop mongo2 mongo3
```

Con√©ctate a mongo1:

```javascript
// Verificar estado
rs.status()

// Intentar escribir
db.productos.insertOne({ nombre: "Test" })
```

**Error esperado:**
```
NotWritablePrimary: not primary
```

**¬øPor qu√©?**
Con solo 1 de 3 nodos activos, no hay **mayor√≠a (quorum)**. MongoDB requiere que la mayor√≠a de nodos est√©n operativos para garantizar consistencia de datos. Con 3 nodos, se necesitan al menos 2. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

```bash
# Reiniciar uno de los nodos para recuperar mayor√≠a
docker start mongo2
# Esperar 15 segundos y las operaciones volver√°n a funcionar
```



## **PARTE 9: PRUEBAS DE EVALUACI√ìN**

### **Test de conocimientos**

Ejecuta estas tareas para verificar tu comprensi√≥n:

1. **Insertar 100 documentos en el PRIMARY y verificar que todos lleguen a los SECONDARY**

```javascript
// En PRIMARY
use testReplica
for(let i=1; i<=100; i++) {
  db.test.insertOne({ numero: i, fecha: new Date() })
}

// En SECONDARY
rs.secondaryOk()
use testReplica
db.test.countDocuments()  // Debe ser 100
```

2. **Simular un failover completo: det√©n el PRIMARY, escribe en el nuevo PRIMARY, reinicia el antiguo PRIMARY**

3. **Medir el tiempo de failover:**

```javascript
// Script de monitorizaci√≥n continua
while(true) {
  try {
    rs.isMaster().primary
    sleep(1000)
  } catch(e) {
    print("PRIMARY NO DISPONIBLE")
  }
}
```

Ejecuta esto en una terminal, det√©n el PRIMARY en otra, y cuenta cu√°ntos segundos tarda en detectarse y elegirse nuevo PRIMARY. Normalmente 10-15 segundos. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)



## **PARTE 10: LIMPIEZA Y CONCLUSI√ìN**

### **Detener y limpiar el entorno**

```bash
# Detener todos los contenedores
docker-compose down

# Eliminar tambi√©n los datos persistentes (¬°CUIDADO!)
sudo rm -rf data/

# O mantener los datos para futuras pr√°cticas
# (solo detiene contenedores, los datos persisten)
```

### **Verificar limpieza**

```bash
docker ps -a | grep mongo
# No deber√≠a mostrar contenedores mongo
```



## üìö **RESUMEN DE CONCEPTOS APRENDIDOS**

‚úÖ **Replica Set**: Conjunto de instancias MongoDB que mantienen los mismos datos  
‚úÖ **PRIMARY**: √önico nodo que acepta escrituras  
‚úÖ **SECONDARY**: Nodos que replican datos del PRIMARY autom√°ticamente  
‚úÖ **Oplog**: Log de operaciones que permite la replicaci√≥n as√≠ncrona [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)
‚úÖ **Failover**: Proceso autom√°tico de elecci√≥n de nuevo PRIMARY cuando el actual falla  
‚úÖ **Quorum**: Mayor√≠a de nodos necesaria para operaciones de escritura  
‚úÖ **Priority**: Determina qu√© nodos son preferidos para ser PRIMARY [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)
‚úÖ **Heartbeats**: Verificaciones cada 2 segundos para detectar nodos ca√≠dos  

### **Aplicaciones en producci√≥n**

Esta arquitectura es **esencial** en sistemas de producci√≥n porque:

1. **Alta disponibilidad**: Si un servidor falla, otro toma el control autom√°ticamente
2. **Redundancia de datos**: M√∫ltiples copias previenen p√©rdida de informaci√≥n
3. **Escalado de lecturas**: Los SECONDARY pueden servir consultas de lectura
4. **Backups sin downtime**: Puedes hacer backup desde un SECONDARY sin afectar el PRIMARY
5. **Tolerancia a fallos**: El sistema sigue operativo mientras haya mayor√≠a de nodos

### **Pr√≥ximos pasos**

Para seguir aprendiendo:
- **Sharding**: Distribuci√≥n horizontal de datos entre m√∫ltiples Replica Sets [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)
- **Config Servers y mongos**: Componentes para arquitecturas con sharding
- **Zonas y distribuci√≥n geogr√°fica**: Replica Sets en m√∫ltiples datacenters
- **Monitorizaci√≥n con MongoDB Atlas o Ops Manager**



## **ANEXO: COMANDOS DE REFERENCIA R√ÅPIDA**

```javascript
// ESTADO DEL CLUSTER
rs.status()                    // Estado completo
rs.isMaster()                  // ¬øQui√©n es el PRIMARY?
rs.conf()                      // Configuraci√≥n actual

// INFORMACI√ìN DE REPLICACI√ìN
rs.printReplicationInfo()      // Info del oplog
rs.printSecondaryReplicationInfo()  // Lag de secundarios

// MODIFICAR CLUSTER
rs.add("host:port")            // A√±adir nodo
rs.remove("host:port")         // Eliminar nodo
rs.addArb("host:port")         // A√±adir √°rbitro
rs.stepDown(60)                // PRIMARY renuncia 60 segundos

// CONFIGURACI√ìN DE NODOS
rs.reconfigure(config)         // Aplicar nueva configuraci√≥n
rs.freeze(seconds)             // Evita que SECONDARY sea elegido PRIMARY

// LECTURAS
rs.secondaryOk()               // Permitir lecturas en SECONDARY
db.getMongo().setReadPref('secondary')  // Preferencia de lectura
```



## **PREGUNTAS FRECUENTES**

**P: ¬øPor qu√© necesito 3 nodos m√≠nimo?**  
R: Para tener mayor√≠a (quorum). Con 2 nodos, si uno cae, no hay mayor√≠a y el cluster se bloquea para escrituras.

**P: ¬øCu√°nto datos puede perder en un failover?**  
R: Por defecto, MongoDB usa write concern `w:1`, que confirma escritura cuando el PRIMARY la registra. Con `w:majority`, espera confirmaci√≥n de la mayor√≠a de nodos, garantizando durabilidad pero con mayor latencia.

**P: ¬øLos SECONDARY est√°n siempre exactamente sincronizados?**  
R: No, puede haber un peque√±o retraso (lag) de milisegundos a segundos. Depende de la carga y la red. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/138042210/58c1624d-8c24-476d-99f1-b65440f4443a/02.md)

**P: ¬øQu√© pasa si falla m√°s de la mitad de los nodos?**  
R: El cluster deja de aceptar escrituras hasta que se recupere la mayor√≠a. Las aplicaciones recibir√°n errores.

**P: ¬øPuedo tener Replica Sets en diferentes ubicaciones geogr√°ficas?**  
R: S√≠, MongoDB soporta Replica Sets distribuidos geogr√°ficamente. Debes ajustar timeouts y considerar latencias de red.