## **5. Escalado Horizontal: Replica Sets**

### **5.1. Configuración de un Replica Set**

La configuración de un Replica Set es el primer paso hacia la alta disponibilidad en MongoDB. Aunque ya hemos introducido el concepto en capítulos anteriores, ahora vamos a profundizar en los detalles prácticos de implementación y las consideraciones operacionales que debe tener en cuenta un administrador.

**Planificación previa al despliegue:**

Antes de configurar un Replica Set, debemos tomar decisiones arquitectónicas importantes. La primera es el número de nodos: MongoDB requiere un número impar de miembros votantes para garantizar que siempre pueda obtenerse una mayoría en las elecciones. Las configuraciones típicas son 3, 5 o 7 nodos. Para la mayoría de aplicaciones, 3 nodos proporcionan un buen equilibrio entre disponibilidad y costes.

La segunda decisión es la distribución geográfica. En un entorno de producción ideal, los tres nodos deberían estar en diferentes racks, centros de datos o incluso regiones geográficas para protegerse contra fallos de infraestructura. Sin embargo, la latencia de red entre nodos afecta directamente al rendimiento de escritura, por lo que debemos encontrar un equilibrio entre disponibilidad geográfica y rendimiento.

**Preparación del entorno:**

Supongamos que vamos a configurar un Replica Set de 3 nodos con las siguientes máquinas:

- **mongo-rs1.ejemplo.com** (192.168.1.101) - Será el primario inicial
- **mongo-rs2.ejemplo.com** (192.168.1.102) - Secundario
- **mongo-rs3.ejemplo.com** (192.168.1.103) - Secundario

**Paso 1: Configurar el archivo mongod.conf en cada servidor**

En **mongo-rs1** creamos `/etc/mongod.conf`:

```yaml
# Configuración para mongo-rs1
storage:
  dbPath: /data/mongodb
  journal:
    enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 8

systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true

net:
  port: 27017
  bindIp: 0.0.0.0  # Escuchar en todas las interfaces
  
replication:
  replSetName: "rsProduccion"
  oplogSizeMB: 5120  # 5GB de oplog

security:
  authorization: enabled
  keyFile: /etc/mongodb/keyfile

processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongod.pid
```

Repetimos la configuración en mongo-rs2 y mongo-rs3, asegurándonos de que todos tienen el mismo `replSetName`.

**Paso 2: Crear keyfile para autenticación inter-nodos**

El keyfile es un archivo de contraseña compartida que permite a los nodos del Replica Set autenticarse entre sí:

```bash
# En mongo-rs1, generar keyfile aleatorio
openssl rand -base64 756 > /etc/mongodb/keyfile
chmod 400 /etc/mongodb/keyfile
chown mongodb:mongodb /etc/mongodb/keyfile

# Copiar el mismo keyfile a mongo-rs2 y mongo-rs3
scp /etc/mongodb/keyfile root@mongo-rs2:/etc/mongodb/
scp /etc/mongodb/keyfile root@mongo-rs3:/etc/mongodb/

# Ajustar permisos en los otros nodos
ssh mongo-rs2 "chmod 400 /etc/mongodb/keyfile && chown mongodb:mongodb /etc/mongodb/keyfile"
ssh mongo-rs3 "chmod 400 /etc/mongodb/keyfile && chown mongodb:mongodb /etc/mongodb/keyfile"
```

**Paso 3: Iniciar mongod en cada servidor**

```bash
# En cada servidor
systemctl start mongod
systemctl enable mongod  # Iniciar automáticamente al arrancar

# Verificar que está corriendo
systemctl status mongod
```

**Paso 4: Inicializar el Replica Set**

Ahora conectamos al primero de los servidores (mongo-rs1) sin autenticación (por ahora) y inicializamos el Replica Set:

```bash
mongosh --host mongo-rs1.ejemplo.com --port 27017
```

Dentro del shell:

```javascript
// Configuración inicial del Replica Set
rs.initiate({
  _id: "rsProduccion",
  members: [
    { 
      _id: 0, 
      host: "mongo-rs1.ejemplo.com:27017",
      priority: 2  // Mayor prioridad para ser primario
    },
    { 
      _id: 1, 
      host: "mongo-rs2.ejemplo.com:27017",
      priority: 1
    },
    { 
      _id: 2, 
      host: "mongo-rs3.ejemplo.com:27017",
      priority: 1
    }
  ]
})
```

La respuesta debería ser:

```javascript
{
  "ok": 1,
  "$clusterTime": {
    "clusterTime": Timestamp(1707497654, 1),
    "signature": { /* ... */ }
  }
}
```

**Paso 5: Verificar el estado del Replica Set**

```javascript
rs.status()
```

Deberíamos ver una salida similar a:

```javascript
{
  "set": "rsProduccion",
  "date": ISODate("2026-02-09T17:32:00Z"),
  "myState": 1,  // 1 = PRIMARY
  "members": [
    {
      "_id": 0,
      "name": "mongo-rs1.ejemplo.com:27017",
      "health": 1,
      "state": 1,  // PRIMARY
      "stateStr": "PRIMARY",
      "uptime": 234,
      "optime": { "ts": Timestamp(1707497654, 1), "t": 1 },
      "optimeDate": ISODate("2026-02-09T17:30:54Z"),
      "electionTime": Timestamp(1707497543, 1),
      "electionDate": ISODate("2026-02-09T17:29:03Z"),
      "self": true
    },
    {
      "_id": 1,
      "name": "mongo-rs2.ejemplo.com:27017",
      "health": 1,
      "state": 2,  // SECONDARY
      "stateStr": "SECONDARY",
      "uptime": 123,
      "optime": { "ts": Timestamp(1707497654, 1), "t": 1 },
      "optimeDate": ISODate("2026-02-09T17:30:54Z"),
      "syncSourceHost": "mongo-rs1.ejemplo.com:27017",
      "syncSourceId": 0
    },
    {
      "_id": 2,
      "name": "mongo-rs3.ejemplo.com:27017",
      "health": 1,
      "state": 2,  // SECONDARY
      "stateStr": "SECONDARY",
      "uptime": 123,
      "optime": { "ts": Timestamp(1707497654, 1), "t": 1 },
      "optimeDate": ISODate("2026-02-09T17:30:54Z"),
      "syncSourceHost": "mongo-rs1.ejemplo.com:27017",
      "syncSourceId": 0
    }
  ],
  "ok": 1
}
```

**Paso 6: Crear usuario administrador**

Ahora que el Replica Set está funcionando, creamos el primer usuario administrador:

```javascript
use admin
db.createUser({
  user: "admin",
  pwd: "ContraseñaSegura123!",
  roles: [ { role: "root", db: "admin" } ]
})
```

A partir de este momento, todas las conexiones requerirán autenticación:

```bash
mongosh "mongodb://admin:ContraseñaSegura123!@mongo-rs1.ejemplo.com,mongo-rs2.ejemplo.com,mongo-rs3.ejemplo.com/?replicaSet=rsProduccion&authSource=admin"
```

### **5.2. Añadir y eliminar nodos secundarios**

Una de las grandes ventajas de los Replica Sets es que podemos añadir o eliminar nodos sin tiempo de inactividad. Esto es especialmente útil para actualizaciones de hardware, cambios de topología o escalado de capacidad de lectura.

**Añadir un nuevo nodo secundario:**

Supongamos que necesitamos añadir un cuarto nodo (mongo-rs4.ejemplo.com) para aumentar la redundancia o distribuir lecturas.

**Paso 1: Preparar el nuevo servidor**

En mongo-rs4, configuramos mongod.conf con la misma configuración que los otros nodos (mismo replSetName y keyfile). Iniciamos el servicio:

```bash
# Copiar keyfile desde un nodo existente
scp mongo-rs1:/etc/mongodb/keyfile /etc/mongodb/
chmod 400 /etc/mongodb/keyfile
chown mongodb:mongodb /etc/mongodb/keyfile

# Iniciar mongod
systemctl start mongod
```

**Paso 2: Añadir el nodo al Replica Set**

Desde el nodo PRIMARY:

```javascript
rs.add({
  host: "mongo-rs4.ejemplo.com:27017",
  priority: 1,
  votes: 1
})
```

MongoDB comenzará automáticamente la sincronización inicial. Podemos monitorizar el progreso:

```javascript
rs.status().members.find(m => m.name === "mongo-rs4.ejemplo.com:27017")
```

Estados durante la sincronización:
- **STARTUP**: El nodo está iniciando
- **STARTUP2**: Realizando sincronización inicial
- **RECOVERING**: Aplicando operaciones del oplog
- **SECONDARY**: Sincronizado y listo

La sincronización inicial puede tardar horas si la base de datos es grande. Durante este tiempo, el nodo copia todos los datos desde un miembro existente y luego aplica las operaciones del oplog que ocurrieron durante la copia.

**Añadir un nodo sin derecho a voto:**

Si queremos añadir más de 7 nodos (el límite de nodos votantes), debemos crear nodos sin voto:

```javascript
rs.add({
  host: "mongo-rs8.ejemplo.com:27017",
  priority: 0,
  votes: 0  // No participa en elecciones
})
```

Estos nodos replican datos y pueden servir lecturas, pero no participan en las elecciones del primario.

**Eliminar un nodo del Replica Set:**

Para remover un nodo, por ejemplo, si está obsoleto o tiene problemas de hardware:

```javascript
// Primero verificar el ID del nodo
rs.conf().members

// Eliminar por hostname
rs.remove("mongo-rs4.ejemplo.com:27017")

// O eliminar por ID de miembro
rs.remove(3)
```

**Advertencia importante:** Nunca elimines nodos si eso deja al Replica Set sin mayoría. Con 3 nodos, si eliminas uno, aún tienes mayoría (2 de 3). Pero si eliminas dos, el nodo restante no puede elegirse como primario porque no puede obtener mayoría de votos.

**Reemplazar un nodo con problemas:**

Un escenario común es reemplazar un nodo con fallos de hardware:

```javascript
// 1. Eliminar el nodo problemático
rs.remove("mongo-rs2.ejemplo.com:27017")

// 2. Preparar nueva máquina con mismo hostname (o diferente)
// 3. Añadir la nueva máquina
rs.add({
  host: "mongo-rs2-nuevo.ejemplo.com:27017",
  priority: 1
})
```

### **5.3. Manejo de elecciones y failover**

El proceso de elección automática es el corazón de la alta disponibilidad en MongoDB. Comprender cómo funciona es crucial para diagnosticar problemas y optimizar la configuración.

**¿Cuándo ocurre una elección?**

Una elección se dispara cuando:
1. El Replica Set se inicializa por primera vez
2. Un nodo secundario pierde conexión con el primario
3. Se ejecuta un comando de stepDown manualmente
4. Un nodo secundario tiene mayor prioridad y está elegible

**El proceso de elección paso a paso:**

```
1. Secundario detecta que el primario no responde (heartbeat timeout: 10 segundos)
2. Secundario llama a elección y solicita votos de otros miembros
3. Para ganar, necesita mayoría de votos (en cluster de 3: necesita 2 votos)
4. Miembros votan considerando:
   - ¿El candidato tiene datos más recientes? (optime)
   - ¿El candidato tiene prioridad > 0?
   - ¿Este miembro ya votó en esta elección?
5. Si obtiene mayoría: se convierte en PRIMARY
6. Nuevo primario comienza a aceptar escrituras
7. Secundarios se conectan al nuevo primario y continúan replicación
```

**Ejemplo de elección manual (stepDown):**

Para mantenimiento programado, podemos forzar que el primario ceda su rol:

```javascript
// Conectar al primario actual
rs.status().members.find(m => m.stateStr === "PRIMARY")

// Ejecutar stepDown (el primario se convierte en secundario por 60 segundos)
rs.stepDown(60)

// Durante 60 segundos, este nodo NO puede ser reelegido primario
// Útil para mantenimiento sin que vuelva a ser elegido inmediatamente
```

**Configuración de tiempos de elección:**

Podemos ajustar el tiempo de detección y elección según nuestras necesidades:

```javascript
cfg = rs.conf()

// Configuración de settings de elección
cfg.settings = {
  heartbeatIntervalMillis: 2000,    // Heartbeat cada 2 seg (default)
  heartbeatTimeoutSecs: 10,          // Timeout después de 10 seg (default)
  electionTimeoutMillis: 10000,      // Timeout de elección: 10 seg (default)
  catchUpTimeoutMillis: -1           // Tiempo para que nuevo primario se ponga al día
}

rs.reconfig(cfg)
```

**Escenario de failover en producción:**

Imaginemos que el primario (mongo-rs1) sufre un fallo de hardware repentino:

```
17:30:00 - mongo-rs1 falla (corte de energía)
17:30:10 - mongo-rs2 y mongo-rs3 detectan pérdida de heartbeat
17:30:12 - mongo-rs2 llama a elección (tiene datos más recientes)
17:30:13 - mongo-rs3 vota por mongo-rs2
17:30:14 - mongo-rs2 obtiene mayoría (2 de 3 votos) y se convierte en PRIMARY
17:30:15 - Aplicaciones reconectan automáticamente al nuevo primario
17:30:16 - Sistema operacional completamente (downtime: ~16 segundos)
```

**Manejo de split-brain (cerebro dividido):**

Un problema potencial en sistemas distribuidos es el split-brain: cuando la red se particiona y diferentes partes del cluster creen que son el primario. MongoDB previene esto mediante el requisito de mayoría:

```
Escenario: 3 nodos, red se particiona

Partición A: mongo-rs1 (antiguo primario)
Partición B: mongo-rs2, mongo-rs3

Resultado:
- mongo-rs1 no puede obtener mayoría (1 de 3), se convierte en SECONDARY
- mongo-rs2 y mongo-rs3 tienen mayoría (2 de 3), eligen nuevo PRIMARY
- Solo una partición tiene primario: ¡split-brain prevenido!
```

**Monitorizar elecciones:**

Para ver el historial de elecciones:

```javascript
// Ver quién es el primario actual y cuándo fue elegido
rs.status().members.filter(m => m.stateStr === "PRIMARY")

// Ver eventos de replicación en el log
db.adminCommand({ getLog: "global" }).log.filter(entry => 
  entry.includes("election") || entry.includes("transition")
)
```

### **5.4. Configuración de prioridades y árbitros**

Las prioridades y árbitros son herramientas avanzadas para controlar qué nodos pueden convertirse en primario y cómo se toman las decisiones de elección.

**Prioridades de miembros:**

La prioridad determina la preferencia de un nodo para ser elegido primario. Va de 0 (nunca primario) a 1000 (máxima prioridad). Por defecto todos los nodos tienen prioridad 1.

**Casos de uso para diferentes prioridades:**

```javascript
cfg = rs.conf()

// Nodo 0: Datacenter principal, hardware potente → prioridad alta
cfg.members[0].priority = 3

// Nodo 1: Datacenter secundario, hardware medio → prioridad normal
cfg.members[1].priority = 1

// Nodo 2: Datacenter remoto (alta latencia), solo para DR → prioridad baja
cfg.members[2].priority = 0.5

rs.reconfig(cfg)
```

Con esta configuración, mongo-rs1 siempre será preferido como primario a menos que esté caído o inaccesible.

**Nodos de prioridad 0 (nunca primarios):**

```javascript
cfg = rs.conf()

// Configurar nodo 3 como solo lectura (nunca será primario)
cfg.members[3].priority = 0

// Además, ocultarlo de las aplicaciones (no aparece en isMaster)
cfg.members[3].hidden = true

// Retrasar su replicación 1 hora (útil para recuperación de errores)
cfg.members[3].slaveDelay = 3600

rs.reconfig(cfg)
```

Este tipo de nodos son ideales para:
- Backups en caliente sin afectar el cluster principal
- Análisis de datos sin impactar el rendimiento de producción
- Recuperación ante borrados accidentales (gracias al retraso)

**Árbitros (Arbiters):**

Un árbitro es un nodo que participa en elecciones pero NO almacena datos. Su único propósito es proporcionar un voto para romper empates.

**Cuándo usar árbitros:**

✅ **Sí usar árbitro:**
- Tienes 2 nodos con datos y necesitas mayoría (2+1 árbitro = 3 votos)
- Quieres reducir costes (árbitro requiere mínimos recursos)

❌ **No usar árbitro:**
- En producción crítica (no añade redundancia de datos)
- Si puedes permitirte un tercer nodo completo
- En clusters grandes (mejor tener más nodos con datos)

**Configurar un árbitro:**

```javascript
// Añadir árbitro al Replica Set
rs.addArb("mongo-arbiter.ejemplo.com:27017")

// Verificar configuración
rs.conf().members.find(m => m.host.includes("arbiter"))
{
  "_id": 3,
  "host": "mongo-arbiter.ejemplo.com:27017",
  "arbiterOnly": true,
  "priority": 0,
  "votes": 1
}
```

El árbitro ejecuta mongod con configuración mínima:

```yaml
# mongod-arbiter.conf
storage:
  dbPath: /data/arbiter  # Ocupa <100MB
  journal:
    enabled: false  # No necesita journal

replication:
  replSetName: "rsProduccion"

net:
  port: 27017
  bindIp: localhost,mongo-arbiter.ejemplo.com
```

**Tags y Read Preferences para control de topología:**

Para clusters distribuidos geográficamente, podemos etiquetar nodos por ubicación:

```javascript
cfg = rs.conf()

// Etiquetar nodos por datacenter
cfg.members[0].tags = { dc: "madrid", uso: "produccion" }
cfg.members[1].tags = { dc: "barcelona", uso: "produccion" }
cfg.members[2].tags = { dc: "valencia", uso: "analytics" }

rs.reconfig(cfg)
```

Luego las aplicaciones pueden especificar preferencias de lectura:

```javascript
// Aplicación en Madrid: leer del datacenter local
db.collection.find().readPref("nearest", [{ dc: "madrid" }])

// Aplicación de análisis: leer del nodo de analytics
db.collection.find().readPref("secondary", [{ uso: "analytics" }])

// Escrituras siempre van al primario
db.collection.insertOne({ ... })  // Sin opción de preferencia
```

**Write Concern con mayoría:**

Para garantizar durabilidad en escrituras críticas:

```javascript
// Escritura debe replicarse a mayoría antes de confirmar
db.pedidos.insertOne(
  { cliente: "C123", total: 500 },
  { writeConcern: { w: "majority", wtimeout: 5000 } }
)

// Si no se logra mayoría en 5 segundos, lanza error
// Pero la escritura eventualmente se completará
```

**Configuración completa de ejemplo para producción:**

```javascript
{
  "_id": "rsProduccion",
  "version": 5,
  "members": [
    {
      "_id": 0,
      "host": "mongo1.madrid.ejemplo.com:27017",
      "priority": 3,
      "tags": { "dc": "madrid", "rack": "A1" }
    },
    {
      "_id": 1,
      "host": "mongo2.madrid.ejemplo.com:27017",
      "priority": 2,
      "tags": { "dc": "madrid", "rack": "A2" }
    },
    {
      "_id": 2,
      "host": "mongo3.barcelona.ejemplo.com:27017",
      "priority": 1,
      "tags": { "dc": "barcelona", "rack": "B1" }
    },
    {
      "_id": 3,
      "host": "mongo4.valencia.ejemplo.com:27017",
      "priority": 0,
      "hidden": true,
      "slaveDelay": 3600,
      "tags": { "dc": "valencia", "uso": "backup" }
    }
  ],
  "settings": {
    "heartbeatIntervalMillis": 2000,
    "electionTimeoutMillis": 10000,
    "getLastErrorModes": {
      "multiDC": { "dc": 2 }  // Escrituras deben ir a 2 DCs diferentes
    }
  }
}
```

Con esta configuración, tenemos un Replica Set robusto con preferencia por el datacenter de Madrid, redundancia geográfica en Barcelona, un nodo de backup retrasado en Valencia, y la garantía de que las escrituras críticas se replican a múltiples datacenters.

Los Replica Sets proporcionan la base para la alta disponibilidad en MongoDB. En el siguiente capítulo, exploraremos el sharding, que nos permitirá escalar horizontalmente no solo en redundancia, sino también en capacidad de procesamiento y almacenamiento.

***

¿Continuamos con el Capítulo 6 (Sharding)?