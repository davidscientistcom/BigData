## **6. Escalado Horizontal: Sharding**

### **6.1. ¿Cuándo implementar sharding?**

El sharding es una decisión arquitectónica importante que añade complejidad operacional significativa a nuestro sistema. Por ello, debemos implementarlo solo cuando sea realmente necesario, no como una optimización prematura. Un Replica Set bien configurado puede manejar fácilmente decenas de miles de operaciones por segundo y varios cientos de gigabytes de datos antes de que el sharding sea necesario. [mongodb](https://www.mongodb.com/docs/manual/sharding/)

**Indicadores de que necesitamos sharding:**

El primer indicador es el **crecimiento del conjunto de datos**. Si nuestros datos están creciendo y proyectamos que superarán la capacidad de almacenamiento de un único servidor en los próximos 6-12 meses, debemos planificar el sharding. Por ejemplo, si tenemos 500GB de datos y crecemos 100GB mensuales, en un año tendremos 1.7TB. Si nuestros servidores tienen discos de 2TB, estamos acercándonos al límite.

El segundo indicador es el **working set** (conjunto de datos activos). El working set es la porción de datos que se accede frecuentemente y que idealmente debe caber en RAM. MongoDB funciona óptimamente cuando el working set cabe en memoria. Si el working set es 100GB pero solo tenemos 64GB de RAM, empezaremos a ver page faults frecuentes y degradación del rendimiento. El sharding permite distribuir el working set entre múltiples máquinas, cada una con su propia RAM.

El tercer indicador es el **throughput de operaciones**. Si estamos ejecutando 50,000 operaciones por segundo y el servidor está constantemente al 90% de CPU o I/O, hemos alcanzado el límite de capacidad. El sharding distribuye la carga entre múltiples servidores, multiplicando efectivamente nuestra capacidad de procesamiento.

**Ejemplo de análisis de necesidad:**

Supongamos que administramos una plataforma de IoT que recibe telemetría de sensores:

```javascript
// Análisis de crecimiento de datos
db.sensores_telemetria.stats()
{
  "size": 536870912000,        // 500GB de datos
  "count": 5000000000,          // 5 mil millones de documentos
  "avgObjSize": 107,            // ~100 bytes por documento
  "storageSize": 322122547200   // 300GB en disco (compresión)
}

// Crecimiento: 10 millones de documentos diarios
// Proyección a 12 meses: 500GB + (10M docs/día * 365 días * 100 bytes) = 865GB

// Working set actual
db.serverStatus().wiredTiger.cache
{
  "bytes currently in the cache": 34359738368,  // 32GB
  "maximum bytes configured": 34359738368,
  "cache overflow score": 45  // ¡Presión de memoria!
}

// Operaciones por segundo
// (medido durante 1 minuto en hora pico)
Inserts: 8,500 ops/s
Queries: 12,000 ops/s
Total: 20,500 ops/s
```

**Decisión:** Con 500GB actuales, crecimiento a 865GB en un año, working set que excede la RAM disponible (cache overflow score > 0), y 20K ops/s, claramente necesitamos sharding.

**Cuándo NO implementar sharding:**

- Datos menores a 500GB con crecimiento lento
- Capacidad sobrante en CPU y I/O (< 60% uso promedio)
- Working set cabe confortablemente en RAM disponible
- Equipo pequeño sin experiencia en administración de clusters distribuidos
- En estos casos, el escalado vertical (más RAM, SSD más rápidos) es más apropiado

### **6.2. Arquitectura de un cluster con sharding**

Un cluster con sharding en MongoDB está compuesto por tres tipos de componentes que trabajan juntos para proporcionar distribución transparente de datos. [digitalocean](https://www.digitalocean.com/community/tutorials/how-to-use-sharding-in-mongodb)

**Los componentes del cluster sharded:**

**Config Servers:** Almacenan todos los metadatos del cluster. Esto incluye qué bases de datos y colecciones están fragmentadas, cuál es la shard key de cada colección, qué rangos de datos (chunks) existen, y en qué shard reside cada chunk. Los Config Servers deben ser un Replica Set de al menos 3 nodos para garantizar disponibilidad. Si los Config Servers fallan completamente, el cluster entero se vuelve de solo lectura (no se pueden mover chunks ni realizar operaciones de administración del cluster). [mongodb](https://www.mongodb.com/docs/manual/core/config-shard/)

**Shards:** Cada shard contiene un subconjunto de los datos. En producción, cada shard debe ser un Replica Set completo (no un servidor standalone) para garantizar alta disponibilidad. Si un shard falla, solo los datos de ese shard se vuelven inaccesibles, el resto del cluster sigue funcionando. Por ejemplo, en un cluster con 4 shards, si uno falla, aún tenemos acceso al 75% de los datos. [mongodb](https://www.mongodb.com/docs/manual/sharding/)

**Mongos (Query Routers):** Son procesos stateless que actúan como punto de entrada para las aplicaciones. No almacenan datos, solo enrutan las operaciones. Cuando una aplicación envía una consulta al mongos, este consulta los Config Servers para determinar qué shards contienen los datos relevantes, envía la consulta a esos shards, recopila los resultados y los devuelve a la aplicación. Las aplicaciones no necesitan saber que el cluster está fragmentado, se conectan al mongos como si fuera un mongod normal. [digitalocean](https://www.digitalocean.com/community/tutorials/how-to-use-sharding-in-mongodb)

**Arquitectura de ejemplo para un cluster de producción:**

```
                    ┌────────────────────────────────┐
                    │     APLICACIONES               │
                    └───────────┬───────────┬────────┘
                                │           │
                    ┌───────────▼────┐  ┌──▼──────────┐
                    │   MONGOS 1     │  │  MONGOS 2   │
                    │  (app-srv1)    │  │  (app-srv2) │
                    └───────┬────────┘  └──────┬──────┘
                            │                  │
                            └────────┬─────────┘
                                     │
                    ┌────────────────┼──────────────────┐
                    │                │                  │
         ┌──────────▼──────┐  ┌─────▼──────┐  ┌───────▼────────┐
         │   SHARD 1 (RS)  │  │ SHARD 2(RS)│  │  SHARD 3 (RS)  │
         │  ┌───┐┌───┐┌───┐│  │┌───┐┌───┐  │  │ ┌───┐┌───┐┌───┐│
         │  │P1 ││S1 ││S1 ││  ││P2 ││S2 │  │  │ │P3 ││S3 ││S3 ││
         │  └───┘└───┘└───┘│  │└───┘└───┘  │  │ └───┘└───┘└───┘│
         └──────────────────┘  └────────────┘  └─────────────────┘
                                     │
                    ┌────────────────┼──────────────┐
                    │                │              │
            ┌───────▼──────┐  ┌─────▼──────┐  ┌───▼─────────┐
            │ CONFIG SVR 1 │  │CONFIG SVR 2│  │CONFIG SVR 3 │
            │   (Primary)  │  │ (Secondary)│  │ (Secondary) │
            └──────────────┘  └────────────┘  └─────────────┘
```

**Chunks y distribución de datos:**

MongoDB divide los datos en **chunks**. Un chunk es un rango contiguo de valores de la shard key, con un tamaño máximo de 64MB por defecto (configurable entre 1MB y 1024MB). Los chunks son la unidad de migración: cuando el balanceador decide que un shard tiene demasiados chunks en comparación con otros, mueve chunks completos entre shards. [xuchao](https://www.xuchao.org/docs/mongodb/core/sharding-balancer-administration.html)

**Ejemplo conceptual de chunks:**

```javascript
// Colección: pedidos
// Shard key: { cliente_id: 1 }
// Supongamos valores de cliente_id de "C0000" a "C9999"

Chunks en el cluster:
┌─────────────────────────────────────────────────────────┐
│ Chunk 1: { cliente_id: MinKey } → { cliente_id: "C2500" }│
│ Ubicación: shard1                                        │
│ Tamaño: 58MB                                             │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ Chunk 2: { cliente_id: "C2500" } → { cliente_id: "C5000"}│
│ Ubicación: shard2                                        │
│ Tamaño: 64MB (máximo alcanzado, se dividirá pronto)     │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ Chunk 3: { cliente_id: "C5000" } → { cliente_id: "C7500"}│
│ Ubicación: shard3                                        │
│ Tamaño: 61MB                                             │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ Chunk 4: { cliente_id: "C7500" } → { cliente_id: MaxKey }│
│ Ubicación: shard1                                        │
│ Tamaño: 55MB                                             │
└─────────────────────────────────────────────────────────┘
```

Cuando el Chunk 2 supere 64MB, MongoDB lo dividirá automáticamente en dos chunks de ~32MB cada uno. Si tras esta división, shard2 tiene significativamente más chunks que shard1 o shard3, el balanceador moverá algunos chunks para equilibrar la carga.

### **6.3. Selección de la shard key**

La selección de la shard key es la decisión más crítica en un despliegue con sharding. Una mala elección puede resultar en distribución desigual de datos, hotspots de escritura, o consultas que deben tocar todos los shards. **Una vez que una colección está fragmentada con una shard key, no se puede cambiar sin recrear la colección** (aunque desde MongoDB 5.0 se puede refinar añadiendo campos adicionales). [mongodb](https://www.mongodb.com/docs/manual/core/sharding-choose-a-shard-key/)

**Características de una buena shard key:**

**Alta cardinalidad:** La shard key debe tener muchos valores únicos posibles. Si usamos un campo con solo 10 valores posibles, solo podremos tener como máximo 10 chunks iniciales, limitando severamente la distribución. [learnmongo](https://learnmongo.com/scaling-mongodb-with-sharding-setup-best-practices/)

```javascript
// Mala cardinalidad: campo "tipo" con solo 3 valores
{ tipo: "A" }  // 33% de los datos
{ tipo: "B" }  // 33% de los datos
{ tipo: "C" }  // 34% de los datos
// Máximo 3 chunks, no escalable más allá de 3 shards

// Buena cardinalidad: campo "usuario_id" con millones de valores
{ usuario_id: "U0001" }
{ usuario_id: "U0002" }
// ... millones de valores diferentes
// Puede crear miles de chunks, distribución flexible
```

**Distribución uniforme:** Los valores de la shard key deben distribuirse uniformemente. Si el 80% de los documentos tienen el mismo valor de shard key, esos documentos irán al mismo chunk y eventualmente al mismo shard, creando un hotspot. [mongodb](https://www.mongodb.com/docs/manual/core/sharding-choose-a-shard-key/)

```javascript
// Mala distribución: campo "pais" en dataset global
{ pais: "USA" }  // 60% de los datos → un shard sobrecargado
{ pais: "China" } // 20% de los datos
{ pais: "otros" } // 20% de los datos distribuido

// Buena distribución: hash del usuario_id
{ usuario_id: "hashed" }  // Distribución pseudo-aleatoria uniforme
```

**No monotónicamente creciente:** Campos que siempre aumentan (timestamps, ObjectId, secuencias auto-incrementales) causan que todas las escrituras nuevas vayan al mismo chunk (el que contiene los valores más altos), creando un hotspot de escritura. [mongodb](https://www.mongodb.com/docs/manual/core/sharding-choose-a-shard-key/)

```javascript
// Problema con timestamp como shard key:
{ timestamp: ISODate("2026-02-09T17:00:00Z") }
{ timestamp: ISODate("2026-02-09T17:00:01Z") }
{ timestamp: ISODate("2026-02-09T17:00:02Z") }
// Todas las escrituras van al chunk con el rango temporal más reciente
// Un solo shard maneja TODAS las escrituras → hotspot

// Solución 1: Usar hash del timestamp
{ timestamp: "hashed" }
// Distribución uniforme, pero perdemos consultas por rango de fechas

// Solución 2: Shard key compuesta
{ cliente_id: 1, timestamp: 1 }
// Distribuye por cliente primero, luego por fecha
```

**Facilita patrones de consulta comunes:** La shard key ideal permite que la mayoría de las consultas contengan la shard key, permitiendo a mongos dirigir las consultas a un único shard (targeted query) en lugar de consultar todos los shards (broadcast query). [mongodb](https://www.mongodb.com/docs/manual/core/sharding-choose-a-shard-key/)

**Ejemplos de selección de shard key por caso de uso:**

**Caso 1: Aplicación multi-tenant (SaaS)**

```javascript
// Cada cliente (tenant) accede solo a sus datos
// Shard key ideal: tenant_id
db.datos_cliente.createIndex({ tenant_id: 1, fecha: 1 })
sh.shardCollection("app.datos_cliente", { tenant_id: 1, fecha: 1 })

// Ventajas:
// - Consultas por tenant siempre van a un solo shard
// - Distribución natural si tenemos muchos tenants
// - Aislamiento de datos por tenant
```

**Caso 2: Sistema de logs/telemetría**

```javascript
// Alto volumen de escrituras con timestamps
// Shard key: hash de algún identificador único
db.logs.createIndex({ aplicacion: "hashed" })
sh.shardCollection("logs.aplicacion", { aplicacion: "hashed" })

// O mejor: compuesta con locality
db.logs.createIndex({ servidor_id: 1, timestamp: 1 })
sh.shardCollection("logs.aplicacion", { servidor_id: 1, timestamp: 1 })

// Ventajas:
// - Escrituras distribuidas uniformemente
// - Consultas por servidor + rango de tiempo son eficientes
```

**Caso 3: Plataforma de e-commerce**

```javascript
// Acceso principal por usuario para sus pedidos
// Shard key compuesta: usuario_id + fecha
db.pedidos.createIndex({ usuario_id: 1, fecha_pedido: -1 })
sh.shardCollection("tienda.pedidos", { usuario_id: 1, fecha_pedido: -1 })

// Consulta típica (targeted):
db.pedidos.find({ 
  usuario_id: "U123456",
  fecha_pedido: { $gte: ISODate("2026-01-01") }
})
// mongos sabe exactamente a qué shard ir

// Consulta sin shard key (broadcast):
db.pedidos.find({ 
  total: { $gt: 1000 }
})
// mongos debe consultar todos los shards → más lento
```

**Herramienta de análisis de shard key:**

Desde MongoDB 4.4, existe el comando `analyzeShardKey` que ayuda a evaluar candidatos:

```javascript
db.adminCommand({
  analyzeShardKey: "tienda.pedidos",
  key: { usuario_id: 1, fecha_pedido: -1 },
  // Analiza queries durante 1 hora
  sampleRate: 100,
  sampleSize: 10000
})
```

Este comando analiza los patrones de consulta reales y proporciona métricas sobre qué porcentaje de consultas serían targeted vs broadcast con esa shard key.

### **6.4. Balanceo de datos entre shards**

El **balancer** es un proceso automático que ejecuta en el primario de los Config Servers y se encarga de mantener la distribución uniforme de chunks entre los shards. [docs.huihoo](https://docs.huihoo.com/mongodb/3.2/core/sharding-chunk-migration.html)

**Funcionamiento del balancer:**

El balancer se ejecuta en segundo plano constantemente (por defecto). Cada cierto tiempo (cada 10 segundos aproximadamente), evalúa la distribución de chunks:

1. Cuenta cuántos chunks tiene cada shard
2. Calcula el número ideal de chunks por shard: `total_chunks / numero_shards`
3. Si algún shard tiene significativamente más chunks que el ideal (diferencia > threshold), inicia migraciones

**Proceso de migración de un chunk:** [xuchao](https://www.xuchao.org/docs/mongodb/core/sharding-balancer-administration.html)

```
1. Balancer selecciona chunk a migrar (del shard más cargado al menos cargado)
2. Shard destino comienza a copiar documentos del chunk desde shard origen
3. Durante la copia, el shard origen sigue aceptando escrituras
4. Shard destino aplica las operaciones incrementales (tracking de cambios)
5. Cuando está casi sincronizado, se bloquean brevemente escrituras (~100ms)
6. Config servers actualizan metadatos: chunk ahora está en shard destino
7. Se desbloquean escrituras
8. Shard origen borra su copia del chunk (asíncrono)
```

**Configuración del balancer:**

```javascript
// Ver estado del balancer
sh.getBalancerState()  // true = activo

// Parar el balancer (para mantenimiento)
sh.stopBalancer()

// Reiniciar el balancer
sh.startBalancer()

// Ver si hay migraciones en progreso
sh.isBalancerRunning()

// Configurar ventana horaria para balanceo
db.settings.updateOne(
  { _id: "balancer" },
  {
    $set: {
      activeWindow: {
        start: "23:00",  // 11 PM
        stop: "06:00"    // 6 AM
      }
    }
  },
  { upsert: true }
)
// Balancer solo migrará chunks entre 11 PM y 6 AM
```

**Configurar ventana de balanceo es crítico en producción:** Las migraciones de chunks, aunque diseñadas para minimizar el impacto, consumen ancho de banda y recursos. Es mejor programarlas para horarios de bajo tráfico.

**Threshold de migración:**

```javascript
// Configurar diferencia mínima para iniciar migración
db.settings.updateOne(
  { _id: "balancer" },
  { $set: { _secondaryThrottle: true } },
  { upsert: true }
)

// Ver configuración actual
sh.getBalancerConfig()
```

**Monitorizar migraciones:**

```javascript
// Ver historial de migraciones recientes
use config
db.changelog.find({ what: "moveChunk.commit" })
  .sort({ time: -1 })
  .limit(10)
  .pretty()

// Salida ejemplo:
{
  "_id": "rs0-2026-02-09T17:38:45-602f3e4a",
  "server": "mongo1.ejemplo.com",
  "time": ISODate("2026-02-09T17:38:45.123Z"),
  "what": "moveChunk.commit",
  "ns": "tienda.pedidos",
  "details": {
    "min": { "usuario_id": "U5000" },
    "max": { "usuario_id": "U5500" },
    "from": "shard01",
    "to": "shard02",
    "cloned": 250000,       // Documentos copiados
    "clonedBytes": 67108864, // 64MB
    "catchup": 1234,        // Operaciones aplicadas durante catchup
    "steady": 567           // Operaciones durante fase estable
  }
}
```

**Problemas comunes de balanceo:**

**Jumbo chunks:** Chunks que superan el tamaño máximo pero no pueden dividirse porque todos los documentos tienen el mismo valor de shard key: [xuchao](https://www.xuchao.org/docs/mongodb/core/sharding-balancer-administration.html)

```javascript
// Identificar jumbo chunks
use config
db.chunks.find({ jumbo: true })

// Causa típica: shard key con baja cardinalidad
// Si muchos documentos tienen usuario_id="U0001"
// Todos van al mismo chunk → chunk "jumbo"
// Solución: refinar shard key añadiendo campo con más cardinalidad
```

**Hotspots persistentes:** Un shard recibe desproporcionadamente más tráfico:

```javascript
// Diagnosticar: ver distribución de chunks
sh.status()

// Buscar líneas como:
"shard01":  45 chunks
"shard02":  44 chunks  
"shard03":  12 chunks  // ← Desbalanceado

// Posible causa: shard key monotónica
// Todas las escrituras nuevas van a un rango específico
// Solución: pre-splitting o cambiar shard key strategy
```

**Pre-splitting para distribución inicial:**

Si sabemos que vamos a cargar grandes cantidades de datos, podemos crear chunks antes de tiempo:

```javascript
// Habilitar sharding en la DB
sh.enableSharding("tienda")

// Crear índice para shard key
db.pedidos.createIndex({ usuario_id: 1 })

// Pre-dividir en múltiples chunks antes de insertar datos
for (var i = 0; i < 10000; i += 1000) {
  sh.splitAt("tienda.pedidos", { usuario_id: "U" + i.toString().padStart(5, '0') })
}

// Ahora shard collection
sh.shardCollection("tienda.pedidos", { usuario_id: 1 })

// Resultado: 10 chunks iniciales distribuidos uniformemente
// Al cargar datos, ya están distribuidos desde el principio
```

El sharding proporciona capacidad de escalado prácticamente ilimitada, pero requiere planificación cuidadosa y administración continua. En el próximo capítulo, pondremos todo esto en práctica con un ejemplo completo de despliegue de un cluster con sharding desde cero.

***

¿Continuamos con el Capítulo 7 (Ejemplo Práctico Completo)?