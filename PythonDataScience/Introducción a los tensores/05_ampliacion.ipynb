{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3697984-ad43-4b1f-aa6a-914ab15c8e47",
   "metadata": {},
   "source": [
    "### 1. **Broadcasting en PyTorch**\n",
    "Broadcasting es una técnica que permite realizar operaciones aritméticas en tensores de dimensiones diferentes. PyTorch aplica reglas específicas para \"expandir\" dimensiones de manera automática, lo que permite combinar tensores de diferentes formas en una misma operación sin necesidad de ajustar sus dimensiones manualmente.\n",
    "\n",
    "#### Reglas del Broadcasting\n",
    "1. **Comparación de dimensiones**: PyTorch comienza a comparar dimensiones de derecha a izquierda.\n",
    "2. **Dimensiones iguales**: Si dos dimensiones son iguales, no se requiere broadcasting.\n",
    "3. **Dimensión igual a 1**: Una dimensión de tamaño 1 puede expandirse para igualar la otra dimensión (por ejemplo, un tensor de forma `[3, 1]` puede combinarse con uno de forma `[3, 4]`).\n",
    "4. **Dimensiones faltantes**: Si un tensor tiene menos dimensiones, se considerará que las dimensiones faltantes son de tamaño 1 en la operación.\n",
    "\n",
    "#### Ejemplos Prácticos\n",
    "Supongamos que tenemos dos tensores de diferentes formas:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Tensor de forma [3, 1]\n",
    "tensor_a = torch.tensor([[1], [2], [3]])\n",
    "\n",
    "# Tensor de forma [1, 4]\n",
    "tensor_b = torch.tensor([[1, 2, 3, 4]])\n",
    "```\n",
    "\n",
    "Si sumamos `tensor_a` y `tensor_b`, PyTorch expandirá automáticamente las dimensiones de cada tensor para que coincidan:\n",
    "\n",
    "```python\n",
    "result = tensor_a + tensor_b\n",
    "print(result)\n",
    "# tensor([[2, 3, 4, 5],\n",
    "#         [3, 4, 5, 6],\n",
    "#         [4, 5, 6, 7]])\n",
    "```\n",
    "\n",
    "En este caso:\n",
    "- `tensor_a` se expande de `[3, 1]` a `[3, 4]`, repitiendo el valor en la segunda dimensión.\n",
    "- `tensor_b` se expande de `[1, 4]` a `[3, 4]`, repitiendo el valor en la primera dimensión.\n",
    "\n",
    "Este tipo de expansión se realiza en memoria sin duplicar los datos físicamente, haciendo las operaciones más eficientes.\n",
    "\n",
    "#### Funciones `expand()` y `expand_as()`\n",
    "A veces, es útil definir explícitamente el broadcasting, y para esto usamos `expand()` y `expand_as()`.\n",
    "\n",
    "- **`expand(sizes)`**: Permite especificar las dimensiones a las cuales debe expandirse un tensor.\n",
    "  ```python\n",
    "  expanded_tensor = tensor_a.expand(3, 4)\n",
    "  print(expanded_tensor)\n",
    "  # tensor([[1, 1, 1, 1],\n",
    "  #         [2, 2, 2, 2],\n",
    "  #         [3, 3, 3, 3]])\n",
    "  ```\n",
    "\n",
    "- **`expand_as(tensor)`**: Expande un tensor para que coincida en forma con otro tensor.\n",
    "  ```python\n",
    "  expanded_tensor_as = tensor_a.expand_as(result)\n",
    "  print(expanded_tensor_as)\n",
    "  # tensor([[1, 1, 1, 1],\n",
    "  #         [2, 2, 2, 2],\n",
    "  #         [3, 3, 3, 3]])\n",
    "  ```\n",
    "\n",
    "El broadcasting facilita la manipulación de tensores en redes neuronales al evitar el ajuste manual de dimensiones, especialmente en tareas como la combinación de vectores y matrices de tamaño variable.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Descomposición en Valores Singulares (SVD)**\n",
    "La Descomposición en Valores Singulares es una técnica en álgebra lineal que descompone una matriz \\( A \\) en tres matrices: \\( U \\), \\( S \\) y \\( V^T \\), donde:\n",
    "\n",
    "- \\( U \\) y \\( V^T \\) son matrices ortogonales.\n",
    "- \\( S \\) es una matriz diagonal que contiene los valores singulares.\n",
    "\n",
    "Esta técnica es útil en tareas de reducción de dimensionalidad, compresión de datos, y análisis de características en redes neuronales.\n",
    "\n",
    "#### Ejemplo Práctico\n",
    "Consideremos una matriz sencilla:\n",
    "\n",
    "```python\n",
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "U, S, V = torch.svd(matrix)\n",
    "```\n",
    "\n",
    "En este caso:\n",
    "- `U` contiene los vectores ortogonales de la matriz original.\n",
    "- `S` contiene los valores singulares (en forma de vector).\n",
    "- `V` contiene los vectores ortogonales de la transpuesta.\n",
    "\n",
    "```python\n",
    "print(\"U:\\n\", U)\n",
    "print(\"S:\\n\", S)\n",
    "print(\"V:\\n\", V)\n",
    "```\n",
    "\n",
    "### Propiedades de SVD\n",
    "1. **Reconstrucción de la Matriz**: La matriz original puede reconstruirse como `U * diag(S) * V^T`.\n",
    "2. **Reducción de Dimensionalidad**: Los valores más altos de `S` suelen contener la mayor parte de la información, permitiendo reducir la matriz eliminando los valores más pequeños.\n",
    "3. **Aplicaciones en Redes Neuronales**: La SVD es útil para optimizar redes neuronales grandes, identificar redundancias y reducir el tamaño de modelos complejos.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Distribuciones en PyTorch**\n",
    "Sí, PyTorch permite generar varias distribuciones directamente con su módulo `torch.distributions`, aunque la librería `numpy` también es una buena opción. PyTorch soporta diversas distribuciones, y el uso de este módulo permite mantener los datos en tensores de PyTorch, lo que facilita su uso en modelos de redes neuronales.\n",
    "\n",
    "#### Distribuciones Comunes en PyTorch\n",
    "1. **Distribución Normal (Gaussiana)**\n",
    "   ```python\n",
    "   from torch.distributions import Normal\n",
    "\n",
    "   normal_dist = Normal(0, 1)  # media=0, desviación estándar=1\n",
    "   sample = normal_dist.sample((5,))  # 5 muestras\n",
    "   print(sample)\n",
    "   ```\n",
    "\n",
    "2. **Distribución Bernoulli**\n",
    "   - Modela experimentos binarios con probabilidad `p`.\n",
    "   ```python\n",
    "   from torch.distributions import Bernoulli\n",
    "\n",
    "   bernoulli_dist = Bernoulli(torch.tensor([0.7, 0.2]))\n",
    "   sample = bernoulli_dist.sample((3,))  # 3 muestras\n",
    "   print(sample)\n",
    "   ```\n",
    "\n",
    "3. **Distribución Poisson**\n",
    "   - Modela la frecuencia de eventos en intervalos de tiempo con tasa `lambda`.\n",
    "   ```python\n",
    "   from torch.distributions import Poisson\n",
    "\n",
    "   poisson_dist = Poisson(2.0)  # tasa lambda=2\n",
    "   sample = poisson_dist.sample((5,))\n",
    "   print(sample)\n",
    "   ```\n",
    "\n",
    "4. **Distribución Uniforme**\n",
    "   - Distribuye valores aleatorios en un rango entre `low` y `high`.\n",
    "   ```python\n",
    "   from torch.distributions import Uniform\n",
    "\n",
    "   uniform_dist = Uniform(0, 10)\n",
    "   sample = uniform_dist.sample((3,))\n",
    "   print(sample)\n",
    "   ```\n",
    "\n",
    "#### Ventajas de Usar PyTorch sobre NumPy\n",
    "- **Compatibilidad**: Las muestras generadas en `torch.distributions` son tensores de PyTorch, lo que facilita su integración en modelos sin necesidad de conversión.\n",
    "- **Soporte de Gradientes**: Las distribuciones de PyTorch pueden usarse en redes neuronales con gradientes, siendo compatibles con el sistema autograd para tareas de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17bfd0-acac-40c0-a357-de35af80dd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
