
## üìö √çndice de Contenidos

1. [Introducci√≥n al PLN](#1-introducci√≥n-al-pln)
2. [Terminolog√≠a Fundamental](#2-terminolog√≠a-fundamental)
3. [Tokenizaci√≥n](#3-tokenizaci√≥n)
4. [Limpieza y Preprocesamiento](#4-limpieza-y-preprocesamiento)
5. [Normalizaci√≥n de Texto](#5-normalizaci√≥n-de-texto)
6. [Visualizaci√≥n de Datos de Texto](#6-visualizaci√≥n-de-datos-de-texto)
7. [Part-of-Speech (POS) Tagging](#7-part-of-speech-pos-tagging)
8. [Ejercicios Pr√°cticos](#8-ejercicios-pr√°cticos)



## 1. Introducci√≥n al PLN

### ¬øQu√© es el Procesamiento del Lenguaje Natural?

El Procesamiento del Lenguaje Natural (PLN o NLP en ingl√©s) es una rama de la inteligencia artificial que se centra en la **interacci√≥n entre computadoras y lenguaje humano**. El objetivo es permitir que las m√°quinas entiendan, interpreten y generen lenguaje humano de manera √∫til.

### ¬øPor qu√© es dif√≠cil procesar el lenguaje natural?

El lenguaje humano presenta varios desaf√≠os √∫nicos:

1. **Ambig√ºedad**: Las palabras pueden tener m√∫ltiples significados
   - "Voy al **banco** a depositar dinero" (instituci√≥n financiera)
   - "Me sent√© en el **banco** del parque" (asiento)

2. **Contexto**: El significado depende del contexto
   - "Hace **fr√≠o**" (temperatura baja)
   - "Me dej√≥ **fr√≠o** su respuesta" (indiferencia)

3. **Variabilidad**: Existen m√∫ltiples formas de expresar la misma idea
   - "El perro persigue al gato"
   - "El gato es perseguido por el perro"
   - "Al gato lo persigue el perro"

4. **Errores y variaciones**: Errores ortogr√°ficos, jerga, dialectos
   - "Ke onda we" vs "¬øQu√© pasa, amigo?"

5. **Conocimiento del mundo**: Requiere informaci√≥n contextual
   - "El trofeo no cabe en la maleta porque es muy grande"
   - ¬øQu√© es muy grande? ¬øEl trofeo o la maleta?

### Aplicaciones del PLN en la vida real

- **Asistentes virtuales**: Siri, Alexa, Google Assistant
- **Traducci√≥n autom√°tica**: Google Translate, DeepL
- **An√°lisis de sentimientos**: An√°lisis de opiniones en redes sociales
- **Chatbots**: Atenci√≥n al cliente automatizada
- **Resumen autom√°tico**: Generaci√≥n de res√∫menes de documentos
- **Correcci√≥n ortogr√°fica y gramatical**: Microsoft Word, Grammarly
- **Motores de b√∫squeda**: Google, Bing
- **Clasificaci√≥n de emails**: Spam vs No Spam



## 2. Terminolog√≠a Fundamental

Antes de sumergirnos en el c√≥digo, es crucial entender los t√©rminos b√°sicos del PLN.

### Corpus (plural: Corpora)

Un **corpus** es una colecci√≥n de documentos de texto. Es el conjunto de datos con el que trabajamos.

**Ejemplos:**
- Una colecci√≥n de art√≠culos de noticias
- Todos los tweets sobre un tema espec√≠fico
- Conjunto de rese√±as de productos
- Documentos legales de una empresa

```python
# Ejemplo de corpus simple
corpus = [
    "Me encanta el procesamiento del lenguaje natural",
    "Python es un lenguaje de programaci√≥n excelente para PLN",
    "Los modelos de lenguaje est√°n revolucionando la IA"
]
```

### Documento

Un **documento** es una unidad individual dentro del corpus. Puede ser:
- Un art√≠culo completo
- Un tweet
- Una rese√±a
- Un p√°rrafo

### Token

Un **token** es la unidad m√≠nima de procesamiento. Generalmente es una palabra, pero tambi√©n puede ser:
- Un n√∫mero
- Un signo de puntuaci√≥n
- Un s√≠mbolo especial

**Ejemplo:**
```
Texto original: "¬°Hola, mundo!"
Tokens: ["¬°", "Hola", ",", "mundo", "!"]
```

### Tokenizaci√≥n

Es el **proceso de dividir un texto en tokens**. Es el primer paso fundamental en casi cualquier tarea de PLN.

### N-gramas

Los **n-gramas** son secuencias continuas de n elementos (palabras) del texto.

**Ejemplo con la frase: "Me gusta programar en Python"**

- **Unigramas (n=1)**: "Me", "gusta", "programar", "en", "Python"
- **Bigramas (n=2)**: "Me gusta", "gusta programar", "programar en", "en Python"
- **Trigramas (n=3)**: "Me gusta programar", "gusta programar en", "programar en Python"

Los n-gramas son √∫tiles para:
- Capturar contexto local
- Modelado de lenguaje
- Extracci√≥n de frases clave

### Morfema

El **morfema** es la forma base de una palabra, sin flexiones ni derivaciones.

**Ejemplo:**
```
Palabra: "antinacionalista"
‚îú‚îÄ‚îÄ Prefijo: "anti-" (morfema)
‚îú‚îÄ‚îÄ Ra√≠z: "nacional" (morfema base)
‚îî‚îÄ‚îÄ Sufijo: "-ista" (morfema)
```

### L√©xico

El **l√©xico** es el conjunto completo de palabras y frases utilizadas en un idioma o dominio espec√≠fico.



## 3. Tokenizaci√≥n

La tokenizaci√≥n es el proceso de **dividir el texto en unidades m√°s peque√±as** (tokens). Es el primer paso en casi cualquier pipeline de PLN.

### 3.1 Instalaci√≥n de bibliotecas necesarias

Primero, instalamos las bibliotecas que vamos a usar:

```python
# En la terminal o celda de Jupyter
!pip install nltk
!pip install gensim
!pip install wordcloud
!pip install matplotlib
!pip install spacy
```

### 3.2 Configuraci√≥n inicial de NLTK

```python
import nltk

# Descargar recursos necesarios
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

print("¬°Recursos descargados exitosamente!")
```

### 3.3 Tokenizaci√≥n por espacios en blanco

La forma m√°s simple de tokenizaci√≥n es dividir el texto por espacios.

```python
texto = "El procesamiento del lenguaje natural es fascinante"

# Tokenizaci√≥n simple con split()
tokens_simple = texto.split()
print("Tokens:", tokens_simple)
# Output: ['El', 'procesamiento', 'del', 'lenguaje', 'natural', 'es', 'fascinante']

# Contar tokens
print(f"N√∫mero de tokens: {len(tokens_simple)}")
```

**Problema con este enfoque:**

```python
texto_problematico = "¬°Hola! ¬øC√≥mo est√°s? Me llamo Juan."
tokens = texto_problematico.split()
print("Tokens:", tokens)
# Output: ['¬°Hola!', '¬øC√≥mo', 'est√°s?', 'Me', 'llamo', 'Juan.']
# Nota: La puntuaci√≥n queda pegada a las palabras
```

### 3.4 Tokenizaci√≥n con expresiones regulares

Podemos usar expresiones regulares para tokenizar de forma m√°s sofisticada.

```python
import re

texto = "El correo es: juan@email.com, y el tel√©fono: 555-1234"

# Tokenizar separando por espacios y signos de puntuaci√≥n
tokens_re = re.findall(r'\w+', texto)
print("Tokens:", tokens_re)
# Output: ['El', 'correo', 'es', 'juan', 'email', 'com', 'y', 'el', 'tel√©fono', '555', '1234']

# Tokenizar conservando la puntuaci√≥n
tokens_re2 = re.findall(r'\w+|[^\w\s]', texto)
print("Tokens con puntuaci√≥n:", tokens_re2)
```

**Ejercicio 1:** Modifica la expresi√≥n regular para que tambi√©n capture emails completos.

### 3.5 Tokenizaci√≥n de palabras con NLTK

NLTK ofrece tokenizadores m√°s robustos que manejan mejor la puntuaci√≥n.

```python
from nltk.tokenize import word_tokenize

texto = "Dr. Smith lleg√≥ a las 9:30 a.m. ¬°Qu√© puntualidad!"

tokens = word_tokenize(texto, language='spanish')
print("Tokens NLTK:", tokens)
# Output: ['Dr.', 'Smith', 'lleg√≥', 'a', 'las', '9:30', 'a.m.', '¬°', 'Qu√©', 'puntualidad', '!']
```

**Ventajas de word_tokenize:**
- Maneja bien las abreviaciones (Dr., a.m.)
- Separa correctamente la puntuaci√≥n
- Soporta m√∫ltiples idiomas

### 3.6 Tokenizaci√≥n de oraciones

A veces necesitamos dividir un texto en oraciones en lugar de palabras.

```python
from nltk.tokenize import sent_tokenize

texto_largo = """
El procesamiento del lenguaje natural es fascinante. 
Permite a las computadoras entender el texto humano. 
¬øNo es incre√≠ble? ¬°Definitivamente lo es!
"""

oraciones = sent_tokenize(texto_largo, language='spanish')

print("N√∫mero de oraciones:", len(oraciones))
for i, oracion in enumerate(oraciones, 1):
    print(f"{i}. {oracion.strip()}")

# Output:
# N√∫mero de oraciones: 4
# 1. El procesamiento del lenguaje natural es fascinante.
# 2. Permite a las computadoras entender el texto humano.
# 3. ¬øNo es incre√≠ble?
# 4. ¬°Definitivamente lo es!
```

### 3.7 Ejemplo completo: Tokenizaci√≥n de un texto real

```python
from nltk.tokenize import word_tokenize, sent_tokenize

# Texto de ejemplo sobre IA
texto_ia = """
La inteligencia artificial est√° transformando el mundo. Los algoritmos de 
aprendizaje autom√°tico procesan millones de datos diariamente. ¬øQu√© nos 
depara el futuro? Seg√∫n expertos, la IA revolucionar√° industrias como la 
medicina, transporte y educaci√≥n. Sin embargo, tambi√©n plantea desaf√≠os √©ticos 
importantes. ¬°El futuro es ahora!
"""

# Tokenizaci√≥n de oraciones
oraciones = sent_tokenize(texto_ia, language='spanish')
print(f"El texto tiene {len(oraciones)} oraciones\n")

# Tokenizaci√≥n de palabras para cada oraci√≥n
for i, oracion in enumerate(oraciones, 1):
    palabras = word_tokenize(oracion, language='spanish')
    print(f"Oraci√≥n {i}: {len(palabras)} tokens")
    print(f"Tokens: {palabras[:10]}...")  # Mostrar solo los primeros 10
    print()

# Tokenizaci√≥n de todas las palabras del texto
todas_las_palabras = word_tokenize(texto_ia, language='spanish')
print(f"Total de tokens en el texto: {len(todas_las_palabras)}")
```

### 3.8 Ejercicio pr√°ctico: An√°lisis b√°sico de un texto

```python
def analizar_texto(texto):
    """
    Funci√≥n que analiza un texto y devuelve estad√≠sticas b√°sicas
    """
    # Tokenizar oraciones
    oraciones = sent_tokenize(texto, language='spanish')
    
    # Tokenizar palabras
    palabras = word_tokenize(texto, language='spanish')
    
    # Filtrar solo palabras (sin puntuaci√≥n)
    palabras_solo = [p for p in palabras if p.isalnum()]
    
    # Calcular estad√≠sticas
    stats = {
        'num_caracteres': len(texto),
        'num_oraciones': len(oraciones),
        'num_tokens': len(palabras),
        'num_palabras': len(palabras_solo),
        'promedio_palabras_por_oracion': len(palabras_solo) / len(oraciones) if oraciones else 0,
        'longitud_promedio_palabra': sum(len(p) for p in palabras_solo) / len(palabras_solo) if palabras_solo else 0
    }
    
    return stats

# Probar la funci√≥n
texto_prueba = """
Python es un lenguaje de programaci√≥n vers√°til y poderoso. 
Se usa en ciencia de datos, desarrollo web, automatizaci√≥n y m√°s. 
Su sintaxis clara lo hace ideal para principiantes.
"""

resultados = analizar_texto(texto_prueba)
for clave, valor in resultados.items():
    print(f"{clave}: {valor:.2f}")
```

**Ejercicio 2:** Modifica la funci√≥n `analizar_texto` para que tambi√©n devuelva:
- La oraci√≥n m√°s larga
- La palabra m√°s larga
- Frecuencia de cada palabra



## 4. Limpieza y Preprocesamiento

El texto real es "sucio": contiene ruido, inconsistencias y elementos innecesarios. La limpieza es crucial para obtener buenos resultados.

### 4.1 Problemas comunes en texto real

```python
texto_sucio = """
   ¬°¬°OFERTA ESPECIAL!! Compra AHORA... solo por $99.99!!!
   
   Visita: https://www.ejemplo.com para m√°s info üì±üíª
   
   Email: contacto@empresa.com
   Tel√©fono: +34-555-1234
   
   #OfertaDelD√≠a #Tecnolog√≠a @usuario123
"""

print("Texto original:")
print(texto_sucio)
```

### 4.2 Convertir a min√∫sculas

Convertir todo el texto a min√∫sculas ayuda a reducir la dimensionalidad del vocabulario.

```python
texto_lower = texto_sucio.lower()
print("Texto en min√∫sculas:")
print(texto_lower)

# Ejemplo de por qu√© es importante
palabras = ["Python", "python", "PYTHON", "PyThOn"]
palabras_lower = [p.lower() for p in palabras]
print(f"\nPalabras originales: {len(set(palabras))} √∫nicas")
print(f"Palabras en min√∫sculas: {len(set(palabras_lower))} √∫nicas")
```

**Cu√°ndo NO convertir a min√∫sculas:**
- En an√°lisis de sentimientos (may√∫sculas pueden indicar emoci√≥n)
- En NER (Named Entity Recognition) - los nombres propios son importantes
- Cuando las may√∫sculas tienen significado (USA vs usa)

### 4.3 Eliminaci√≥n de URLs

```python
import re

texto_con_urls = """
Visita mi blog en https://www.miblog.com/articulo o 
mi perfil en http://twitter.com/usuario. 
Tambi√©n tengo un canal: www.youtube.com/canal
"""

# Patr√≥n para URLs
patron_url = r'https?://\S+|www\.\S+'
texto_sin_urls = re.sub(patron_url, '', texto_con_urls)

print("Texto sin URLs:")
print(texto_sin_urls)
```

### 4.4 Eliminaci√≥n de emails

```python
texto_con_emails = """
Contacta con soporte@empresa.com o ventas@empresa.co.uk 
para m√°s informaci√≥n. Tambi√©n puedes escribir a juan.perez@mail.com
"""

# Patr√≥n para emails
patron_email = r'\S+@\S+'
texto_sin_emails = re.sub(patron_email, '', texto_con_emails)

print("Texto sin emails:")
print(texto_sin_emails)
```

### 4.5 Eliminaci√≥n de menciones y hashtags

```python
texto_social = """
@usuario1 mira esto! #PLN #MachineLearning 
cc: @usuario2 @usuario3 
#InteligenciaArtificial #Python
"""

# Eliminar menciones (@usuario)
texto_sin_menciones = re.sub(r'@\w+', '', texto_social)
print("Sin menciones:", texto_sin_menciones)

# Eliminar hashtags (#tag)
texto_sin_hashtags = re.sub(r'#\w+', '', texto_social)
print("Sin hashtags:", texto_sin_hashtags)

# Eliminar ambos
texto_limpio = re.sub(r'[@#]\w+', '', texto_social)
print("Sin menciones ni hashtags:", texto_limpio)
```

### 4.6 Eliminaci√≥n de n√∫meros

```python
texto_con_numeros = """
En 2024, el PIB creci√≥ un 3.5%. La empresa tiene 150 empleados 
y factur√≥ $2,500,000. El tel√©fono es 555-1234.
"""

# Eliminar todos los n√∫meros
texto_sin_numeros = re.sub(r'\d+', '', texto_con_numeros)
print("Sin n√∫meros:")
print(texto_sin_numeros)

# Eliminar n√∫meros pero mantener decimales y separadores
texto_sin_numeros2 = re.sub(r'[0-9]+', '[NUM]', texto_con_numeros)
print("\nN√∫meros reemplazados por [NUM]:")
print(texto_sin_numeros2)
```

### 4.7 Eliminaci√≥n de puntuaci√≥n

```python
import string

texto_con_puntuacion = "¬°Hola! ¬øC√≥mo est√°s? Espero que bien... (muy bien)"

# M√©todo 1: Usando string.punctuation
print("Puntuaci√≥n est√°ndar:", string.punctuation)

texto_sin_puntuacion = texto_con_puntuacion.translate(
    str.maketrans('', '', string.punctuation)
)
print("Sin puntuaci√≥n:", texto_sin_puntuacion)

# M√©todo 2: Usando expresiones regulares (m√°s flexible)
texto_sin_puntuacion2 = re.sub(r'[^\w\s]', '', texto_con_puntuacion)
print("Sin puntuaci√≥n (regex):", texto_sin_puntuacion2)

# M√©todo 3: Mantener puntuaci√≥n importante (., !, ?)
puntuacion_a_eliminar = string.punctuation.replace('.', '').replace('!', '').replace('?', '')
texto_parcial = texto_con_puntuacion.translate(
    str.maketrans('', '', puntuacion_a_eliminar)
)
print("Manteniendo ., !, ?:", texto_parcial)
```

### 4.8 Eliminaci√≥n de espacios extras

```python
texto_espacios = "Este    texto   tiene     muchos espacios      extras"

# Eliminar espacios extras
texto_limpio = ' '.join(texto_espacios.split())
print("Texto limpio:", texto_limpio)

# Eliminar espacios al inicio y final
texto_con_espacios_extremos = "   texto con espacios   "
texto_sin_espacios = texto_con_espacios_extremos.strip()
print(f"Original: '{texto_con_espacios_extremos}'")
print(f"Limpio: '{texto_sin_espacios}'")
```

### 4.9 Eliminaci√≥n de caracteres especiales y emojis

```python
texto_emojis = "Me encanta Python üêçüíª! Es incre√≠ble üòçüî•"

# Eliminar emojis y caracteres no ASCII
texto_sin_emojis = texto_emojis.encode('ascii', 'ignore').decode('ascii')
print("Sin emojis:", texto_sin_emojis)

# Mantener solo letras, n√∫meros y espacios
texto_solo_alfanumerico = re.sub(r'[^a-zA-Z0-9\s]', '', texto_emojis)
print("Solo alfanum√©rico:", texto_solo_alfanumerico)
```

### 4.10 Funci√≥n de limpieza completa

```python
def limpiar_texto(texto, 
                  minusculas=True,
                  eliminar_urls=True,
                  eliminar_emails=True,
                  eliminar_menciones=True,
                  eliminar_hashtags=False,  # A veces los hashtags son √∫tiles
                  eliminar_numeros=False,
                  eliminar_puntuacion=True,
                  eliminar_espacios_extras=True):
    """
    Funci√≥n completa para limpiar texto
    
    Par√°metros:
    --
    texto : str
        Texto a limpiar
    minusculas : bool
        Convertir a min√∫sculas
    eliminar_urls : bool
        Eliminar URLs
    eliminar_emails : bool
        Eliminar direcciones de email
    eliminar_menciones : bool
        Eliminar menciones (@usuario)
    eliminar_hashtags : bool
        Eliminar hashtags (#tag)
    eliminar_numeros : bool
        Eliminar n√∫meros
    eliminar_puntuacion : bool
        Eliminar signos de puntuaci√≥n
    eliminar_espacios_extras : bool
        Eliminar espacios m√∫ltiples
        
    Retorna:
    --
    str
        Texto limpio
    """
    
    # Convertir a min√∫sculas
    if minusculas:
        texto = texto.lower()
    
    # Eliminar URLs
    if eliminar_urls:
        texto = re.sub(r'https?://\S+|www\.\S+', '', texto)
    
    # Eliminar emails
    if eliminar_emails:
        texto = re.sub(r'\S+@\S+', '', texto)
    
    # Eliminar menciones
    if eliminar_menciones:
        texto = re.sub(r'@\w+', '', texto)
    
    # Eliminar hashtags
    if eliminar_hashtags:
        texto = re.sub(r'#\w+', '', texto)
    
    # Eliminar n√∫meros
    if eliminar_numeros:
        texto = re.sub(r'\d+', '', texto)
    
    # Eliminar puntuaci√≥n
    if eliminar_puntuacion:
        texto = texto.translate(str.maketrans('', '', string.punctuation))
    
    # Eliminar espacios extras
    if eliminar_espacios_extras:
        texto = ' '.join(texto.split())
    
    return texto


# Ejemplo de uso
texto_original = """
¬°S√öPER OFERTA! üéâ Visita https://www.tienda.com 
Contacto: ventas@tienda.com o @TiendaOficial
#Descuento #Ahorra 50% ¬°Solo HOY!
Llama al 555-1234
"""

print("=" * 50)
print("TEXTO ORIGINAL:")
print("=" * 50)
print(texto_original)

print("\n" + "=" * 50)
print("TEXTO LIMPIO:")
print("=" * 50)
texto_limpio = limpiar_texto(texto_original)
print(texto_limpio)

# Ejemplo con diferentes configuraciones
print("\n" + "=" * 50)
print("TEXTO LIMPIO (conservando hashtags):")
print("=" * 50)
texto_limpio2 = limpiar_texto(texto_original, eliminar_hashtags=False)
print(texto_limpio2)
```

### 4.11 Eliminaci√≥n de stopwords

Las **stopwords** son palabras muy comunes que generalmente no aportan mucho significado (art√≠culos, preposiciones, etc.).

```python
from nltk.corpus import stopwords

# Cargar stopwords en espa√±ol
stop_words_es = set(stopwords.words('spanish'))

print("Primeras 20 stopwords en espa√±ol:")
print(list(stop_words_es)[:20])

# Ejemplo de uso
texto = "El procesamiento del lenguaje natural es una rama de la inteligencia artificial"

# Tokenizar
tokens = word_tokenize(texto, language='spanish')
print(f"\nTokens originales ({len(tokens)}):", tokens)

# Filtrar stopwords
tokens_filtrados = [palabra for palabra in tokens if palabra.lower() not in stop_words_es]
print(f"\nTokens sin stopwords ({len(tokens_filtrados)}):", tokens_filtrados)

# Ver qu√© palabras se eliminaron
palabras_eliminadas = [p for p in tokens if p.lower() in stop_words_es]
print(f"\nPalabras eliminadas: {palabras_eliminadas}")
```

**Cu√°ndo NO eliminar stopwords:**
- An√°lisis de sentimientos (negaciones son importantes: "no me gusta")
- Traducci√≥n autom√°tica
- Generaci√≥n de texto
- Preguntas y respuestas

### 4.12 Personalizar lista de stopwords

```python
# Agregar stopwords personalizadas
stop_words_custom = stop_words_es.copy()
stop_words_custom.update(['se√±or', 'se√±ora', 'favor', 'etc'])

# O crear una lista completamente personalizada
mi_lista_stopwords = {'el', 'la', 'de', 'y', 'a', 'en'}

texto = "El se√±or Garc√≠a vive en Madrid y trabaja en Barcelona"
tokens = word_tokenize(texto.lower(), language='spanish')

tokens_sin_custom = [p for p in tokens if p not in stop_words_custom]
print("Con stopwords personalizadas:", tokens_sin_custom)
```

### 4.13 Ejercicio: Pipeline completo de limpieza

```python
def pipeline_limpieza_completo(texto):
    """
    Pipeline completo de limpieza de texto
    """
    print("PASO 1: Texto original")
    print(f"Longitud: {len(texto)} caracteres")
    print(texto[:200] + "...")
    
    print("\nPASO 2: Convertir a min√∫sculas")
    texto = texto.lower()
    print(texto[:200] + "...")
    
    print("\nPASO 3: Eliminar URLs, emails y menciones")
    texto = re.sub(r'https?://\S+|www\.\S+', '', texto)
    texto = re.sub(r'\S+@\S+', '', texto)
    texto = re.sub(r'@\w+', '', texto)
    print(texto[:200] + "...")
    
    print("\nPASO 4: Eliminar puntuaci√≥n")
    texto = texto.translate(str.maketrans('', '', string.punctuation))
    print(texto[:200] + "...")
    
    print("\nPASO 5: Tokenizar")
    tokens = word_tokenize(texto, language='spanish')
    print(f"N√∫mero de tokens: {len(tokens)}")
    print(f"Primeros 20 tokens: {tokens[:20]}")
    
    print("\nPASO 6: Eliminar stopwords")
    stop_words = set(stopwords.words('spanish'))
    tokens_filtrados = [t for t in tokens if t not in stop_words]
    print(f"Tokens despu√©s de eliminar stopwords: {len(tokens_filtrados)}")
    print(f"Primeros 20 tokens: {tokens_filtrados[:20]}")
    
    print("\nPASO 7: Filtrar palabras muy cortas")
    tokens_finales = [t for t in tokens_filtrados if len(t) > 2]
    print(f"Tokens finales: {len(tokens_finales)}")
    print(f"Primeros 20 tokens: {tokens_finales[:20]}")
    
    return tokens_finales

# Probar con un texto real
texto_ejemplo = """
La inteligencia artificial (IA) est√° revolucionando m√∫ltiples sectores. 
Seg√∫n expertos, para 2025 el 80% de las empresas habr√°n adoptado alguna 
forma de IA. ¬øQu√© significa esto para el futuro del trabajo?

Visita https://www.ejemplo.com para m√°s informaci√≥n o escribe a info@ia.com

#InteligenciaArtificial #Tecnolog√≠a @ExpertoIA
"""

tokens_limpios = pipeline_limpieza_completo(texto_ejemplo)
```

**Ejercicio 3:** Crea una versi√≥n de `pipeline_limpieza_completo` que:
- Reciba par√°metros para activar/desactivar cada paso
- Devuelva tanto los tokens como estad√≠sticas del proceso
- Guarde un log de las transformaciones



## 5. Normalizaci√≥n de Texto

La normalizaci√≥n busca reducir las palabras a su forma can√≥nica o base. Hay dos t√©cnicas principales: **stemming** y **lemmatization**.

### 5.1 ¬øQu√© es el Stemming?

El **stemming** es el proceso de reducir palabras a su ra√≠z o "stem" mediante reglas heur√≠sticas (eliminando sufijos y prefijos).

**Ventajas:**
- Muy r√°pido
- Simple de implementar
- No requiere diccionario

**Desventajas:**
- Puede generar stems que no son palabras reales
- Menos preciso que lemmatization
- Puede sobre-reducir o sub-reducir

```python
from nltk.stem import SnowballStemmer

# Crear stemmer para espa√±ol
stemmer_es = SnowballStemmer('spanish')

# Ejemplos de stemming
palabras = [
    'programar', 'programaci√≥n', 'programador', 'programadores', 'programa',
    'correr', 'corriendo', 'corri√≥', 'correr√°', 'corredor',
    'estudiar', 'estudiante', 'estudiantes', 'estudi√≥', 'estudioso'
]

print("Palabra Original ‚Üí Stem")
print("-" * 40)
for palabra in palabras:
    stem = stemmer_es.stem(palabra)
    print(f"{palabra:20} ‚Üí {stem}")
```

### 5.2 Problemas del Stemming

```python
# Caso 1: Sobre-reducci√≥n (palabras diferentes con el mismo stem)
palabras_diferentes = ['organizar', 'organismo', '√≥rgano']
print("\nSOBRE-REDUCCI√ìN:")
for palabra in palabras_diferentes:
    print(f"{palabra} ‚Üí {stemmer_es.stem(palabra)}")

# Caso 2: Sub-reducci√≥n (misma palabra con stems diferentes)
palabras_iguales = ['mejor', 'mejorar', 'mejora', 'mejoramiento']
print("\nSUB-REDUCCI√ìN:")
for palabra in palabras_iguales:
    print(f"{palabra} ‚Üí {stemmer_es.stem(palabra)}")

# Caso 3: Stems que no son palabras reales
palabras_normales = ['an√°lisis', 'analizar', 'anal√≠tico']
print("\nSTEMS NO REALES:")
for palabra in palabras_normales:
    stem = stemmer_es.stem(palabra)
    print(f"{palabra} ‚Üí {stem}")
```

### 5.3 ¬øQu√© es la Lematizaci√≥n?

La **lematizaci√≥n** reduce las palabras a su forma base (lema) usando un diccionario y an√°lisis morfol√≥gico.

**Ventajas:**
- Produce palabras reales
- M√°s preciso
- Considera el contexto

**Desventajas:**
- M√°s lento que stemming
- Requiere diccionario
- M√°s complejo

```python
# Para espa√±ol, usamos spaCy que tiene mejor soporte
# !python -m spacy download es_core_news_sm

import spacy

# Cargar modelo en espa√±ol
nlp = spacy.load('es_core_news_sm')

# Ejemplos de lematizaci√≥n
palabras = [
    'programar', 'programaci√≥n', 'programador', 'programadores',
    'corriendo', 'corri√≥', 'correr√°', 'corredor',
    'mejor', 'mejores', 'mejorar', 'mejoramiento'
]

print("Palabra Original ‚Üí Lema")
print("-" * 40)
for palabra in palabras:
    doc = nlp(palabra)
    for token in doc:
        print(f"{token.text:20} ‚Üí {token.lemma_}")
```

### 5.4 Comparaci√≥n Stemming vs Lematizaci√≥n

```python
# Crear funci√≥n de comparaci√≥n
def comparar_normalizacion(palabras):
    """
    Compara stemming y lematizaci√≥n en una lista de palabras
    """
    print(f"{'Palabra':<20} {'Stem':<20} {'Lema':<20}")
    print("-" * 60)
    
    for palabra in palabras:
        # Stemming
        stem = stemmer_es.stem(palabra)
        
        # Lematizaci√≥n
        doc = nlp(palabra)
        lema = doc[0].lemma_ if doc else palabra
        
        print(f"{palabra:<20} {stem:<20} {lema:<20}")

# Probar con diferentes palabras
palabras_prueba = [
    'corriendo', 'mejor', 'organizaci√≥n', 'an√°lisis',
    'estudiar', 'programadores', 'desarrollando',
    'fui', 'fuiste', 'fue', 'fuimos', 'fueron'
]

comparar_normalizacion(palabras_prueba)
```

### 5.5 Aplicaci√≥n en texto real

```python
texto = """
Los cient√≠ficos est√°n desarrollando nuevos algoritmos de aprendizaje autom√°tico.
Estos algoritmos pueden analizar grandes cantidades de datos y encontrar patrones
complejos. Las aplicaciones son infinitas: desde diagn√≥sticos m√©dicos hasta
predicciones financieras. Los investigadores creen que estos avances revolucionar√°n
m√∫ltiples industrias en los pr√≥ximos a√±os.
"""

print("TEXTO ORIGINAL:")
print(texto)

# Tokenizar
tokens = word_tokenize(texto.lower(), language='spanish')
print(f"\nN√∫mero de tokens: {len(tokens)}")

# Aplicar stemming
tokens_stem = [stemmer_es.stem(token) for token in tokens if token.isalnum()]
print(f"\nTokens con stemming ({len(set(tokens_stem))} √∫nicos):")
print(tokens_stem)

# Aplicar lematizaci√≥n
doc = nlp(texto.lower())
tokens_lema = [token.lemma_ for token in doc if token.is_alpha]
print(f"\nTokens con lematizaci√≥n ({len(set(tokens_lema))} √∫nicos):")
print(tokens_lema)

# Comparar reducci√≥n de vocabulario
print(f"\nVocabulario original: {len(set([t for t in tokens if t.isalnum()]))} palabras √∫nicas")
print(f"Vocabulario con stemming: {len(set(tokens_stem))} palabras √∫nicas")
print(f"Vocabulario con lematizaci√≥n: {len(set(tokens_lema))} palabras √∫nicas")
```

### 5.6 ¬øCu√°ndo usar cada uno?

**Usar STEMMING cuando:**
- La velocidad es cr√≠tica
- Trabajas con grandes vol√∫menes de texto
- La precisi√≥n exacta no es crucial
- Trabajas en sistemas de b√∫squeda/recuperaci√≥n de informaci√≥n

**Usar LEMATIZACI√ìN cuando:**
- Necesitas preservar el significado exacto
- Trabajas con an√°lisis de sentimientos
- La interpretabilidad es importante
- Tienes suficientes recursos computacionales

```python
# Ejemplo: Sistema de b√∫squeda vs An√°lisis de sentimientos

# B√öSQUEDA (stemming es suficiente)
consulta_busqueda = "programando en Python"
terminos_busqueda = [stemmer_es.stem(t) for t in consulta_busqueda.lower().split()]
print("T√©rminos de b√∫squeda (stemming):", terminos_busqueda)
# Output: ['program', 'en', 'python']
# Encontrar√°: "programar", "programador", "programa", etc.

# AN√ÅLISIS DE SENTIMIENTOS (lematizaci√≥n es mejor)
comentario = "Los mejores programadores son autodidactas"
doc = nlp(comentario.lower())
tokens_analisis = [token.lemma_ for token in doc if token.is_alpha]
print("An√°lisis de sentimientos (lematizaci√≥n):", tokens_analisis)
# Output: ['bueno', 'programador', 'ser', 'autodidacta']
# Preserva "bueno" en lugar de reducirlo a "buen"
```

### 5.7 Ejercicio: An√°lisis de frecuencias con normalizaci√≥n

```python
from collections import Counter

def analizar_frecuencias(texto, metodo='lema'):
    """
    Analiza las frecuencias de palabras en un texto
    
    Par√°metros:
    --
    texto : str
        Texto a analizar
    metodo : str
        'stem' para stemming, 'lema' para lematizaci√≥n, 'original' para tokens sin normalizar
    
    Retorna:
    --
    Counter
        Contador de frecuencias
    """
    # Limpiar texto
    texto_limpio = limpiar_texto(texto)
    
    # Tokenizar
    tokens = word_tokenize(texto_limpio, language='spanish')
    
    # Filtrar stopwords
    stop_words = set(stopwords.words('spanish'))
    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]
    
    # Normalizar seg√∫n m√©todo
    if metodo == 'stem':
        tokens_normalizados = [stemmer_es.stem(t) for t in tokens]
    elif metodo == 'lema':
        doc = nlp(' '.join(tokens))
        tokens_normalizados = [token.lemma_ for token in doc if token.is_alpha]
    else:
        tokens_normalizados = tokens
    
    # Contar frecuencias
    frecuencias = Counter(tokens_normalizados)
    
    return frecuencias

# Texto de ejemplo
texto_largo = """
La inteligencia artificial est√° revolucionando la forma en que vivimos y trabajamos.
Los desarrolladores est√°n creando nuevas aplicaciones cada d√≠a. Estas aplicaciones
utilizan algoritmos avanzados de aprendizaje autom√°tico. El aprendizaje profundo,
una rama del aprendizaje autom√°tico, ha demostrado resultados impresionantes en
m√∫ltiples √°reas. Los investigadores contin√∫an desarrollando mejores algoritmos y
las empresas contin√∫an invirtiendo en esta tecnolog√≠a revolucionaria.
"""

# Comparar m√©todos
for metodo in ['original', 'stem', 'lema']:
    print(f"\n{'='*50}")
    print(f"M√âTODO: {metodo.upper()}")
    print(f"{'='*50}")
    
    freq = analizar_frecuencias(texto_largo, metodo)
    print(f"Vocabulario √∫nico: {len(freq)} palabras")
    print("\nTop 10 palabras m√°s frecuentes:")
    for palabra, count in freq.most_common(10):
        print(f"  {palabra:20} ‚Üí {count}")
```

**Ejercicio 4:** Modifica `analizar_frecuencias` para que:
- Genere un gr√°fico de barras con las 20 palabras m√°s frecuentes
- Calcule el ratio de reducci√≥n de vocabulario para cada m√©todo
- Identifique bigramas frecuentes



## 6. Visualizaci√≥n de Datos de Texto

La visualizaci√≥n nos ayuda a entender r√°pidamente patrones en los datos de texto.

### 6.1 Nubes de palabras (Word Clouds)

Las nubes de palabras muestran visualmente las palabras m√°s frecuentes, donde el tama√±o indica la frecuencia.

```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Texto de ejemplo
texto_visualizar = """
Python es un lenguaje de programaci√≥n interpretado de alto nivel. Python es 
conocido por su sintaxis clara y legible. Muchos desarrolladores eligen Python
para ciencia de datos, desarrollo web y automatizaci√≥n. La comunidad de Python
es grande y activa. Python tiene excelentes bibliotecas para machine learning,
an√°lisis de datos y visualizaci√≥n. Python es vers√°til y poderoso.
"""

# Crear nube de palabras b√°sica
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_visualizar)

# Visualizar
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Nube de Palabras - B√°sica')
plt.tight_layout(pad=0)
plt.show()
```

### 6.2 Nube de palabras personalizada

```python
# Limpiar y preparar el texto
texto_limpio = limpiar_texto(texto_visualizar)
tokens = word_tokenize(texto_limpio, language='spanish')

# Eliminar stopwords
stop_words = set(stopwords.words('spanish'))
tokens_filtrados = [t for t in tokens if t not in stop_words]

# Unir tokens de nuevo
texto_para_wordcloud = ' '.join(tokens_filtrados)

# Crear nube de palabras personalizada
wordcloud_custom = WordCloud(
    width=1200,
    height=600,
    background_color='white',
    colormap='viridis',  # Esquema de colores
    max_words=50,        # N√∫mero m√°ximo de palabras
    relative_scaling=0.5,
    min_font_size=10
).generate(texto_para_wordcloud)

# Visualizar
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud_custom, interpolation='bilinear')
plt.axis('off')
plt.title('Nube de Palabras - Personalizada (sin stopwords)', fontsize=16)
plt.tight_layout(pad=0)
plt.show()
```

### 6.3 Nube de palabras con frecuencias espec√≠ficas

```python
from collections import Counter

# Calcular frecuencias
frecuencias = Counter(tokens_filtrados)

# Crear nube desde frecuencias
wordcloud_freq = WordCloud(
    width=1200,
    height=600,
    background_color='black',
    colormap='plasma'
).generate_from_frequencies(frecuencias)

# Visualizar
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud_freq, interpolation='bilinear')
plt.axis('off')
plt.title('Nube de Palabras - Desde Frecuencias', fontsize=16, color='white')
plt.tight_layout(pad=0)
plt.show()

# Mostrar top 10 palabras
print("\nTop 10 palabras m√°s frecuentes:")
for palabra, freq in frecuencias.most_common(10):
    print(f"{palabra:15} ‚Üí {freq}")
```

### 6.4 Gr√°fico de barras de frecuencias

```python
# Preparar datos
top_n = 15
palabras_top = frecuencias.most_common(top_n)
palabras = [p[0] for p in palabras_top]
counts = [p[1] for p in palabras_top]

# Crear gr√°fico
plt.figure(figsize=(12, 6))
plt.barh(palabras, counts, color='skyblue', edgecolor='navy')
plt.xlabel('Frecuencia', fontsize=12)
plt.ylabel('Palabra', fontsize=12)
plt.title(f'Top {top_n} Palabras M√°s Frecuentes', fontsize=14)
plt.gca().invert_yaxis()  # Invertir para que la m√°s frecuente est√© arriba
plt.tight_layout()
plt.show()
```

### 6.5 Distribuci√≥n de longitudes de palabras

```python
# Calcular longitudes
longitudes = [len(token) for token in tokens_filtrados]

# Crear histograma
plt.figure(figsize=(10, 6))
plt.hist(longitudes, bins=range(1, max(longitudes)+2), color='coral', edgecolor='black', alpha=0.7)
plt.xlabel('Longitud de la palabra (caracteres)', fontsize=12)
plt.ylabel('Frecuencia', fontsize=12)
plt.title('Distribuci√≥n de Longitudes de Palabras', fontsize=14)
plt.xticks(range(1, max(longitudes)+1))
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# Estad√≠sticas
print(f"Longitud promedio: {sum(longitudes)/len(longitudes):.2f} caracteres")
print(f"Longitud m√≠nima: {min(longitudes)} caracteres")
print(f"Longitud m√°xima: {max(longitudes)} caracteres")
```

### 6.6 Visualizaci√≥n de n-gramas

```python
from nltk import ngrams

def obtener_ngramas(tokens, n=2):
    """
    Obtiene n-gramas de una lista de tokens
    """
    ngramas = list(ngrams(tokens, n))
    ngramas_texto = [' '.join(ngrama) for ngrama in ngramas]
    return Counter(ngramas_texto)

# Obtener bigramas
bigramas = obtener_ngramas(tokens_filtrados, 2)

# Visualizar top bigramas
top_bigramas = bigramas.most_common(10)
bigramas_labels = [b[0] for b in top_bigramas]
bigramas_counts = [b[1] for b in top_bigramas]

plt.figure(figsize=(12, 6))
plt.barh(bigramas_labels, bigramas_counts, color='lightgreen', edgecolor='darkgreen')
plt.xlabel('Frecuencia', fontsize=12)
plt.ylabel('Bigrama', fontsize=12)
plt.title('Top 10 Bigramas M√°s Frecuentes', fontsize=14)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Obtener trigramas
print("\nTop 5 Trigramas:")
trigramas = obtener_ngramas(tokens_filtrados, 3)
for trigrama, freq in trigramas.most_common(5):
    print(f"{trigrama:40} ‚Üí {freq}")
```

### 6.7 Ejercicio: Dashboard de an√°lisis de texto

```python
def crear_dashboard_texto(texto):
    """
    Crea un dashboard completo con m√∫ltiples visualizaciones
    """
    # Limpiar y preparar
    texto_limpio = limpiar_texto(texto)
    tokens = word_tokenize(texto_limpio, language='spanish')
    stop_words = set(stopwords.words('spanish'))
    tokens_filtrados = [t for t in tokens if t not in stop_words and len(t) > 2]
    
    # Calcular m√©tricas
    frecuencias = Counter(tokens_filtrados)
    bigramas = obtener_ngramas(tokens_filtrados, 2)
    longitudes = [len(t) for t in tokens_filtrados]
    
    # Crear figura con subplots
    fig = plt.figure(figsize=(16, 10))
    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
    
    # 1. Nube de palabras
    ax1 = fig.add_subplot(gs[0, :])
    wordcloud = WordCloud(
        width=1200, height=300,
        background_color='white',
        colormap='viridis'
    ).generate(' '.join(tokens_filtrados))
    ax1.imshow(wordcloud, interpolation='bilinear')
    ax1.axis('off')
    ax1.set_title('Nube de Palabras', fontsize=14, fontweight='bold')
    
    # 2. Top palabras
    ax2 = fig.add_subplot(gs[1, 0])
    top_palabras = frecuencias.most_common(10)
    palabras = [p[0] for p in top_palabras]
    counts = [p[1] for p in top_palabras]
    ax2.barh(palabras, counts, color='skyblue', edgecolor='navy')
    ax2.set_xlabel('Frecuencia')
    ax2.set_title('Top 10 Palabras', fontsize=12, fontweight='bold')
    ax2.invert_yaxis()
    
    # 3. Top bigramas
    ax3 = fig.add_subplot(gs[1, 1])
    top_bi = bigramas.most_common(10)
    bi_labels = [b[0] for b in top_bi]
    bi_counts = [b[1] for b in top_bi]
    ax3.barh(bi_labels, bi_counts, color='lightgreen', edgecolor='darkgreen')
    ax3.set_xlabel('Frecuencia')
    ax3.set_title('Top 10 Bigramas', fontsize=12, fontweight='bold')
    ax3.invert_yaxis()
    
    # 4. Distribuci√≥n de longitudes
    ax4 = fig.add_subplot(gs[2, 0])
    ax4.hist(longitudes, bins=range(1, max(longitudes)+2), 
             color='coral', edgecolor='black', alpha=0.7)
    ax4.set_xlabel('Longitud (caracteres)')
    ax4.set_ylabel('Frecuencia')
    ax4.set_title('Distribuci√≥n de Longitudes', fontsize=12, fontweight='bold')
    ax4.grid(axis='y', alpha=0.3)
    
    # 5. Estad√≠sticas textuales
    ax5 = fig.add_subplot(gs[2, 1])
    ax5.axis('off')
    
    stats_text = f"""
    ESTAD√çSTICAS DEL TEXTO
    
    Tokens totales: {len(tokens)}
    Tokens √∫nicos: {len(set(tokens))}
    Tokens filtrados: {len(tokens_filtrados)}
    Vocabulario √∫nico: {len(frecuencias)}
    
    Longitud promedio: {sum(longitudes)/len(longitudes):.2f} caracteres
    Palabra m√°s larga: {max(tokens_filtrados, key=len)} ({len(max(tokens_filtrados, key=len))} chars)
    Palabra m√°s corta: {min(tokens_filtrados, key=len)} ({len(min(tokens_filtrados, key=len))} chars)
    
    Palabra m√°s frecuente: {frecuencias.most_common(1)[0][0]} ({frecuencias.most_common(1)[0][1]} veces)
    """
    
    ax5.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',
             verticalalignment='center')
    
    plt.suptitle('Dashboard de An√°lisis de Texto', fontsize=16, fontweight='bold', y=0.995)
    plt.show()

# Probar con un texto extenso
texto_dashboard = """
La inteligencia artificial y el aprendizaje autom√°tico est√°n transformando radicalmente 
nuestra sociedad. Los algoritmos de aprendizaje profundo pueden procesar cantidades 
masivas de datos y encontrar patrones que ser√≠an imposibles de detectar para los humanos.

Las aplicaciones son infinitas: desde veh√≠culos aut√≥nomos hasta diagn√≥sticos m√©dicos 
avanzados, pasando por sistemas de recomendaci√≥n y asistentes virtuales. El procesamiento 
del lenguaje natural permite que las m√°quinas entiendan y generen texto humano con 
una precisi√≥n cada vez mayor.

Los desaf√≠os √©ticos son importantes. Debemos asegurarnos de que estos sistemas sean 
justos, transparentes y beneficiosos para toda la humanidad. El futuro de la IA depende 
de nuestra capacidad para desarrollar tecnolog√≠a responsable y centrada en las personas.

Python se ha convertido en el lenguaje preferido para desarrollar soluciones de IA y 
aprendizaje autom√°tico, gracias a su sintaxis clara y sus poderosas bibliotecas como 
TensorFlow, PyTorch y scikit-learn. La comunidad de desarrolladores contin√∫a creando 
herramientas innovadoras que democratizan el acceso a estas tecnolog√≠as.
"""

crear_dashboard_texto(texto_dashboard)
```

**Ejercicio 5:** Extiende `crear_dashboard_texto` para incluir:
- Un gr√°fico de l√≠nea mostrando la frecuencia acumulada de palabras
- Comparaci√≥n de frecuencias antes y despu√©s de normalizaci√≥n
- Identificaci√≥n de palabras clave usando TF-IDF



## 7. Part-of-Speech (POS) Tagging

El etiquetado gramatical (POS tagging) asigna a cada palabra su categor√≠a gramatical (sustantivo, verbo, adjetivo, etc.).

### 7.1 ¬øQu√© es POS Tagging?

POS Tagging es el proceso de etiquetar cada palabra de un texto con su parte de la oraci√≥n correspondiente.

**Categor√≠as principales:**
- **Sustantivos (N)**: Nombran personas, lugares, cosas
- **Verbos (V)**: Indican acciones o estados
- **Adjetivos (A)**: Describen o modifican sustantivos
- **Adverbios (R)**: Modifican verbos, adjetivos u otros adverbios
- **Pronombres (P)**: Sustituyen a sustantivos
- **Preposiciones (ADP)**: Relacionan palabras
- **Conjunciones (C)**: Conectan palabras o frases
- **Determinantes (D)**: Preceden a sustantivos

### 7.2 POS Tagging con NLTK (ingl√©s)

```python
from nltk import pos_tag
from nltk.tokenize import word_tokenize

# Texto en ingl√©s
texto_en = "The quick brown fox jumps over the lazy dog"

# Tokenizar
tokens = word_tokenize(texto_en)

# Etiquetar
tags = pos_tag(tokens)

print("Token        ‚Üí POS Tag")
print("-" * 30)
for token, tag in tags:
    print(f"{token:12} ‚Üí {tag}")

"""
Explicaci√≥n de tags comunes en ingl√©s:
DT  - Determiner (el, la, un, una)
JJ  - Adjective (adjetivo)
NN  - Noun, singular (sustantivo singular)
NNS - Noun, plural (sustantivo plural)
VB  - Verb, base form (verbo en infinitivo)
VBZ - Verb, 3rd person singular present (verbo 3ra persona)
IN  - Preposition (preposici√≥n)
"""
```

### 7.3 POS Tagging con spaCy (espa√±ol)

spaCy proporciona mejor soporte para espa√±ol y es m√°s preciso.

```python
import spacy

# Cargar modelo de espa√±ol
nlp = spacy.load('es_core_news_sm')

# Texto de ejemplo
texto_es = "Los cient√≠ficos desarrollan nuevos algoritmos de aprendizaje autom√°tico"

# Procesar
doc = nlp(texto_es)

# Mostrar etiquetas
print("Token           ‚Üí POS    ‚Üí Tag Detallado ‚Üí Explicaci√≥n")
print("-" * 80)
for token in doc:
    print(f"{token.text:15} ‚Üí {token.pos_:7} ‚Üí {token.tag_:15} ‚Üí {spacy.explain(token.tag_)}")

"""
Principales tags de spaCy en espa√±ol:
NOUN  - Sustantivo
VERB  - Verbo
ADJ   - Adjetivo
ADV   - Adverbio
DET   - Determinante
ADP   - Adposici√≥n (preposici√≥n)
PRON  - Pronombre
CONJ  - Conjunci√≥n
NUM   - N√∫mero
"""
```

### 7.4 Extracci√≥n de sustantivos

```python
def extraer_sustantivos(texto):
    """
    Extrae todos los sustantivos de un texto
    """
    doc = nlp(texto)
    sustantivos = [token.text for token in doc if token.pos_ == 'NOUN']
    return sustantivos

texto = """
El procesamiento del lenguaje natural es una rama fascinante de la inteligencia 
artificial. Los investigadores trabajan constantemente en mejorar los algoritmos 
y modelos que permiten a las computadoras entender el lenguaje humano.
"""

sustantivos = extraer_sustantivos(texto)
print("Sustantivos encontrados:")
print(sustantivos)
```

### 7.5 Extracci√≥n de verbos

```python
def extraer_verbos(texto):
    """
    Extrae todos los verbos de un texto
    """
    doc = nlp(texto)
    verbos = [token.lemma_ for token in doc if token.pos_ == 'VERB']
    return verbos

verbos = extraer_verbos(texto)
print("\nVerbos encontrados (forma base):")
print(verbos)
```

### 7.6 An√°lisis morfol√≥gico completo

```python
def analisis_morfologico(texto):
    """
    Realiza un an√°lisis morfol√≥gico completo
    """
    doc = nlp(texto)
    
    # Contar por categor√≠a
    pos_counts = {}
    for token in doc:
        if token.pos_ not in pos_counts:
            pos_counts[token.pos_] = []
        pos_counts[token.pos_].append(token.text)
    
    # Mostrar resultados
    print("AN√ÅLISIS MORFOL√ìGICO")
    print("=" * 60)
    
    for pos, palabras in sorted(pos_counts.items()):
        print(f"\n{pos} ({len(palabras)} palabras):")
        print(f"  {', '.join(set(palabras))}")
    
    # Mostrar detalles palabra por palabra
    print("\n" + "=" * 60)
    print("DETALLE POR PALABRA")
    print("=" * 60)
    print(f"{'Token':<20} {'Lema':<15} {'POS':<10} {'Morfolog√≠a'}")
    print("-" * 70)
    
    for token in doc:
        if token.is_alpha:  # Solo palabras, no puntuaci√≥n
            print(f"{token.text:<20} {token.lemma_:<15} {token.pos_:<10} {token.morph}")

# Probar
texto_complejo = """
Los desarrolladores est√°n creando aplicaciones innovadoras usando inteligencia 
artificial. Estas aplicaciones procesan datos complejos y generan resultados 
sorprendentes. El futuro de la tecnolog√≠a ser√° revolucionario.
"""

analisis_morfologico(texto_complejo)
```

### 7.7 Identificaci√≥n de patrones gramaticales

```python
def encontrar_patron(texto, patron):
    """
    Encuentra patrones gramaticales espec√≠ficos
    
    Par√°metros:
    --
    texto : str
        Texto a analizar
    patron : list
        Lista de tags POS a buscar en secuencia
        Ejemplo: ['DET', 'NOUN'] para determinante + sustantivo
    """
    doc = nlp(texto)
    resultados = []
    
    for i in range(len(doc) - len(patron) + 1):
        secuencia = doc[i:i+len(patron)]
        tags = [token.pos_ for token in secuencia]
        
        if tags == patron:
            texto_patron = ' '.join([token.text for token in secuencia])
            resultados.append(texto_patron)
    
    return resultados

# Buscar patrones espec√≠ficos
texto_patrones = """
El gato negro duerme en el sof√° c√≥modo. 
La casa grande tiene un jard√≠n hermoso. 
Los estudiantes inteligentes aprenden r√°pido.
"""

# Patr√≥n: Determinante + Sustantivo + Adjetivo (el gato negro)
patron_det_noun_adj = ['DET', 'NOUN', 'ADJ']
resultados = encontrar_patron(texto_patrones, patron_det_noun_adj)

print("Patr√≥n: DET + NOUN + ADJ")
print("Resultados:")
for r in resultados:
    print(f"  - {r}")

# Patr√≥n: Sustantivo + Verbo (estudiantes aprenden)
patron_noun_verb = ['NOUN', 'VERB']
resultados2 = encontrar_patron(texto_patrones, patron_noun_verb)

print("\nPatr√≥n: NOUN + VERB")
print("Resultados:")
for r in resultados2:
    print(f"  - {r}")
```

### 7.8 Desambiguaci√≥n de palabras

POS tagging ayuda a resolver ambig√ºedades.

```python
# Palabra "bajo" puede ser adjetivo, sustantivo o verbo
frases_bajo = [
    "El techo es muy bajo",           # Adjetivo
    "Toco el bajo en la banda",       # Sustantivo
    "Yo bajo por las escaleras"       # Verbo
]

print("Desambiguaci√≥n de 'bajo':")
print("-" * 50)

for frase in frases_bajo:
    doc = nlp(frase)
    for token in doc:
        if 'bajo' in token.text.lower():
            print(f"Frase: {frase}")
            print(f"  ‚Üí POS: {token.pos_} ({token.tag_})")
            print(f"  ‚Üí Lema: {token.lemma_}")
            print()
```

### 7.9 Extracci√≥n de frases nominales

```python
def extraer_frases_nominales(texto):
    """
    Extrae frases nominales (sustantivo con sus modificadores)
    """
    doc = nlp(texto)
    frases = []
    
    for chunk in doc.noun_chunks:
        frases.append({
            'texto': chunk.text,
            'raiz': chunk.root.text,
            'raiz_pos': chunk.root.pos_
        })
    
    return frases

texto_frases = """
La inteligencia artificial moderna utiliza redes neuronales profundas. 
Los algoritmos avanzados de aprendizaje autom√°tico procesan grandes 
cantidades de datos complejos en tiempo real.
"""

frases_nominales = extraer_frases_nominales(texto_frases)

print("FRASES NOMINALES:")
print("-" * 50)
for i, frase in enumerate(frases_nominales, 1):
    print(f"{i}. {frase['texto']}")
    print(f"   Ra√≠z: {frase['raiz']} ({frase['raiz_pos']})")
    print()
```

### 7.10 Aplicaci√≥n: Generador de res√∫menes extractivos

```python
def generar_resumen_extractivo(texto, num_oraciones=3):
    """
    Genera un resumen extractivo basado en frecuencia de sustantivos
    """
    # Dividir en oraciones
    oraciones = sent_tokenize(texto, language='spanish')
    
    # Procesar todo el texto
    doc = nlp(texto)
    
    # Contar frecuencia de sustantivos
    sustantivos = [token.lemma_.lower() for token in doc 
                   if token.pos_ == 'NOUN' and not token.is_stop]
    freq_sustantivos = Counter(sustantivos)
    
    # Puntuar oraciones
    scores = []
    for oracion in oraciones:
        doc_oracion = nlp(oracion)
        score = sum(freq_sustantivos.get(token.lemma_.lower(), 0) 
                   for token in doc_oracion if token.pos_ == 'NOUN')
        scores.append((oracion, score))
    
    # Ordenar por puntuaci√≥n y tomar las mejores
    scores.sort(key=lambda x: x[1], reverse=True)
    resumen = ' '.join([oracion for oracion, score in scores[:num_oraciones]])
    
    return resumen, freq_sustantivos.most_common(5)

# Texto largo para resumir
texto_largo = """
La inteligencia artificial est√° revolucionando m√∫ltiples industrias en todo el mundo.
Los sistemas de aprendizaje autom√°tico pueden analizar patrones complejos en grandes
conjuntos de datos. Las empresas utilizan estos sistemas para mejorar sus operaciones
y tomar decisiones m√°s informadas. Los investigadores contin√∫an desarrollando nuevos
algoritmos m√°s eficientes y precisos. El procesamiento del lenguaje natural permite
que las m√°quinas entiendan el texto humano. Las aplicaciones incluyen traducci√≥n
autom√°tica, an√°lisis de sentimientos y generaci√≥n de texto. Los chatbots utilizan
PLN para mantener conversaciones naturales con usuarios. El futuro de la IA promete
avances a√∫n m√°s impresionantes en los pr√≥ximos a√±os.
"""

resumen, palabras_clave = generar_resumen_extractivo(texto_largo, 3)

print("RESUMEN:")
print("=" * 60)
print(resumen)
print("\n" + "=" * 60)
print("PALABRAS CLAVE:")
for palabra, freq in palabras_clave:
    print(f"  - {palabra}: {freq} veces")
```



## 8. Ejercicios Pr√°cticos

### Ejercicio 1: An√°lisis de Tweets

```python
# Dataset de tweets simulados
tweets = [
    "Me encanta #Python! Es el mejor lenguaje para #DataScience üòç",
    "Odio cuando el c√≥digo no funciona üò§ @desarrollador ayuda!",
    "Nuevo tutorial de #MachineLearning en mi blog: https://example.com/ml",
    "¬øAlguien sabe resolver este error? TypeError en l√≠nea 42...",
    "¬°Consegu√≠ mi certificaci√≥n en #AI! üéâüéì Gracias @mentor"
]

# Tareas:
# 1. Limpiar tweets (eliminar URLs, menciones, hashtags)
# 2. Tokenizar
# 3. Eliminar stopwords
# 4. Calcular frecuencias
# 5. Identificar sentimiento (positivo/negativo) basado en palabras clave

# TU C√ìDIGO AQU√ç
```

### Ejercicio 2: Comparador de Documentos

```python
# Dos art√≠culos sobre el mismo tema
doc1 = """
El cambio clim√°tico es uno de los mayores desaf√≠os de nuestra era. 
Las temperaturas globales contin√∫an aumentando debido a las emisiones 
de gases de efecto invernadero.
"""

doc2 = """
El calentamiento global representa una amenaza seria para el planeta. 
El aumento de las temperaturas es causado principalmente por la actividad 
humana y las emisiones de CO2.
"""

# Tareas:
# 1. Procesar ambos documentos (limpiar, tokenizar, normalizar)
# 2. Calcular vocabulario com√∫n
# 3. Identificar palabras √∫nicas de cada documento
# 4. Calcular similitud basada en palabras compartidas

# TU C√ìDIGO AQU√ç
```

### Ejercicio 3: Extractor de Informaci√≥n

```python
texto_noticia = """
Madrid, 15 de marzo de 2024. El presidente del gobierno, Pedro S√°nchez,
anunci√≥ ayer un nuevo plan de inversi√≥n en tecnolog√≠a por valor de 
1.500 millones de euros. La ministra de Ciencia, Diana Morant, 
explic√≥ que el 60% se destinar√° a inteligencia artificial y el 40% 
restante a computaci√≥n cu√°ntica. El plan se implementar√° entre 2024 y 2027.
"""

# Tareas:
# 1. Extraer todas las fechas
# 2. Extraer todos los nombres propios (usando POS tagging)
# 3. Extraer todas las cantidades num√©ricas
# 4. Identificar las organizaciones mencionadas

# TU C√ìDIGO AQU√ç
```

### Ejercicio 4: Generador de Estad√≠sticas

```python
def analisis_completo_texto(archivo_texto):
    """
    Lee un archivo de texto y genera un informe completo
    
    El informe debe incluir:
    - N√∫mero total de palabras
    - N√∫mero de palabras √∫nicas
    - Top 20 palabras m√°s frecuentes (sin stopwords)
    - Top 10 bigramas
    - Distribuci√≥n de partes de la oraci√≥n
    - Longitud promedio de palabras
    - Longitud promedio de oraciones
    - Lista de palabras m√°s largas
    - Palabras que solo aparecen una vez (hapax legomena)
    """
    # TU C√ìDIGO AQU√ç
    pass
```

### Ejercicio 5: Clasificador de Sentimientos B√°sico

```python
# Listas de palabras positivas y negativas
palabras_positivas = {'excelente', 'bueno', 'genial', 'fant√°stico', 'incre√≠ble', 
                      'maravilloso', 'perfecto', 'amor', 'feliz', '√©xito'}

palabras_negativas = {'malo', 'terrible', 'horrible', 'p√©simo', 'odio', 
                      'fracaso', 'triste', 'error', 'problema', 'dif√≠cil'}

def clasificar_sentimiento(texto):
    """
    Clasifica un texto como positivo, negativo o neutro
    bas√°ndose en las palabras que contiene
    
    Retorna:
    --
    tuple
        (clasificaci√≥n, score_positivo, score_negativo)
    """
    # TU C√ìDIGO AQU√ç
    pass

# Probar con diferentes textos
textos_prueba = [
    "Este producto es excelente, me encanta!",
    "Terrible experiencia, muy malo",
    "El servicio es normal, nada especial"
]

# TU C√ìDIGO AQU√ç
```



## 9. Resumen y Mejores Pr√°cticas

### 9.1 Pipeline t√≠pico de preprocesamiento

```python
def pipeline_completo(texto):
    """
    Pipeline completo de preprocesamiento de texto
    """
    # 1. Limpieza b√°sica
    texto = texto.lower()
    texto = re.sub(r'https?://\S+|www\.\S+', '', texto)  # URLs
    texto = re.sub(r'\S+@\S+', '', texto)  # Emails
    texto = re.sub(r'[@#]\w+', '', texto)  # Menciones y hashtags
    
    # 2. Tokenizaci√≥n
    tokens = word_tokenize(texto, language='spanish')
    
    # 3. Filtrar puntuaci√≥n y n√∫meros
    tokens = [t for t in tokens if t.isalpha()]
    
    # 4. Eliminar stopwords
    stop_words = set(stopwords.words('spanish'))
    tokens = [t for t in tokens if t not in stop_words]
    
    # 5. Filtrar palabras muy cortas
    tokens = [t for t in tokens if len(t) > 2]
    
    # 6. Lematizaci√≥n
    doc = nlp(' '.join(tokens))
    tokens_finales = [token.lemma_ for token in doc if token.is_alpha]
    
    return tokens_finales
```

### 9.2 Cu√°ndo aplicar cada t√©cnica

| T√©cnica | Usar cuando... | No usar cuando... |
||-|-|
| Conversi√≥n a min√∫sculas | B√∫squeda, clasificaci√≥n | NER, an√°lisis de sentimientos |
| Eliminaci√≥n de stopwords | B√∫squeda, topic modeling | Traducci√≥n, generaci√≥n de texto |
| Stemming | B√∫squeda r√°pida, grandes vol√∫menes | Necesitas precisi√≥n exacta |
| Lematizaci√≥n | An√°lisis sem√°ntico, precisi√≥n | Procesamiento en tiempo real |
| Eliminaci√≥n de puntuaci√≥n | An√°lisis de palabras | An√°lisis sint√°ctico |
| Eliminaci√≥n de n√∫meros | An√°lisis de opiniones | An√°lisis cuantitativo |

### 9.3 Errores comunes a evitar

```python
# ‚ùå ERROR: No normalizar antes de comparar
texto1 = "Python es genial"
texto2 = "PYTHON ES GENIAL"
print(texto1 == texto2)  # False

# ‚úÖ CORRECTO:
print(texto1.lower() == texto2.lower())  # True

# ‚ùå ERROR: No manejar caracteres especiales
texto = "Hola c√≥mo est√°s"
tokens = texto.split()  # Mantiene tildes

# ‚úÖ CORRECTO (si quieres normalizar):
from unicodedata import normalize
texto_normalizado = normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')

# ‚ùå ERROR: No verificar el idioma
# Usar stopwords en ingl√©s para texto en espa√±ol

# ‚úÖ CORRECTO:
stop_words_es = set(stopwords.words('spanish'))

# ‚ùå ERROR: Aplicar todas las t√©cnicas sin pensar
# No siempre necesitas aplicar todo el pipeline

# ‚úÖ CORRECTO:
# Adapta el pipeline a tu tarea espec√≠fica
```

### 9.4 Optimizaci√≥n de rendimiento

```python
# Para grandes vol√∫menes de texto:

# 1. Compilar regex una sola vez
import re

PATRON_URL = re.compile(r'https?://\S+|www\.\S+')
PATRON_EMAIL = re.compile(r'\S+@\S+')

def limpiar_rapido(texto):
    texto = PATRON_URL.sub('', texto)
    texto = PATRON_EMAIL.sub('', texto)
    return texto

# 2. Usar sets para stopwords (b√∫squeda O(1))
stop_words = set(stopwords.words('spanish'))  # ‚úÖ R√°pido
# vs
stop_words = stopwords.words('spanish')  # ‚ùå Lento (lista)

# 3. Procesar por lotes con spaCy
textos = ["texto1", "texto2", "texto3", ...]
for doc in nlp.pipe(textos, batch_size=50):
    # Procesar doc
    pass

# 4. Cachear resultados frecuentes
from functools import lru_cache

@lru_cache(maxsize=10000)
def normalizar_palabra(palabra):
    return stemmer_es.stem(palabra.lower())
```



## 10. Recursos Adicionales y Pr√≥ximos Pasos

### Bibliotecas importantes:
- **NLTK**: Fundamentos y educaci√≥n
- **spaCy**: Producci√≥n y alto rendimiento
- **Gensim**: Topic modeling y word embeddings
- **TextBlob**: An√°lisis de sentimientos simple
- **Transformers**: Modelos estado del arte (Hugging Face)

### Conceptos avanzados para estudiar despu√©s:
1. **Word Embeddings**: Word2Vec, GloVe, FastText
2. **Topic Modeling**: LDA, NMF
3. **Named Entity Recognition (NER)**
4. **An√°lisis de Sentimientos avanzado**
5. **Modelos de lenguaje**: BERT, GPT
6. **Secuencia a secuencia**: Traducci√≥n, resumen

### Datasets para practicar:
- Reviews de productos (Amazon, Yelp)
- Tweets en espa√±ol
- Noticias (BBC, El Pa√≠s)
- Libros del Proyecto Gutenberg
- Corpus de la RAE



## Conclusi√≥n

El preprocesamiento de texto es fundamental en PLN. Los pasos que hemos visto:

1. **Tokenizaci√≥n**: Dividir texto en unidades
2. **Limpieza**: Eliminar ruido
3. **Normalizaci√≥n**: Reducir variabilidad
4. **Visualizaci√≥n**: Entender los datos
5. **POS Tagging**: An√°lisis gramatical
